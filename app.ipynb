{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba07feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from pdfplumber) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anshul\\desktop\\literature review generator\\venv\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
      "Collecting groq\n",
      "  Using cached groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting sniffio (from groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.10 (from groq)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->groq)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->groq)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached groq-0.31.1-py3-none-any.whl (134 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, python-dotenv, idna, h11, distro, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, groq\n",
      "\n",
      "   ----- ----------------------------------  2/15 [python-dotenv]\n",
      "   ----- ----------------------------------  2/15 [python-dotenv]\n",
      "   -------- -------------------------------  3/15 [idna]\n",
      "   ---------- -----------------------------  4/15 [h11]\n",
      "   ------------- --------------------------  5/15 [distro]\n",
      "   ---------------- -----------------------  6/15 [certifi]\n",
      "   ------------------------ ---------------  9/15 [pydantic-core]\n",
      "   -------------------------- ------------- 10/15 [httpcore]\n",
      "   -------------------------- ------------- 10/15 [httpcore]\n",
      "   -------------------------- ------------- 10/15 [httpcore]\n",
      "   ----------------------------- ---------- 11/15 [anyio]\n",
      "   ----------------------------- ---------- 11/15 [anyio]\n",
      "   ----------------------------- ---------- 11/15 [anyio]\n",
      "   ----------------------------- ---------- 11/15 [anyio]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   -------------------------------- ------- 12/15 [pydantic]\n",
      "   ---------------------------------- ----- 13/15 [httpx]\n",
      "   ---------------------------------- ----- 13/15 [httpx]\n",
      "   ---------------------------------- ----- 13/15 [httpx]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ------------------------------------- -- 14/15 [groq]\n",
      "   ---------------------------------------- 15/15 [groq]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 distro-1.9.0 groq-0.31.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 pydantic-2.11.9 pydantic-core-2.33.2 python-dotenv-1.1.1 sniffio-1.3.1 typing-extensions-4.15.0 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "!pip install pdfplumber\n",
    "!pip install groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736f2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF parsing\n",
    "import re    # Regular expressions\n",
    "from collections import defaultdict  # For grouping blocks by page\n",
    "import pdfplumber\n",
    "from groq import Groq\n",
    "import json # Import the json library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3078a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your API key or load from environment\n",
    "client = Groq(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12597a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1, Bbox: (39.86970138549805, 72.0516357421875, 336.1473083496094, 80.04115295410156)\n",
      "Text:\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400...\n",
      "\n",
      "Page 1, Bbox: (39.86970138549805, 91.03842163085938, 400.13232421875, 112.45281219482422)\n",
      "Text:\n",
      "PE-GPT: A New Paradigm for Power Electronics Design...\n",
      "\n",
      "Page 1, Bbox: (39.86970138549805, 126.50065612792969, 228.0218505859375, 135.51092529296875)\n",
      "Text:\n",
      "Article  in  IEEE Transactions on Industrial Electronics · October 2024...\n",
      "\n",
      "Page 1, Bbox: (39.86970138549805, 138.43756103515625, 99.71084594726562, 144.65162658691406)\n",
      "Text:\n",
      "DOI: 10.1109/TIE.2024.3454408...\n",
      "\n",
      "Page 1, Bbox: (39.86970138549805, 170.3651123046875, 63.66929626464844, 189.11892700195312)\n",
      "Text:\n",
      "CITATIONS\n",
      "8...\n",
      "\n",
      "Page 1, Bbox: (305.667724609375, 170.3651123046875, 323.5054016113281, 189.11892700195312)\n",
      "Text:\n",
      "READS\n",
      "1,067...\n",
      "\n",
      "Page 1, Bbox: (39.86970138549805, 204.91107177734375, 97.39005279541016, 213.8596954345703)\n",
      "Text:\n",
      "8 authors, including:...\n",
      "\n",
      "Page 1, Bbox: (66.44950866699219, 222.92381286621094, 95.22240447998047, 231.80105590820312)\n",
      "Text:\n",
      "Fanfan Lin...\n",
      "\n",
      "Page 1, Bbox: (66.44950866699219, 233.5557403564453, 259.76568603515625, 242.4329833984375)\n",
      "Text:\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute...\n",
      "\n",
      "Page 1, Bbox: (66.44950866699219, 245.5166473388672, 149.6239471435547, 254.39389038085938)\n",
      "Text:\n",
      "54 PUBLICATIONS   561 CITATIONS...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Load PDF\n",
    "pdf_path = 'file.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "extracted_blocks = []\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text_blocks = page.get_text(\"blocks\")\n",
    "\n",
    "    for block in text_blocks:\n",
    "        x0, y0, x1, y1, text, block_no, block_type = block\n",
    "        bbox = (x0, y0, x1, y1)\n",
    "\n",
    "        # Clean up text (remove extra spaces/newlines)\n",
    "        clean_text = text.strip()\n",
    "\n",
    "        # Store only non-empty text blocks\n",
    "        if clean_text:\n",
    "            extracted_blocks.append({\n",
    "                'page': page_num + 1,\n",
    "                'text': clean_text,\n",
    "                'bbox': bbox\n",
    "            })\n",
    "\n",
    "doc.close()\n",
    "\n",
    "# Debugging: print first few blocks\n",
    "for i, block_data in enumerate(extracted_blocks[:10]):\n",
    "    print(f\"Page {block_data['page']}, Bbox: {block_data['bbox']}\")\n",
    "    print(f\"Text:\\n{block_data['text'][:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0bec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Headings:\n",
      "Page 1, Bbox: (66.44950866699219, 245.5166473388672, 149.6239471435547, 254.39389038085938), Text: 54 PUBLICATIONS   561 CITATIONS\n",
      "Page 1, Bbox: (332.9120178222656, 245.5166473388672, 416.08648681640625, 254.39389038085938), Text: 54 PUBLICATIONS   577 CITATIONS\n",
      "Page 1, Bbox: (66.44950866699219, 308.6436767578125, 157.40061950683594, 317.5209045410156), Text: 178 PUBLICATIONS   3,120 CITATIONS\n",
      "Page 2, Bbox: (140.82000732421875, 529.0392456054688, 217.0262908935547, 542.7164916992188), Text: INTRODUCTION\n",
      "Page 3, Bbox: (70.35700988769531, 627.6192016601562, 266.8639221191406, 641.4384765625), Text: II. PRELIMINARIES: LARGE LANGUAGE MODELS\n",
      "Page 3, Bbox: (46.770599365234375, 646.9979248046875, 301.3240051269531, 741.0250244140625), Text: introduction of the Transformer model in 2017 [10]. Its core\n",
      "Page 4, Bbox: (46.78997802734375, 507.0202941894531, 199.33445739746094, 520.2767333984375), Text: A. Hybrid Framework of PE-GPT\n",
      "Page 5, Bbox: (46.800018310546875, 185.80918884277344, 240.50665283203125, 199.06564331054688), Text: B. Retrieval Augmented Generation (RAG)\n",
      "Page 5, Bbox: (46.80000305175781, 203.02352905273438, 106.89995574951172, 216.53048706054688), Text: a) Why RAG\n",
      "Page 5, Bbox: (46.77995300292969, 504.0045471191406, 128.0020751953125, 517.511474609375), Text: b) RAG Workflow\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 1) Chunking: As an initial preprocessing step, the existing\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 2) Embedding: Both the chunks from the knowledge base and\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 3) Retrieval: The retriever calculates the similarity score\n",
      "Page 5, Bbox: (313.194091796875, 201.92649841308594, 543.6741943359375, 215.18295288085938), Text: C. Model Zoo and the Adopted PANN Model Inside\n",
      "Page 5, Bbox: (313.18408203125, 219.14083862304688, 377.18182373046875, 232.64779663085938), Text: a) Model Zoo\n",
      "Page 5, Bbox: (313.1138916015625, 492.3363342285156, 387.0715026855469, 505.8432922363281), Text: b) PANN Model\n",
      "Page 6, Bbox: (313.20001220703125, 52.580055236816406, 527.5478515625, 65.83651733398438), Text: D. Discussion on the Generalization of PE-GPT\n",
      "Page 6, Bbox: (313.1999816894531, 69.73428344726562, 456.87677001953125, 83.24124145507812), Text: a) RAG Enhances Generalization\n",
      "Page 6, Bbox: (313.1698303222656, 196.77792358398438, 481.034912109375, 210.28488159179688), Text: b) Model Zoo Enhances Generalization\n",
      "Page 6, Bbox: (313.2041320800781, 475.99884033203125, 567.9705810546875, 500.7783203125), Text: A. Design Case I Background: the DAB Converter and\n",
      "Page 7, Bbox: (46.79197692871094, 196.59593200683594, 191.0599365234375, 209.85238647460938), Text: B. Design Case I Specifications\n",
      "Page 7, Bbox: (52.20000076293945, 225.0931396484375, 294.9596862792969, 391.52886962890625), Text: 200 V\n",
      "Page 7, Bbox: (313.20001220703125, 217.04002380371094, 567.9785766601562, 241.81948852539062), Text: C. Design Case I Breakdown and Analysis on the\n",
      "Page 7, Bbox: (313.1868896484375, 360.5740051269531, 567.7099609375, 431.77618408203125), Text: 1) Initial modulation strategy recommendations (ID 1 and 2):\n",
      "Page 7, Bbox: (313.1868896484375, 360.5740051269531, 567.7099609375, 431.77618408203125), Text: 2) Design of modulation parameters and performance analysis\n",
      "Page 7, Bbox: (80.58000183105469, 579.93310546875, 178.27499389648438, 607.1898803710938), Text: 200 V, V2 = 160 V, Prated=1000 W\n",
      "Page 7, Bbox: (192.24000549316406, 661.5868530273438, 501.552490234375, 687.382568359375), Text: 18.69 A.\n",
      "Page 8, Bbox: (46.779937744140625, 52.042091369628906, 301.30291748046875, 169.12588500976562), Text: 3) Adaptation to user feedback and redesign (ID 4 and 5): PE-\n",
      "Page 8, Bbox: (46.779937744140625, 52.042091369628906, 301.30291748046875, 169.12588500976562), Text: 4) Model fine-tuning (ID 6): Lastly, PE-GPT enhances its\n",
      "Page 8, Bbox: (46.779937744140625, 173.09378051757812, 298.8130187988281, 213.26397705078125), Text: a) Adaptive Semantic Understanding (ASU)\n",
      "Page 8, Bbox: (46.75990295410156, 210.59866333007812, 301.2679443359375, 285.27777099609375), Text: b) PE-Specific Knowledge and Reasoning (PE-KR)\n",
      "Page 8, Bbox: (46.74986267089844, 282.6124572753906, 301.25885009765625, 345.7685546875), Text: c) Multimodal Processing (MMP)\n",
      "Page 8, Bbox: (46.73985290527344, 395.6080627441406, 188.84349060058594, 408.8645324707031), Text: D. Fine-Tuning of PANN Model\n",
      "Page 8, Bbox: (351.5400085449219, 476.2364196777344, 414.9784240722656, 503.144775390625), Text: 160 V (train & val.)\n",
      "Page 8, Bbox: (475.09576416015625, 479.79412841796875, 533.302490234375, 520.1152954101562), Text: 200 V, 240 V (test)\n",
      "Page 8, Bbox: (313.20001220703125, 626.0000610351562, 446.71649169921875, 639.2564697265625), Text: E. Hyperparameters of RAG\n",
      "Page 9, Bbox: (46.79914855957031, 416.3648986816406, 193.82260131835938, 429.6213684082031), Text: A. Design Case II Specifications\n",
      "Page 9, Bbox: (318.6000061035156, 81.34081268310547, 561.3150024414062, 172.22886657714844), Text: 200 V\n",
      "Page 9, Bbox: (313.20001220703125, 179.1200408935547, 511.6961975097656, 192.37649536132812), Text: B. Design Case II Breakdown and Analysis\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 1) Circuit Parameter Design (ID 1-3): PE-GPT understands\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 2) Rationale Analysis (ID 4-5): PE-GPT explains the rationale\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 3) Simulation Verification (ID 6): With the user’s request to\n",
      "Page 9, Bbox: (313.16021728515625, 427.1446533203125, 453.3399963378906, 440.401123046875), Text: C. Design Outcomes Analysis\n",
      "Page 9, Bbox: (280.32000732421875, 570.7991943359375, 399.8565673828125, 588.4425659179688), Text: 2. (…)\n",
      "Page 9, Bbox: (273.29998779296875, 679.2792358398438, 501.5930480957031, 704.9625854492188), Text: 0.0549 A (3rd)… The harmonics of output voltage are 80.0 V (DC), 0.204 V (1st),\n",
      "Page 9, Bbox: (273.29998779296875, 679.2792358398438, 501.5930480957031, 704.9625854492188), Text: 0.0157 V (2nd), 0.00466 V (3rd)…\n",
      "Page 10, Bbox: (46.82164001464844, 347.78973388671875, 301.60528564453125, 372.5692138671875), Text: A. Validation for PE Domain Comprehension: Benchmark\n",
      "Page 10, Bbox: (313.20147705078125, 162.05821228027344, 567.9700317382812, 186.83767700195312), Text: B. Validation for the 5DoF Design Case: Steady-State\n",
      "Page 10, Bbox: (313.19146728515625, 190.79556274414062, 433.53167724609375, 204.30252075195312), Text: a) Steady-State Waveforms\n",
      "Page 11, Bbox: (46.78694152832031, 75.0792007446289, 301.306396484375, 169.03903198242188), Text: 300 W, the zero-voltage plateau of vs is aggregated to its right\n",
      "Page 11, Bbox: (46.790008544921875, 167.05917358398438, 301.34490966796875, 261.0791320800781), Text: 12. Similarly, across the entire operating conditions, all-switch\n",
      "Page 11, Bbox: (46.801025390625, 265.2774963378906, 195.21728515625, 278.7844543457031), Text: b) Detailed Performance Analysis\n",
      "Page 11, Bbox: (313.2042236328125, 302.5389709472656, 518.6142578125, 315.7954406738281), Text: A. Extending Across the PE Design Workflow\n",
      "Page 11, Bbox: (313.20001220703125, 594.320068359375, 424.4019775390625, 607.5764770507812), Text: B. AI Ethics of PE-GPT\n",
      "Page 11, Bbox: (313.2099914550781, 611.303955078125, 567.7213745117188, 739.8505249023438), Text: a) Privacy and Security: Privacy concern covers not only the\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: b) Bias and Fairness: Bias in model will generate design\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: c) Transparency and Accountability: Transparency in how\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: d) Impact on Employment and Skills: To the authors’\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: e) Ethical Use: It is important to define the intended and\n",
      "Page 12, Bbox: (131.949951171875, 558.6192016601562, 216.38555908203125, 572.421142578125), Text: VIII. CONCLUSION\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: a) Enhancing PE-GPT Expertise: Explore diversified domain\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: b) Generalizing PE-GPT's Applications: Improve PE-GPT’s\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: c) Integrating into the Existing Design Workflow: Facilitate the\n",
      "Page 12, Bbox: (411.1199951171875, 536.1192016601562, 470.00555419921875, 549.7964477539062), Text: REFERENCES\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: S. Kouro, J. I. Leon, D. Vinnikov, and L. G. Franquelo, “Grid-Connected\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: J. M. Guerrero, J. C. Vasquez, J. Matas, L. G. De Vicuna, and M. Castilla,\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: X. Li, X. Zhang, F. Lin, and F. Blaabjerg, “Artificial-Intelligence-Based\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: Y. Chang et al., “A Survey on Evaluation of Large Language Models,”\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: H. Touvron et al., “Llama 2: Open Foundation and Fine-Tuned Chat\n",
      "Page 12, Bbox: (313.1999816894531, 553.7808227539062, 567.2366943359375, 720.9219360351562), Text: P. Lee, S. Bubeck, and J. Petro, “Benefits, Limits, and Risks of GPT-4 as\n",
      "Page 12, Bbox: (320.6400146484375, 727.7208251953125, 543.0150146484375, 738.4699096679688), Text: 1 PE-GPT [Online]. Available: https://github.com/XinzeLee/PE-GPT\n",
      "Page 13, Bbox: (46.79243469238281, 52.30077362060547, 300.9138488769531, 578.1111450195312), Text: P. Wang, X. Wei, F. Hu, and W. Han, “TransGPT: Multi-modal\n",
      "Page 13, Bbox: (46.79243469238281, 52.30077362060547, 300.9138488769531, 578.1111450195312), Text: S. Wu and M. Sun, “Exploring the Efficacy of Pre-trained Checkpoints\n",
      "Page 13, Bbox: (383.7079772949219, 52.10943603515625, 567.4739379882812, 118.31951141357422), Text: 2023. He joined the University of Arkansas, USA, as\n",
      "Page 13, Bbox: (313.20001220703125, 487.1208190917969, 569.6111450195312, 608.3929443359375), Text: 2023 IEEE Transactions on Industrial Electronics Outstanding Paper\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "identified_headings = []\n",
    "\n",
    "# === BROADENED HEADING PATTERNS ===\n",
    "heading_patterns = [\n",
    "    re.compile(r'^ABSTRACT$', re.IGNORECASE),                          # Abstract\n",
    "    re.compile(r'^[IVXLCDM]+\\.\\s+.*'),                                 # Roman numeral headings: I. Intro\n",
    "    re.compile(r'^\\d+\\.\\s+.*'),                                       # 1. Intro\n",
    "    re.compile(r'^\\d+\\s+[A-Z].*'),                                   # 1 INTRODUCTION (no dot)\n",
    "    re.compile(r'^\\d+(\\.\\d+)+\\s+.*'),                                # 3.1 Training-Data Poisoning / 3.1.1 Batch Learner\n",
    "    re.compile(r'^[A-Z]\\.\\s+.*'),                                    # A. Title / B. Design Case\n",
    "    re.compile(r'^[a-z]\\)\\s+.*'),                                    # a) Why RAG\n",
    "    re.compile(r'^\\d+\\)\\s+.*'),                                      # 1) Classification\n",
    "    re.compile(r'^Theorem\\s+\\d+\\.?', re.IGNORECASE),                  # Theorem 3.\n",
    "    re.compile(r'^(Preface|Abstract|Introduction|Conclusion|References|Acknowledgements)\\s*$', re.IGNORECASE),\n",
    "    re.compile(r'^(Preface|Abstract|Introduction|Conclusion|References|Acknowledgements)\\s*\\.*\\s+.*', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "# === IDENTIFY HEADINGS ===\n",
    "for block_data in extracted_blocks:\n",
    "    text = block_data['text'].strip()\n",
    "    page_num = block_data['page']\n",
    "    bbox = block_data['bbox']\n",
    "\n",
    "    lines_in_block = text.split('\\n')\n",
    "    for line in lines_in_block:\n",
    "        cleaned_line = line.strip()\n",
    "        if not cleaned_line:\n",
    "            continue\n",
    "\n",
    "        is_heading = False\n",
    "        for pattern in heading_patterns:\n",
    "            if pattern.match(cleaned_line):\n",
    "                identified_headings.append({'page': page_num, 'text': cleaned_line, 'bbox': bbox})\n",
    "                is_heading = True\n",
    "                break\n",
    "\n",
    "        # OPTIONAL: could add heuristics for very short lines that look like headings\n",
    "        # but leaving this disabled per your preference.\n",
    "\n",
    "# === OUTPUT ===\n",
    "print(\"Identified Headings:\")\n",
    "for heading in identified_headings:\n",
    "    print(f\"Page {heading['page']}, Bbox: {heading['bbox']}, Text: {heading['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a84d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings up to References:\n",
      "Page 1, Bbox: (66.44950866699219, 245.5166473388672, 149.6239471435547, 254.39389038085938), Text: 54 PUBLICATIONS   561 CITATIONS\n",
      "Page 1, Bbox: (332.9120178222656, 245.5166473388672, 416.08648681640625, 254.39389038085938), Text: 54 PUBLICATIONS   577 CITATIONS\n",
      "Page 1, Bbox: (66.44950866699219, 308.6436767578125, 157.40061950683594, 317.5209045410156), Text: 178 PUBLICATIONS   3,120 CITATIONS\n",
      "Page 2, Bbox: (140.82000732421875, 529.0392456054688, 217.0262908935547, 542.7164916992188), Text: INTRODUCTION\n",
      "Page 3, Bbox: (70.35700988769531, 627.6192016601562, 266.8639221191406, 641.4384765625), Text: II. PRELIMINARIES: LARGE LANGUAGE MODELS\n",
      "Page 3, Bbox: (46.770599365234375, 646.9979248046875, 301.3240051269531, 741.0250244140625), Text: introduction of the Transformer model in 2017 [10]. Its core\n",
      "Page 4, Bbox: (46.78997802734375, 507.0202941894531, 199.33445739746094, 520.2767333984375), Text: A. Hybrid Framework of PE-GPT\n",
      "Page 5, Bbox: (46.800018310546875, 185.80918884277344, 240.50665283203125, 199.06564331054688), Text: B. Retrieval Augmented Generation (RAG)\n",
      "Page 5, Bbox: (46.80000305175781, 203.02352905273438, 106.89995574951172, 216.53048706054688), Text: a) Why RAG\n",
      "Page 5, Bbox: (46.77995300292969, 504.0045471191406, 128.0020751953125, 517.511474609375), Text: b) RAG Workflow\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 1) Chunking: As an initial preprocessing step, the existing\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 2) Embedding: Both the chunks from the knowledge base and\n",
      "Page 5, Bbox: (46.759918212890625, 521.299072265625, 301.3140563964844, 730.1619873046875), Text: 3) Retrieval: The retriever calculates the similarity score\n",
      "Page 5, Bbox: (313.194091796875, 201.92649841308594, 543.6741943359375, 215.18295288085938), Text: C. Model Zoo and the Adopted PANN Model Inside\n",
      "Page 5, Bbox: (313.18408203125, 219.14083862304688, 377.18182373046875, 232.64779663085938), Text: a) Model Zoo\n",
      "Page 5, Bbox: (313.1138916015625, 492.3363342285156, 387.0715026855469, 505.8432922363281), Text: b) PANN Model\n",
      "Page 6, Bbox: (313.20001220703125, 52.580055236816406, 527.5478515625, 65.83651733398438), Text: D. Discussion on the Generalization of PE-GPT\n",
      "Page 6, Bbox: (313.1999816894531, 69.73428344726562, 456.87677001953125, 83.24124145507812), Text: a) RAG Enhances Generalization\n",
      "Page 6, Bbox: (313.1698303222656, 196.77792358398438, 481.034912109375, 210.28488159179688), Text: b) Model Zoo Enhances Generalization\n",
      "Page 6, Bbox: (313.2041320800781, 475.99884033203125, 567.9705810546875, 500.7783203125), Text: A. Design Case I Background: the DAB Converter and\n",
      "Page 7, Bbox: (46.79197692871094, 196.59593200683594, 191.0599365234375, 209.85238647460938), Text: B. Design Case I Specifications\n",
      "Page 7, Bbox: (52.20000076293945, 225.0931396484375, 294.9596862792969, 391.52886962890625), Text: 200 V\n",
      "Page 7, Bbox: (313.20001220703125, 217.04002380371094, 567.9785766601562, 241.81948852539062), Text: C. Design Case I Breakdown and Analysis on the\n",
      "Page 7, Bbox: (313.1868896484375, 360.5740051269531, 567.7099609375, 431.77618408203125), Text: 1) Initial modulation strategy recommendations (ID 1 and 2):\n",
      "Page 7, Bbox: (313.1868896484375, 360.5740051269531, 567.7099609375, 431.77618408203125), Text: 2) Design of modulation parameters and performance analysis\n",
      "Page 7, Bbox: (80.58000183105469, 579.93310546875, 178.27499389648438, 607.1898803710938), Text: 200 V, V2 = 160 V, Prated=1000 W\n",
      "Page 7, Bbox: (192.24000549316406, 661.5868530273438, 501.552490234375, 687.382568359375), Text: 18.69 A.\n",
      "Page 8, Bbox: (46.779937744140625, 52.042091369628906, 301.30291748046875, 169.12588500976562), Text: 3) Adaptation to user feedback and redesign (ID 4 and 5): PE-\n",
      "Page 8, Bbox: (46.779937744140625, 52.042091369628906, 301.30291748046875, 169.12588500976562), Text: 4) Model fine-tuning (ID 6): Lastly, PE-GPT enhances its\n",
      "Page 8, Bbox: (46.779937744140625, 173.09378051757812, 298.8130187988281, 213.26397705078125), Text: a) Adaptive Semantic Understanding (ASU)\n",
      "Page 8, Bbox: (46.75990295410156, 210.59866333007812, 301.2679443359375, 285.27777099609375), Text: b) PE-Specific Knowledge and Reasoning (PE-KR)\n",
      "Page 8, Bbox: (46.74986267089844, 282.6124572753906, 301.25885009765625, 345.7685546875), Text: c) Multimodal Processing (MMP)\n",
      "Page 8, Bbox: (46.73985290527344, 395.6080627441406, 188.84349060058594, 408.8645324707031), Text: D. Fine-Tuning of PANN Model\n",
      "Page 8, Bbox: (351.5400085449219, 476.2364196777344, 414.9784240722656, 503.144775390625), Text: 160 V (train & val.)\n",
      "Page 8, Bbox: (475.09576416015625, 479.79412841796875, 533.302490234375, 520.1152954101562), Text: 200 V, 240 V (test)\n",
      "Page 8, Bbox: (313.20001220703125, 626.0000610351562, 446.71649169921875, 639.2564697265625), Text: E. Hyperparameters of RAG\n",
      "Page 9, Bbox: (46.79914855957031, 416.3648986816406, 193.82260131835938, 429.6213684082031), Text: A. Design Case II Specifications\n",
      "Page 9, Bbox: (318.6000061035156, 81.34081268310547, 561.3150024414062, 172.22886657714844), Text: 200 V\n",
      "Page 9, Bbox: (313.20001220703125, 179.1200408935547, 511.6961975097656, 192.37649536132812), Text: B. Design Case II Breakdown and Analysis\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 1) Circuit Parameter Design (ID 1-3): PE-GPT understands\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 2) Rationale Analysis (ID 4-5): PE-GPT explains the rationale\n",
      "Page 9, Bbox: (313.179931640625, 196.10391235351562, 567.7109985351562, 359.1488952636719), Text: 3) Simulation Verification (ID 6): With the user’s request to\n",
      "Page 9, Bbox: (313.16021728515625, 427.1446533203125, 453.3399963378906, 440.401123046875), Text: C. Design Outcomes Analysis\n",
      "Page 9, Bbox: (280.32000732421875, 570.7991943359375, 399.8565673828125, 588.4425659179688), Text: 2. (…)\n",
      "Page 9, Bbox: (273.29998779296875, 679.2792358398438, 501.5930480957031, 704.9625854492188), Text: 0.0549 A (3rd)… The harmonics of output voltage are 80.0 V (DC), 0.204 V (1st),\n",
      "Page 9, Bbox: (273.29998779296875, 679.2792358398438, 501.5930480957031, 704.9625854492188), Text: 0.0157 V (2nd), 0.00466 V (3rd)…\n",
      "Page 10, Bbox: (46.82164001464844, 347.78973388671875, 301.60528564453125, 372.5692138671875), Text: A. Validation for PE Domain Comprehension: Benchmark\n",
      "Page 10, Bbox: (313.20147705078125, 162.05821228027344, 567.9700317382812, 186.83767700195312), Text: B. Validation for the 5DoF Design Case: Steady-State\n",
      "Page 10, Bbox: (313.19146728515625, 190.79556274414062, 433.53167724609375, 204.30252075195312), Text: a) Steady-State Waveforms\n",
      "Page 11, Bbox: (46.78694152832031, 75.0792007446289, 301.306396484375, 169.03903198242188), Text: 300 W, the zero-voltage plateau of vs is aggregated to its right\n",
      "Page 11, Bbox: (46.790008544921875, 167.05917358398438, 301.34490966796875, 261.0791320800781), Text: 12. Similarly, across the entire operating conditions, all-switch\n",
      "Page 11, Bbox: (46.801025390625, 265.2774963378906, 195.21728515625, 278.7844543457031), Text: b) Detailed Performance Analysis\n",
      "Page 11, Bbox: (313.2042236328125, 302.5389709472656, 518.6142578125, 315.7954406738281), Text: A. Extending Across the PE Design Workflow\n",
      "Page 11, Bbox: (313.20001220703125, 594.320068359375, 424.4019775390625, 607.5764770507812), Text: B. AI Ethics of PE-GPT\n",
      "Page 11, Bbox: (313.2099914550781, 611.303955078125, 567.7213745117188, 739.8505249023438), Text: a) Privacy and Security: Privacy concern covers not only the\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: b) Bias and Fairness: Bias in model will generate design\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: c) Transparency and Accountability: Transparency in how\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: d) Impact on Employment and Skills: To the authors’\n",
      "Page 12, Bbox: (46.769927978515625, 52.042091369628906, 301.3089294433594, 548.6637573242188), Text: e) Ethical Use: It is important to define the intended and\n",
      "Page 12, Bbox: (131.949951171875, 558.6192016601562, 216.38555908203125, 572.421142578125), Text: VIII. CONCLUSION\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: a) Enhancing PE-GPT Expertise: Explore diversified domain\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: b) Generalizing PE-GPT's Applications: Improve PE-GPT’s\n",
      "Page 12, Bbox: (313.10845947265625, 316.6263122558594, 567.609619140625, 487.5000305175781), Text: c) Integrating into the Existing Design Workflow: Facilitate the\n",
      "Page 12, Bbox: (411.1199951171875, 536.1192016601562, 470.00555419921875, 549.7964477539062), Text: REFERENCES\n"
     ]
    }
   ],
   "source": [
    "# === REMOVE HEADINGS AFTER REFERENCES ===\n",
    "filtered_headings = []\n",
    "stop_after_references = False\n",
    "\n",
    "for heading in identified_headings:\n",
    "    if stop_after_references:\n",
    "        break  # Stop collecting once References is reached\n",
    "    filtered_headings.append(heading)\n",
    "    if re.match(r'^REFERENCES$', heading['text'], re.IGNORECASE):\n",
    "        stop_after_references = True\n",
    "\n",
    "# === OUTPUT FILTERED HEADINGS ===\n",
    "print(\"Headings up to References:\")\n",
    "for heading in filtered_headings:\n",
    "    print(f\"Page {heading['page']}, Bbox: {heading['bbox']}, Text: {heading['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f13aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Section: Metadata (Title/Authors) ---\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\n",
      "PE-GPT: A New Paradigm for Power Electronics Design\n",
      "Article  in  IEEE Transactions on Industrial Electronics · October 2024\n",
      "DOI: 10.1109/TIE.2024.3454408\n",
      "CITATIONS\n",
      "8\n",
      "READS\n",
      "1,067\n",
      "8 authors, including:\n",
      "Fanfan Lin\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute...\n",
      "\n",
      "--- Section: 54 PUBLICATIONS   561 CITATIONS ---\n",
      "SEE PROFILE\n",
      "Xinze Li\n",
      "University of Arkansas at Fayetteville...\n",
      "\n",
      "--- Section: 54 PUBLICATIONS   577 CITATIONS ---\n",
      "SEE PROFILE\n",
      "J.J. Rodriguez-Andina\n",
      "University of Vigo...\n",
      "\n",
      "--- Section: 178 PUBLICATIONS   3,120 CITATIONS ---\n",
      "SEE PROFILE\n",
      "Josep M. Guerrero\n",
      "Zhejiang University\n",
      "2,111 PUBLICATIONS   115,701 CITATIONS\n",
      "SEE PROFILE\n",
      "All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\n",
      "The user has requested enhancement of the downloaded file.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "Abstract— Large Language Models (LLMs) have shown \n",
      "exciting potential in powering the growth of many \n",
      "industries, yet their adoption in the power electronics (PE) \n",
      "sector is hindered by a lack of specialize...\n",
      "\n",
      "--- Section: INTRODUCTION ---\n",
      "O accelerate the world’s transition to sustainable energy,\n",
      "power electronics (PE) devices play important roles.\n",
      "Unlike traditional fossil energy from which we can expect \n",
      "steady and predictable power, most of the sustainable energy \n",
      "sources exhibit the characteristic of intermittency, including \n",
      "solar and wind energy [1]. This intermittency, resulted from \n",
      "day-night cycle for solar energy and wind speed for wind \n",
      "energy, poses serious challenges for grid stability and reliability. \n",
      "To address th...\n",
      "\n",
      "--- Section: II. PRELIMINARIES: LARGE LANGUAGE MODELS ---\n",
      "...\n",
      "\n",
      "--- Section: introduction of the Transformer model in 2017 [10]. Its core ---\n",
      "capture long-term dependencies in language.\n",
      "(\n",
      ")\n",
      "Attention\n",
      ",\n",
      ",\n",
      "softmax\n",
      "T\n",
      "k\n",
      "QK\n",
      "Q K V\n",
      "V\n",
      "d\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(1)\n",
      "Following that, Google introduced Bidirectional Encoder \n",
      "Representations from Transformers (BERT) in 2018 which \n",
      "overcame the preceding language models’ constraints in \n",
      "unidirectional language modeling. In 2023, OpenAI trained \n",
      "GPT-4 on extensive Internet datasets, which has 1.76 trillion \n",
      "parameters, pushing the development of LLM to a new height. \n",
      "Although GPT-4 is capable of processing...\n",
      "\n",
      "--- Section: A. Hybrid Framework of PE-GPT ---\n",
      "Fig. 2 presents the proposed hybrid framework of multimodal \n",
      "PE-GPT. There are two types of acceptable input data from the \n",
      "user: textual queries and PE-specific fine-tuning data.\n",
      "All textual queries first go through the RAG process, \n",
      "indicated by the green dotted section in Fig. 2, to enhance the \n",
      "domain expertise of the original LLM agent. To facilitate RAG, \n",
      "a specialized PE knowledge base is constructed beforehand \n",
      "based on the application-specific needs. For instance, if the PE-\n",
      "GPT is expe...\n",
      "\n",
      "--- Section: B. Retrieval Augmented Generation (RAG) ---\n",
      "a) Why RAG\n",
      "LLM agents such as GPT-4 and Llama have been pretrained \n",
      "on vast and general corpora. The pretraining equips them with \n",
      "rich general domain knowledge. However, it also inherently \n",
      "limits their depth in specific domain expertise.\n",
      "The analysis in Section II explores three techniques: prompt \n",
      "engineering, RAG, and model fine-tuning. Among these, PE-\n",
      "GPT notably leverages RAG to tailor LLM for PE-specific \n",
      "applications. RAG effectively bridges the knowledge gap of \n",
      "LLMs through retrieving...\n",
      "\n",
      "--- Section: C. Model Zoo and the Adopted PANN Model Inside ---\n",
      "a) Model Zoo\n",
      "As elaborated in PE-GPT Workflow 1, the role of the Model \n",
      "Zoo is function evaluation, upon which the metaheuristic \n",
      "algorithm iterates until the optimal design outcome is found.\n",
      "Model Zoo consists of three types of models: white-box \n",
      "models, black-box models and gray-box models. Both black-\n",
      "box models and gray-box models are open for finetuning with \n",
      "new data from the user.  \n",
      "• \n",
      "White-Box Models: These are theoretically closed-form \n",
      "analytical formulas for the evaluated objective. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sections = {}\n",
    "current_section_title = \"Metadata (Title/Authors)\"  # Default section before first heading\n",
    "sections[current_section_title] = []\n",
    "\n",
    "# Sort filtered headings by page and vertical position\n",
    "filtered_headings.sort(key=lambda x: (x['page'], x['bbox'][1]))\n",
    "\n",
    "heading_index = 0\n",
    "bbox_tolerance = 10  # Relaxed vertical tolerance\n",
    "\n",
    "for block_data in extracted_blocks:\n",
    "    block_page = block_data['page']\n",
    "    block_bbox = block_data['bbox']\n",
    "    block_text = block_data['text'].strip()\n",
    "\n",
    "    # If there are headings left to match\n",
    "    if heading_index < len(filtered_headings):\n",
    "        current_heading = filtered_headings[heading_index]\n",
    "        heading_page = current_heading['page']\n",
    "        heading_bbox = current_heading['bbox']\n",
    "        heading_text = current_heading['text'].strip()\n",
    "\n",
    "        # Relaxed matching: same page + similar Y position + heading text appears in block text\n",
    "        if (\n",
    "            block_page == heading_page\n",
    "            and abs(block_bbox[1] - heading_bbox[1]) < bbox_tolerance\n",
    "            and heading_text.lower() in block_text.lower()\n",
    "        ):\n",
    "            # Start a new section\n",
    "            current_section_title = heading_text\n",
    "            sections[current_section_title] = []\n",
    "            heading_index += 1\n",
    "            continue  # Skip adding heading text itself to content\n",
    "\n",
    "    # Otherwise, treat as part of current section\n",
    "    sections[current_section_title].append(block_text)\n",
    "\n",
    "# Combine text blocks per section\n",
    "section_contents = {\n",
    "    title: \"\\n\".join(blocks) for title, blocks in sections.items()\n",
    "}\n",
    "\n",
    "# Print first few characters of each section for verification\n",
    "for title, content in section_contents.items():\n",
    "    print(f\"--- Section: {title} ---\")\n",
    "    print(content[:500] + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3917b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Section: Metadata (Title/Authors) ---\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\n",
      "PE-GPT: A New Paradigm for Power Electronics Design\n",
      "Article in IEEE Transactions on Industrial Electronics · October 2024\n",
      "DOI: 10.1109/TIE.2024.3454408\n",
      "CITATIONS 8\n",
      "READS 1,067\n",
      "8 authors, including:\n",
      "Fanfan Lin\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute...\n",
      "\n",
      "--- Cleaned Section: 54 PUBLICATIONS   561 CITATIONS ---\n",
      "University of Arkansas at Fayetteville...\n",
      "\n",
      "--- Cleaned Section: 54 PUBLICATIONS   577 CITATIONS ---\n",
      "...\n",
      "\n",
      "--- Cleaned Section: 178 PUBLICATIONS   3,120 CITATIONS ---\n",
      "2,111 PUBLICATIONS 115,701 CITATIONS\n",
      "All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\n",
      "The user has requested enhancement of the downloaded file.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "Abstract— Large Language Models (LLMs) have shown exciting potential in powering the growth of many industries, yet their adoption in the power electronics (PE) sector is hindered by a lack of specialized PE technical expertise and challenges in processing PE-specific d...\n",
      "\n",
      "--- Cleaned Section: INTRODUCTION ---\n",
      "O accelerate the world’s transition to sustainable energy, power electronics (PE) devices play important roles. Unlike traditional fossil energy from which we can expect steady and predictable power, most of the sustainable energy sources exhibit the characteristic of intermittency, including solar and wind energy [1]. This intermittency, resulted from day-night cycle for solar energy and wind speed for wind energy, poses serious challenges for grid stability and reliability. To address this cha...\n",
      "\n",
      "--- Cleaned Section: II. PRELIMINARIES: LARGE LANGUAGE MODELS ---\n",
      "...\n",
      "\n",
      "--- Cleaned Section: introduction of the Transformer model in 2017 [10]. Its core ---\n",
      "capture long-term dependencies in language.\n",
      "Following that, Google introduced Bidirectional Encoder Representations from Transformers (BERT) in 2018 which overcame the preceding language models’ constraints in unidirectional language modeling. In 2023, OpenAI trained GPT-4 on extensive Internet datasets, which has 1.76 trillion parameters, pushing the development of LLM to a new height. Although GPT-4 is capable of processing both text and image data formats, its direct application to technical ...\n",
      "\n",
      "--- Cleaned Section: A. Hybrid Framework of PE-GPT ---\n",
      "Fig. 2 presents the proposed hybrid framework of multimodal PE-GPT. There are two types of acceptable input data from the user: textual queries and PE-specific fine-tuning data.\n",
      "All textual queries first go through the RAG process, indicated by the green dotted section in Fig. 2, to enhance the domain expertise of the original LLM agent. To facilitate RAG, a specialized PE knowledge base is constructed beforehand based on the application-specific needs. For instance, if the PE- GPT is expected t...\n",
      "\n",
      "--- Cleaned Section: B. Retrieval Augmented Generation (RAG) ---\n",
      "LLM agents such as GPT-4 and Llama have been pretrained on vast and general corpora. The pretraining equips them with rich general domain knowledge. However, it also inherently limits their depth in specific domain expertise.\n",
      "The analysis in Section II explores three techniques: prompt engineering, RAG, and model fine-tuning. Among these, PE- GPT notably leverages RAG to tailor LLM for PE-specific applications. RAG effectively bridges the knowledge gap of LLMs through retrieving relevant excerpt...\n",
      "\n",
      "--- Cleaned Section: C. Model Zoo and the Adopted PANN Model Inside ---\n",
      "As elaborated in PE-GPT Workflow 1, the role of the Model Zoo is function evaluation, upon which the metaheuristic algorithm iterates until the optimal design outcome is found.\n",
      "Model Zoo consists of three types of models: white-box models, black-box models and gray-box models. Both black- box models and gray-box models are open for finetuning with new data from the user. • White-Box Models: These are theoretically closed-form analytical formulas for the evaluated objective. • Black-Box Models: T...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# === CLEAN AND NORMALIZE SECTION TEXTS ===\n",
    "cleaned_section_contents = {}\n",
    "\n",
    "# Pattern to detect lines that are just page numbers\n",
    "page_number_pattern = re.compile(r'^\\s*\\d+\\s*$')\n",
    "\n",
    "# Minimum line length for regular sections\n",
    "min_line_length = 30\n",
    "\n",
    "for title, blocks in sections.items():  # Using sections built from filtered_headings\n",
    "    cleaned_lines = []\n",
    "    for line in blocks:\n",
    "        cleaned_line = line.strip()\n",
    "        if not cleaned_line:\n",
    "            continue\n",
    "        # Skip lines that are just page numbers\n",
    "        if page_number_pattern.match(cleaned_line):\n",
    "            continue\n",
    "        # Normalize whitespace\n",
    "        cleaned_line = re.sub(r'\\s+', ' ', cleaned_line)\n",
    "        # Preserve short lines in Metadata / ABSTRACT\n",
    "        if title in [\"Metadata (Title/Authors)\", \"ABSTRACT\"]:\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        else:\n",
    "            # For other sections, apply minimum length heuristic\n",
    "            if len(cleaned_line) >= min_line_length:\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    # Combine back into single string per section\n",
    "    cleaned_section_contents[title] = '\\n'.join(cleaned_lines)\n",
    "\n",
    "# Replace original section_contents with cleaned version\n",
    "section_contents = cleaned_section_contents\n",
    "\n",
    "# Print first 500 characters of each cleaned section for verification\n",
    "for title, content in section_contents.items():\n",
    "    print(f\"--- Cleaned Section: {title} ---\")\n",
    "    print(content[:500] + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aecbcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tables on Page 1 ---\n",
      "Table 1:\n",
      "['See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\\nPE-GPT: A New Paradigm for Power Electronics Design\\nArticle in IEEE Transactions on Industrial Electronics · October 2024\\nDOI: 10.1109/TIE.2024.3454408\\nCITATIONS READS\\n8 1,067\\n8 authors, including:\\nFanfan Lin Xinze Li\\nZhejiang University - University of Illinois Urbana-Champaign Institute University of Arkansas at Fayetteville\\n54 PUBLICATIONS 561 CITATIONS 54 PUBLICATIONS 577 CITATIONS\\nSEE PROFILE SEE PROFILE\\nJ.J. Rodriguez-Andina Josep M. Guerrero\\nUniversity of Vigo Zhejiang University\\n178 PUBLICATIONS 3,120 CITATIONS 2,111 PUBLICATIONS 115,701 CITATIONS\\nSEE PROFILE SEE PROFILE', None]\n",
      "['All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\\nThe user has requested enhancement of the downloaded file.', '']\n",
      "--------------------\n",
      "--- Tables on Page 2 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 3 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 4 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 5 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 6 ---\n",
      "Table 1:\n",
      "['', '', '', '']\n",
      "--------------------\n",
      "Table 2:\n",
      "[None, 's1\\n+\\nvp-\\ns2', None]\n",
      "[None, '', None]\n",
      "[None, None, '+\\nvp-\\ns2']\n",
      "['', None, None]\n",
      "['', '', None]\n",
      "--------------------\n",
      "Table 3:\n",
      "['1 q3\\n+\\nvs-\\n2 q4', None, None, None]\n",
      "['', None, '', None]\n",
      "['', None, None, None]\n",
      "[None, None, None, '']\n",
      "['', 'q4', None, None]\n",
      "['2', None, '', None]\n",
      "--------------------\n",
      "--- Tables on Page 7 ---\n",
      "Table 1:\n",
      "['s\\n4']\n",
      "['s\\n3']\n",
      "--------------------\n",
      "Table 2:\n",
      "['q\\n4']\n",
      "['q\\n3']\n",
      "--------------------\n",
      "Table 3:\n",
      "['ID', 'User’s Textual Input', None, None, 'PE-GPT’s Response', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Capability and', '']\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Workflow (WF)', None]\n",
      "['1', '', 'INITIALIZE DESIGN PROCESS:', '', '', \"Of course, I'd be happy to assist (\", None, None, None, None, None, None, None, None, None, '…)', '', None, None, None, None, None, None, None, None, None, None, None, 'Ø PE-KR', None]\n",
      "[None, None, 'Hi, can you help me design the', None, None, \"If there are any particular performance indicators or constraints you're aiming for, such as efficiency, thermal\", None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, 'modulation strategy for my dual\\nactive bridge converter?', None, None, 'performance, control complexity, or ease of implementation, please let me know. Your detailed requirements\\nwill help me provide you with the most suitable modulation strategy.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "['2', 'DEFINE DESIGN\\nREQUIREMENTS\\nCan you recommend a phase shift\\nmodulation for a lowest inductor\\ncurrent for the full operating range?', None, None, '', '', '(…),', None, 'I would recommend the Triple Phase Shift (TPS) modulation method. The TPS method is effective in', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Ø PE-KR', None]\n",
      "[None, None, None, None, None, 'reducing the current stress on the inductor by allowing independent control of the phase shifts between the', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'primary and secondary sides of the converter, as well as between the active bridges. This can lead to a reduction\\nin the RMS current, which in turn minimizes the current stress on the inductor.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'To further refine the recommendation and', None, None, None, None, None, None, None, None, None, None, None, '(…)', ', could you please provide the following operating conditions', None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, '(…)', None, '', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "['3', 'PROVIDE OPERATING\\nCONDITIONS:\\nThe operating conditions are V1 =\\n200 V, V2 = 160 V, Prated=1000 W', None, None, '', 'Under the TPS modulation strategy, the optimal D1 is (', None, None, None, None, None, None, None, None, None, None, None, None, None, '…),', 'the number of switches that achieve zero-voltage', None, None, None, None, None, None, None, 'Ø MMP\\nØ PE-KR\\no WF1', None]\n",
      "[None, None, None, None, None, 'turn-on is 8.', None, None, None, None, None, '(…)', 'At rated power level (Prated = 1000 W), the peak-to-peak current is 19.05 A. Under light load', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'conditions (PL = 300 W),', None, None, None, None, None, None, None, '(…)', '', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'Under the SPS modulation strategy, the number of switches that achieve zero-voltage turn-on is', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '(…)', '. Under', None, None]\n",
      "[None, None, None, None, None, 'light load conditions (PL = 300 W),', None, None, None, None, None, None, None, None, None, '(…)', '', None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'The performance of current TPS modulation far exceeds that of conventional SPS.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "['4', 'PROVIDE FEEDBACK FOR\\nITERATIVE DESIGN\\nBut I think the soft switching range\\ncannot satisfy my application in TPS.', None, None, '', 'If the soft-switching range provided by the TPS modulation strategy does not meet the requirements of your', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Ø ASU\\nØ PE-KR', None]\n",
      "[None, None, None, None, None, 'application, I recommend considering the Five-Degree-of-Freedom (5DoF) modulation scheme. The 5DoF', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'modulation scheme is not a phase shift modulation method, but it offers superior performance in terms of soft-', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'switching range. The 5DoF modulation scheme provides enhanced control over the converter by independently', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'adjusting five different parameters:', None, None, None, None, None, None, None, None, None, '(…)', '. This level of control allows for an expanded ZVS and ZCS ranges,', None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, 'which can significantly improve efficiency, especially under varying load conditions.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '(…)', '', None, None, None, None]\n",
      "['5', '', 'CONFIRM MODULATION', '', '', 'Under the 5DoF modulation strategy, the optimal D1 is (', None, None, None, None, None, None, None, None, None, None, None, None, None, '…)', ', φ1 is designed to be', None, '(…)', None, ', the number of switches', None, None, None, 'Ø MMP\\no WF1', None]\n",
      "[None, None, 'STRATEGY', None, None, 'that achieve zero-voltage turn-on is', None, None, None, None, None, None, None, None, None, '(…', '). At rated power level(Prated=1000 W), the peak-to-peak current is', None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, 'OK, let us go ahead with 5DoF in the', None, None, '18.69 A.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, 'same operating conditions.', None, None, 'Under light load conditions (PL = 300 W),', None, None, None, None, None, None, None, None, None, None, None, '(…)', '', None, None, None, None, None, None, None, None, None, None, None]\n",
      "['6', '', 'ASK FOR MODEL FINE-TUNING', None, None, None, None, None, '', 'Absolutely, I can assist you with improving the design accuracy using your experimental data.\\nPlease upload three CSV files containing the following waveforms. (…)', 'Absolutely, I can assist you with improving the design accuracy using your experimental data.', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Ø MMP\\no WF3', None]\n",
      "[None, None, 'Now I have some experimental data. Can you', None, None, None, None, None, None, None, 'Please upload three CSV files containing the following waveforms.', None, None, None, None, None, None, None, None, None, None, '(…)', None, '', None, None, None, None, None, None]\n",
      "[None, None, 'help improve the design accuracy?', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "--------------------\n",
      "Table 4:\n",
      "['REQUIREMENTS']\n",
      "['Can you recommend a phase shift']\n",
      "['modulation for a lowest inductor']\n",
      "['current for the full operating range?']\n",
      "--------------------\n",
      "Table 5:\n",
      "['PROVIDE OPERATING']\n",
      "['CONDITIONS:']\n",
      "['The operating conditions are V1 =']\n",
      "['200 V, V2 = 160 V, Prated=1000 W']\n",
      "--------------------\n",
      "Table 6:\n",
      "['Ø MMP']\n",
      "['Ø PE-KR']\n",
      "['o WF1']\n",
      "--------------------\n",
      "Table 7:\n",
      "['PROVIDE FEEDBACK FOR']\n",
      "['ITERATIVE DESIGN']\n",
      "['But I think the soft switching range']\n",
      "['cannot satisfy my application in TPS.']\n",
      "--------------------\n",
      "Table 8:\n",
      "['Ø ASU']\n",
      "['Ø PE-KR']\n",
      "--------------------\n",
      "Table 9:\n",
      "['Ø MMP']\n",
      "['WF1']\n",
      "--------------------\n",
      "Table 10:\n",
      "['Ø MMP']\n",
      "['WF3']\n",
      "--------------------\n",
      "--- Tables on Page 8 ---\n",
      "Table 1:\n",
      "['2 L 67\\nSmoothly convergesR L R\\n1.6 L 65\\n1.2 Converge 63\\nOscillations n after 80th R\\nL\\n0.8 61\\nConverge after 50th\\nEpo\\n0 20 40 60 80 100', 'L\\nSmoothly convergesR L', '67\\n65\\n63', 'R', 'Initial θ\\n=500mΩ\\nL\\nL=65μH\\nn = 1.1']\n",
      "[None, '', None, None, None]\n",
      "[None, 'L', None, None, None]\n",
      "[None, 'Converge', None, 'R\\nL', None]\n",
      "[None, 'Oscillations n after 80th', None, None, None]\n",
      "[None, '', None, None, None]\n",
      "[None, 'Converge after 50th', None, None, None]\n",
      "--------------------\n",
      "Table 2:\n",
      "['Before\\nfine-tun', 'e', '', 'Modeling accuracy MSE val', None, None, None, None, None, None, None, None, None, None, '', '', None]\n",
      "[None, None, '', 'greatly improves', None, None, None, None, '', '', 'MSE\\ntest', None, None, None, None, None, None]\n",
      "['(Epoch=', '1) 0.6', '', '', None, None, None, None, '', '71%', '', '', '', '', '', '', None]\n",
      "[None, '', '', '', '', None, '', 'MSE\\ntrain', '', '', '', '', '', '', None, None, None]\n",
      "['', '0.3', '', '', '', None, '', '93%', '', '', '', '', '84%', '', '', '', None]\n",
      "['', '0', '', '', '', None, '', '', '', '', None, '', '', '', '', '', None]\n",
      "[None, None, '', '', '', None, '', None, '', '', None, '', '', None, None, None, None]\n",
      "['', '1 34 1 34 1 34 Ep', None, None, None, None, None, None, None, None, None, None, None, None, None, 'och', None]\n",
      "['', '', None, None, None, 'MSE\\ntest', None, None, None, None, None, None, None, None, None, '', None]\n",
      "[None, None, None, None, None, 'After fine-tuning', None, None, None, None, None, None, None, None, None, None, None]\n",
      "['', None, None, None, None, '(Epoch = 34) MSE val', None, None, None, None, None, None, None, None, None, None, None]\n",
      "['MSE\\ntrain', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '']\n",
      "--------------------\n",
      "Table 3:\n",
      "['1.2\\n0.8\\n0.4', 'MSE<0.03,\\nVariations<2e-6']\n",
      "--------------------\n",
      "Table 4:\n",
      "['12\\n8\\n4', '0.5\\n0.4\\n0.3']\n",
      "--------------------\n",
      "Table 5:\n",
      "['RR22 == 9944..5533%% PANN\\nHardware\\nt: 20 μs/div\\nwaveform\\ni: 2 A/div\\nL']\n",
      "['Before fine-tuning']\n",
      "--------------------\n",
      "Table 6:\n",
      "['RR22 == 9999..4411%% PANN\\nt: 20 μs/div RR22 iimmpprroovveess\\ni: 2 A/div bbyy 44..8888%%\\nL']\n",
      "['After fine-tuning']\n",
      "--------------------\n",
      "Table 7:\n",
      "['', '']\n",
      "--------------------\n",
      "--- Tables on Page 9 ---\n",
      "Table 1:\n",
      "[None, '', '']\n",
      "['', \"HR\\nRel 0. 1 5 M RR R P e E sp -G on P s T e ' i s s :\\n0 Ø Precise\\nØ Complete\\nFF RS Ø Efficient\", '']\n",
      "['', '', '']\n",
      "--------------------\n",
      "Table 2:\n",
      "['ID', 'User’s Textual Input', None, 'PE-GPT’s Response', None, None, None, None, None, None, None, None, 'Capability and', '']\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, 'Workflow (WF)', None]\n",
      "['1', 'INITIALIZE DESIGN PROCESS:\\nI want to design the output LC filter for a Buck converter. Can\\nyou help me?', None, \"Absolutely, I'd be glad to help with (…) To get started, could you share with\\nme the specific design objectives and requirements for your converter? For\\nexample, are you focusing on minimizing ripple, maximizing efficiency, or\\noptimizing for power density? Any details you provide will help me tailor my\\nassistance to best meet your needs.\", None, None, None, None, '(…)', 'To get started, could you share with', None, None, 'Ø PE-KR', None]\n",
      "['2', '', 'DEFINE DESIGN REQUIREMENTS\\nYes, I have two design requirements. Firstly, the output voltage\\nripple and inductor current ripple should be limited. Secondly,\\nhigh power density is expected.', 'Great, (…). To provide you with the best guidance, could you please share the\\nfollowing details:\\n1.The output voltage ripple constraint (%)\\n2. (…)', '(…)', '. To provide you with the best guidance, could you please share the', None, None, None, None, None, None, 'Ø ASU', None]\n",
      "[None, None, None, None, '', None, None, None, None, None, None, None, None, None]\n",
      "['3', '', 'PROVIDE OPERATING CONDITIONS:\\nVoltage ripple constraint is 0.5%, and current ripple constraint is\\n20%. And operating conditions are as follows: switching\\nfrequency is 50 kHz; input voltage is 200V; output voltage is 80V;\\noutput power is 800W.', 'Well received. (…) The optimal inductance L is designed to be 482.7 μH, and the\\noptimal capacitance C is designed to be 12.51 μF. The output voltage ripple and\\ninductor current ripple are 0.398 V (0.50%) and 1.992 A (19.92%), respectively.\\nThe output waveform of the inductor current in steady state under this operating\\ncondition is shown in the following figure: (…)', None, None, '(…)', 'The optimal inductance L is designed to be 482.7 μH, and the', None, None, None, None, 'Ø MMP\\no WF1', None]\n",
      "[None, None, None, None, None, None, None, None, None, '(…)', '', None, None, None]\n",
      "['4', 'ASK FOR RATIONALE\\nWhat is the rationale behind this design?', None, '(…) Output Voltage Ripple Constraint: Minimizing the output voltage ripple is\\ncritical for stable load operation. A smaller output ripple requires an adequately\\nsized capacitor, which directly influences the quality of the DC output.\\nInductor Current Ripple Constraint: Controlling the inductor current ripple is\\nessential for efficient energy transfer and minimizing losses. It also impacts the\\ninductor size—lower ripple percentages typically require larger inductors. (…)', 'Output Voltage Ripple Constraint: Minimizing the output voltage ripple is', None, None, None, None, None, None, None, 'Ø PE-KR', None]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, '(…)', None, None]\n",
      "['5', '', 'ASK FOR FURTHER ANALYSIS\\nCan you analyze the harmonic components of the inductor current\\nand the capacitor voltage?', 'The harmonics of inductor current are 10.0 A (DC), 0.8 A (1st), 0.123 A (2nd),\\n0.0549 A (3rd)… The harmonics of output voltage are 80.0 V (DC), 0.204 V (1st),\\n0.0157 V (2nd), 0.00466 V (3rd)…', None, None, None, None, None, None, None, None, 'Ø MMP', None]\n",
      "['6', '', 'ASK FOR SIMULATION VERIFICATION\\nThe design sounds reasonable. Can we verify this design in PLECS', 'Waiting... PLECS is starting up... The PLECS simulation is running…\\n? Complete! (…)', None, None, None, None, None, None, None, None, 'o WF2', None]\n",
      "--------------------\n",
      "Table 3:\n",
      "['Ø MMP']\n",
      "['o WF1']\n",
      "--------------------\n",
      "--- Tables on Page 10 ---\n",
      "Table 1:\n",
      "['MMSSEE == 22..99ee--33\\nPANN\\nv: 0.2 V/div\\no']\n",
      "['']\n",
      "['t: 20 μs/div PLECS\\nwaveform']\n",
      "['Δv o% = 0.49% ≤ v ripple (0.5%)']\n",
      "--------------------\n",
      "Table 2:\n",
      "['MMSSEE == 33..0022ee--44\\nPANN']\n",
      "['']\n",
      "['i: 2 A/div t: 20 μs/div\\nL']\n",
      "['Δi L% = 20% ≤ i ripple (20%)']\n",
      "--------------------\n",
      "Table 3:\n",
      "['', '']\n",
      "--------------------\n",
      "--- Tables on Page 11 ---\n",
      "Table 1:\n",
      "['']\n",
      "['Efficiency η Current stress i /A\\n0% pp 30\\n7% 1.8% 5DoF-η\\n4.9%\\n97.53% 24\\n1 5 % % T P S- η η Ø Optimal i pp T 5D P o S F -i - p i p pp 18\\nP S- SPS-i pp 12\\n9% S\\n6\\n3%\\n50 100 300 500 700 900 P L /W']\n",
      "['P L /W 50 100 200 300 400 500 600 700 800 900 100']\n",
      "--------------------\n",
      "Table 2:\n",
      "['1.8% 5DoF-η', None, None, None, None, None, None, None, None]\n",
      "['%\\n97.53%\\nS- η Ø Optimal i 5DoF-i pp', None, None, None, None, None, None, None, None]\n",
      "['P pp TPS-i\\nT η pp', None, None, None, None, '', None, '', '']\n",
      "['S-\\nP SPS-i pp\\nS', None, None, None, '', '', '', '', '']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "['', '', '', '', '', '', '', '', '']\n",
      "--------------------\n",
      "Table 3:\n",
      "['TPS 4 6 6 6 6 8 8 8 8 8 8\\n5DoF 8 8 8 8 8 8 8 8 8 8 8']\n",
      "['TPS 0 0 0 0 2 2 2 0 0 0 0\\n5DoF 0 0 2 4 3 2 2 0 0 0 0']\n",
      "--------------------\n",
      "Table 4:\n",
      "['']\n",
      "['0% Efficiency η Current stress i pp /A 30\\n5DoF-η\\n7%\\n2.9% 97.16% 24\\n1 5 % % 7.1% S- η Optimal i pp T 5D P o S F -i - p i p pp 18\\nS- η S P SPS-i pp 12\\n9% T P\\n6\\n3%\\n50 100 300 500 700 900 P L /']\n",
      "['P L /W 50 100 200 300 400 500 600 700 800 900 100']\n",
      "--------------------\n",
      "Table 5:\n",
      "['5DoF-η', None, None, None, None, None, None, None, None, None]\n",
      "['2.9%\\n.1%', '97.16%', None, None, None, None, None, None, None, None]\n",
      "['S- η Optimal i pp T 5D P o S F -i - p i p pp\\nηP SPS-i\\npp', None, None, None, None, None, None, None, None, None]\n",
      "['S-S\\nP\\nT', None, None, '', '', '', '', '', '', '']\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "--------------------\n",
      "Table 6:\n",
      "['TPS 4 4 8 8 8 8 8 8 8 8 8\\n5DoF 8 8 8 8 8 8 8 8 8 8 8']\n",
      "['TPS 0 0 0 0 2 2 4 2 0 0 0\\n5DoF 0 2 2 4 2 2 4 2 0 0 0']\n",
      "--------------------\n",
      "--- Tables on Page 12 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 13 ---\n",
      "No tables found on this page.\n",
      "--- Tables on Page 14 ---\n",
      "No tables found on this page.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_path = 'file.pdf'\n",
    "extracted_tables_data = [] # Store extracted tables with page number\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_num, page in enumerate(pdf.pages):\n",
    "        print(f\"--- Tables on Page {page_num + 1} ---\")\n",
    "        tables_on_page = page.extract_tables()\n",
    "        if tables_on_page:\n",
    "            extracted_tables_data.append({'page': page_num + 1, 'tables': tables_on_page})\n",
    "            for table_num, table in enumerate(tables_on_page):\n",
    "                print(f\"Table {table_num + 1}:\")\n",
    "                for row in table:\n",
    "                    print(row)\n",
    "                print(\"-\" * 20)\n",
    "        else:\n",
    "            print(\"No tables found on this page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61381f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tables on Page 1 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "| See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\n",
      "PE-GPT: A New Paradigm for Power Electronics Design\n",
      "Article in IEEE Transactions on Industrial Electronics · October 2024\n",
      "DOI: 10.1109/TIE.2024.3454408\n",
      "CITATIONS READS\n",
      "8 1,067\n",
      "8 authors, including:\n",
      "Fanfan Lin Xinze Li\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute University of Arkansas at Fayetteville\n",
      "54 PUBLICATIONS 561 CITATIONS 54 PUBLICATIONS 577 CITATIONS\n",
      "SEE PROFILE SEE PROFILE\n",
      "J.J. Rodriguez-Andina Josep M. Guerrero\n",
      "University of Vigo Zhejiang University\n",
      "178 PUBLICATIONS 3,120 CITATIONS 2,111 PUBLICATIONS 115,701 CITATIONS\n",
      "SEE PROFILE SEE PROFILE |  |\n",
      "|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n",
      "| All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\n",
      "The user has requested enhancement of the downloaded file. |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 6 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "|  |  |  |  |\n",
      "|--|--|--|--|\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "|  | s1\n",
      "+\n",
      "vp-\n",
      "s2 |  |\n",
      "|--|-------------|--|\n",
      "|  |  |  |\n",
      "|  |  | +\n",
      "vp-\n",
      "s2 |\n",
      "|  |  |  |\n",
      "|  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "| 1 q3\n",
      "+\n",
      "vs-\n",
      "2 q4 |  |  |  |\n",
      "|-----------------|--|--|--|\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  |  |  |  |\n",
      "|  | q4 |  |  |\n",
      "| 2 |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 7 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "| s\n",
      "4 |\n",
      "|-----|\n",
      "| s\n",
      "3 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "| q\n",
      "4 |\n",
      "|-----|\n",
      "| q\n",
      "3 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "| ID | User’s Textual Input |  |  | PE-GPT’s Response |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Capability and |  |\n",
      "|----|----------------------|--|--|-------------------|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|----------------|--|\n",
      "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Workflow (WF) |  |\n",
      "| 1 |  | INITIALIZE DESIGN PROCESS: |  |  | Of course, I'd be happy to assist ( |  |  |  |  |  |  |  |  |  | …) |  |  |  |  |  |  |  |  |  |  |  |  | Ø PE-KR |  |\n",
      "|  |  | Hi, can you help me design the |  |  | If there are any particular performance indicators or constraints you're aiming for, such as efficiency, thermal |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  | modulation strategy for my dual\n",
      "active bridge converter? |  |  | performance, control complexity, or ease of implementation, please let me know. Your detailed requirements\n",
      "will help me provide you with the most suitable modulation strategy. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| 2 | DEFINE DESIGN\n",
      "REQUIREMENTS\n",
      "Can you recommend a phase shift\n",
      "modulation for a lowest inductor\n",
      "current for the full operating range? |  |  |  |  | (…), |  | I would recommend the Triple Phase Shift (TPS) modulation method. The TPS method is effective in |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Ø PE-KR |  |\n",
      "|  |  |  |  |  | reducing the current stress on the inductor by allowing independent control of the phase shifts between the |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | primary and secondary sides of the converter, as well as between the active bridges. This can lead to a reduction\n",
      "in the RMS current, which in turn minimizes the current stress on the inductor. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | To further refine the recommendation and |  |  |  |  |  |  |  |  |  |  |  | (…) | , could you please provide the following operating conditions |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | (…) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| 3 | PROVIDE OPERATING\n",
      "CONDITIONS:\n",
      "The operating conditions are V1 =\n",
      "200 V, V2 = 160 V, Prated=1000 W |  |  |  | Under the TPS modulation strategy, the optimal D1 is ( |  |  |  |  |  |  |  |  |  |  |  |  |  | …), | the number of switches that achieve zero-voltage |  |  |  |  |  |  |  | Ø MMP\n",
      "Ø PE-KR\n",
      "o WF1 |  |\n",
      "|  |  |  |  |  | turn-on is 8. |  |  |  |  |  | (…) | At rated power level (Prated = 1000 W), the peak-to-peak current is 19.05 A. Under light load |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | conditions (PL = 300 W), |  |  |  |  |  |  |  | (…) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | Under the SPS modulation strategy, the number of switches that achieve zero-voltage turn-on is |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | (…) | . Under |  |  |\n",
      "|  |  |  |  |  | light load conditions (PL = 300 W), |  |  |  |  |  |  |  |  |  | (…) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | The performance of current TPS modulation far exceeds that of conventional SPS. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| 4 | PROVIDE FEEDBACK FOR\n",
      "ITERATIVE DESIGN\n",
      "But I think the soft switching range\n",
      "cannot satisfy my application in TPS. |  |  |  | If the soft-switching range provided by the TPS modulation strategy does not meet the requirements of your |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Ø ASU\n",
      "Ø PE-KR |  |\n",
      "|  |  |  |  |  | application, I recommend considering the Five-Degree-of-Freedom (5DoF) modulation scheme. The 5DoF |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | modulation scheme is not a phase shift modulation method, but it offers superior performance in terms of soft- |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | switching range. The 5DoF modulation scheme provides enhanced control over the converter by independently |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | adjusting five different parameters: |  |  |  |  |  |  |  |  |  | (…) | . This level of control allows for an expanded ZVS and ZCS ranges, |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | which can significantly improve efficiency, especially under varying load conditions. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | (…) |  |  |  |  |  |\n",
      "| 5 |  | CONFIRM MODULATION |  |  | Under the 5DoF modulation strategy, the optimal D1 is ( |  |  |  |  |  |  |  |  |  |  |  |  |  | …) | , φ1 is designed to be |  | (…) |  | , the number of switches |  |  |  | Ø MMP\n",
      "o WF1 |  |\n",
      "|  |  | STRATEGY |  |  | that achieve zero-voltage turn-on is |  |  |  |  |  |  |  |  |  | (… | ). At rated power level(Prated=1000 W), the peak-to-peak current is |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  | OK, let us go ahead with 5DoF in the |  |  | 18.69 A. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  | same operating conditions. |  |  | Under light load conditions (PL = 300 W), |  |  |  |  |  |  |  |  |  |  |  | (…) |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| 6 |  | ASK FOR MODEL FINE-TUNING |  |  |  |  |  |  | Absolutely, I can assist you with improving the design accuracy using your experimental data.\n",
      "Please upload three CSV files containing the following waveforms. (…) | Absolutely, I can assist you with improving the design accuracy using your experimental data. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Ø MMP\n",
      "o WF3 |  |\n",
      "|  |  | Now I have some experimental data. Can you |  |  |  |  |  |  |  | Please upload three CSV files containing the following waveforms. |  |  |  |  |  |  |  |  |  |  | (…) |  |  |  |  |  |  |  |  |\n",
      "|  |  | help improve the design accuracy? |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 4:\n",
      "\n",
      "| REQUIREMENTS |\n",
      "|--------------|\n",
      "| Can you recommend a phase shift |\n",
      "| modulation for a lowest inductor |\n",
      "| current for the full operating range? |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 5:\n",
      "\n",
      "| PROVIDE OPERATING |\n",
      "|-------------------|\n",
      "| CONDITIONS: |\n",
      "| The operating conditions are V1 = |\n",
      "| 200 V, V2 = 160 V, Prated=1000 W |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 6:\n",
      "\n",
      "| Ø MMP |\n",
      "|-------|\n",
      "| Ø PE-KR |\n",
      "| o WF1 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 7:\n",
      "\n",
      "| PROVIDE FEEDBACK FOR |\n",
      "|----------------------|\n",
      "| ITERATIVE DESIGN |\n",
      "| But I think the soft switching range |\n",
      "| cannot satisfy my application in TPS. |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 8:\n",
      "\n",
      "| Ø ASU |\n",
      "|-------|\n",
      "| Ø PE-KR |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 9:\n",
      "\n",
      "| Ø MMP |\n",
      "|-------|\n",
      "| WF1 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 10:\n",
      "\n",
      "| Ø MMP |\n",
      "|-------|\n",
      "| WF3 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 8 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "| 2 L 67\n",
      "Smoothly convergesR L R\n",
      "1.6 L 65\n",
      "1.2 Converge 63\n",
      "Oscillations n after 80th R\n",
      "L\n",
      "0.8 61\n",
      "Converge after 50th\n",
      "Epo\n",
      "0 20 40 60 80 100 | L\n",
      "Smoothly convergesR L | 67\n",
      "65\n",
      "63 | R | Initial θ\n",
      "=500mΩ\n",
      "L\n",
      "L=65μH\n",
      "n = 1.1 |\n",
      "|----------------------------------------------------------------------------------------------------------------------------------------|-------------------------|----------|---|-----------------------------------|\n",
      "|  |  |  |  |  |\n",
      "|  | L |  |  |  |\n",
      "|  | Converge |  | R\n",
      "L |  |\n",
      "|  | Oscillations n after 80th |  |  |  |\n",
      "|  |  |  |  |  |\n",
      "|  | Converge after 50th |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "| Before\n",
      "fine-tun | e |  | Modeling accuracy MSE val |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|-----------------|---|--|---------------------------|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
      "|  |  |  | greatly improves |  |  |  |  |  |  | MSE\n",
      "test |  |  |  |  |  |  |\n",
      "| (Epoch= | 1) 0.6 |  |  |  |  |  |  |  | 71% |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  | MSE\n",
      "train |  |  |  |  |  |  |  |  |  |\n",
      "|  | 0.3 |  |  |  |  |  | 93% |  |  |  |  | 84% |  |  |  |  |\n",
      "|  | 0 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  | 1 34 1 34 1 34 Ep |  |  |  |  |  |  |  |  |  |  |  |  |  | och |  |\n",
      "|  |  |  |  |  | MSE\n",
      "test |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | After fine-tuning |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | (Epoch = 34) MSE val |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| MSE\n",
      "train |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "| 1.2\n",
      "0.8\n",
      "0.4 | MSE<0.03,\n",
      "Variations<2e-6 |\n",
      "|-------------|---------------------------|\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 4:\n",
      "\n",
      "| 12\n",
      "8\n",
      "4 | 0.5\n",
      "0.4\n",
      "0.3 |\n",
      "|--------|-------------|\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 5:\n",
      "\n",
      "| RR22 == 9944..5533%% PANN\n",
      "Hardware\n",
      "t: 20 μs/div\n",
      "waveform\n",
      "i: 2 A/div\n",
      "L |\n",
      "|-----------------------------------------------------------------------|\n",
      "| Before fine-tuning |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 6:\n",
      "\n",
      "| RR22 == 9999..4411%% PANN\n",
      "t: 20 μs/div RR22 iimmpprroovveess\n",
      "i: 2 A/div bbyy 44..8888%%\n",
      "L |\n",
      "|-------------------------------------------------------------------------------------------|\n",
      "| After fine-tuning |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 7:\n",
      "\n",
      "|  |  |\n",
      "|--|--|\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 9 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "|  |  |  |\n",
      "|--|--|--|\n",
      "|  | HR\n",
      "Rel 0. 1 5 M RR R P e E sp -G on P s T e ' i s s :\n",
      "0 Ø Precise\n",
      "Ø Complete\n",
      "FF RS Ø Efficient |  |\n",
      "|  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "| ID | User’s Textual Input |  | PE-GPT’s Response |  |  |  |  |  |  |  |  | Capability and |  |\n",
      "|----|----------------------|--|-------------------|--|--|--|--|--|--|--|--|----------------|--|\n",
      "|  |  |  |  |  |  |  |  |  |  |  |  | Workflow (WF) |  |\n",
      "| 1 | INITIALIZE DESIGN PROCESS:\n",
      "I want to design the output LC filter for a Buck converter. Can\n",
      "you help me? |  | Absolutely, I'd be glad to help with (…) To get started, could you share with\n",
      "me the specific design objectives and requirements for your converter? For\n",
      "example, are you focusing on minimizing ripple, maximizing efficiency, or\n",
      "optimizing for power density? Any details you provide will help me tailor my\n",
      "assistance to best meet your needs. |  |  |  |  | (…) | To get started, could you share with |  |  | Ø PE-KR |  |\n",
      "| 2 |  | DEFINE DESIGN REQUIREMENTS\n",
      "Yes, I have two design requirements. Firstly, the output voltage\n",
      "ripple and inductor current ripple should be limited. Secondly,\n",
      "high power density is expected. | Great, (…). To provide you with the best guidance, could you please share the\n",
      "following details:\n",
      "1.The output voltage ripple constraint (%)\n",
      "2. (…) | (…) | . To provide you with the best guidance, could you please share the |  |  |  |  |  |  | Ø ASU |  |\n",
      "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n",
      "| 3 |  | PROVIDE OPERATING CONDITIONS:\n",
      "Voltage ripple constraint is 0.5%, and current ripple constraint is\n",
      "20%. And operating conditions are as follows: switching\n",
      "frequency is 50 kHz; input voltage is 200V; output voltage is 80V;\n",
      "output power is 800W. | Well received. (…) The optimal inductance L is designed to be 482.7 μH, and the\n",
      "optimal capacitance C is designed to be 12.51 μF. The output voltage ripple and\n",
      "inductor current ripple are 0.398 V (0.50%) and 1.992 A (19.92%), respectively.\n",
      "The output waveform of the inductor current in steady state under this operating\n",
      "condition is shown in the following figure: (…) |  |  | (…) | The optimal inductance L is designed to be 482.7 μH, and the |  |  |  |  | Ø MMP\n",
      "o WF1 |  |\n",
      "|  |  |  |  |  |  |  |  |  | (…) |  |  |  |  |\n",
      "| 4 | ASK FOR RATIONALE\n",
      "What is the rationale behind this design? |  | (…) Output Voltage Ripple Constraint: Minimizing the output voltage ripple is\n",
      "critical for stable load operation. A smaller output ripple requires an adequately\n",
      "sized capacitor, which directly influences the quality of the DC output.\n",
      "Inductor Current Ripple Constraint: Controlling the inductor current ripple is\n",
      "essential for efficient energy transfer and minimizing losses. It also impacts the\n",
      "inductor size—lower ripple percentages typically require larger inductors. (…) | Output Voltage Ripple Constraint: Minimizing the output voltage ripple is |  |  |  |  |  |  |  | Ø PE-KR |  |\n",
      "|  |  |  |  |  |  |  |  |  |  |  | (…) |  |  |\n",
      "| 5 |  | ASK FOR FURTHER ANALYSIS\n",
      "Can you analyze the harmonic components of the inductor current\n",
      "and the capacitor voltage? | The harmonics of inductor current are 10.0 A (DC), 0.8 A (1st), 0.123 A (2nd),\n",
      "0.0549 A (3rd)… The harmonics of output voltage are 80.0 V (DC), 0.204 V (1st),\n",
      "0.0157 V (2nd), 0.00466 V (3rd)… |  |  |  |  |  |  |  |  | Ø MMP |  |\n",
      "| 6 |  | ASK FOR SIMULATION VERIFICATION\n",
      "The design sounds reasonable. Can we verify this design in PLECS | Waiting... PLECS is starting up... The PLECS simulation is running…\n",
      "? Complete! (…) |  |  |  |  |  |  |  |  | o WF2 |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "| Ø MMP |\n",
      "|-------|\n",
      "| o WF1 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 10 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "| MMSSEE == 22..99ee--33\n",
      "PANN\n",
      "v: 0.2 V/div\n",
      "o |\n",
      "|--------------------------------------------|\n",
      "|  |\n",
      "| t: 20 μs/div PLECS\n",
      "waveform |\n",
      "| Δv o% = 0.49% ≤ v ripple (0.5%) |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "| MMSSEE == 33..0022ee--44\n",
      "PANN |\n",
      "|-------------------------------|\n",
      "|  |\n",
      "| i: 2 A/div t: 20 μs/div\n",
      "L |\n",
      "| Δi L% = 20% ≤ i ripple (20%) |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "|  |  |\n",
      "|--|--|\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Tables on Page 11 ---\n",
      "\n",
      "\n",
      "Table 1:\n",
      "\n",
      "|  |\n",
      "|--|\n",
      "| Efficiency η Current stress i /A\n",
      "0% pp 30\n",
      "7% 1.8% 5DoF-η\n",
      "4.9%\n",
      "97.53% 24\n",
      "1 5 % % T P S- η η Ø Optimal i pp T 5D P o S F -i - p i p pp 18\n",
      "P S- SPS-i pp 12\n",
      "9% S\n",
      "6\n",
      "3%\n",
      "50 100 300 500 700 900 P L /W |\n",
      "| P L /W 50 100 200 300 400 500 600 700 800 900 100 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 2:\n",
      "\n",
      "| 1.8% 5DoF-η |  |  |  |  |  |  |  |  |\n",
      "|-------------|--|--|--|--|--|--|--|--|\n",
      "| %\n",
      "97.53%\n",
      "S- η Ø Optimal i 5DoF-i pp |  |  |  |  |  |  |  |  |\n",
      "| P pp TPS-i\n",
      "T η pp |  |  |  |  |  |  |  |  |\n",
      "| S-\n",
      "P SPS-i pp\n",
      "S |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 3:\n",
      "\n",
      "| TPS 4 6 6 6 6 8 8 8 8 8 8\n",
      "5DoF 8 8 8 8 8 8 8 8 8 8 8 |\n",
      "|------------------------------------------------------|\n",
      "| TPS 0 0 0 0 2 2 2 0 0 0 0\n",
      "5DoF 0 0 2 4 3 2 2 0 0 0 0 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 4:\n",
      "\n",
      "|  |\n",
      "|--|\n",
      "| 0% Efficiency η Current stress i pp /A 30\n",
      "5DoF-η\n",
      "7%\n",
      "2.9% 97.16% 24\n",
      "1 5 % % 7.1% S- η Optimal i pp T 5D P o S F -i - p i p pp 18\n",
      "S- η S P SPS-i pp 12\n",
      "9% T P\n",
      "6\n",
      "3%\n",
      "50 100 300 500 700 900 P L / |\n",
      "| P L /W 50 100 200 300 400 500 600 700 800 900 100 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 5:\n",
      "\n",
      "| 5DoF-η |  |  |  |  |  |  |  |  |  |\n",
      "|--------|--|--|--|--|--|--|--|--|--|\n",
      "| 2.9%\n",
      ".1% | 97.16% |  |  |  |  |  |  |  |  |\n",
      "| S- η Optimal i pp T 5D P o S F -i - p i p pp\n",
      "ηP SPS-i\n",
      "pp |  |  |  |  |  |  |  |  |  |\n",
      "| S-S\n",
      "P\n",
      "T |  |  |  |  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  |  |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "Table 6:\n",
      "\n",
      "| TPS 4 4 8 8 8 8 8 8 8 8 8\n",
      "5DoF 8 8 8 8 8 8 8 8 8 8 8 |\n",
      "|------------------------------------------------------|\n",
      "| TPS 0 0 0 0 2 2 4 2 0 0 0\n",
      "5DoF 0 2 2 4 2 2 4 2 0 0 0 |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'extracted_tables_data' contains the extracted tables from the previous step\n",
    "\n",
    "def format_table_as_markdown(table):\n",
    "    \"\"\"\n",
    "    Formats a list of lists representing a table into a Markdown table string.\n",
    "    Handles potential None values in cells.\n",
    "    \"\"\"\n",
    "    if not table:\n",
    "        return \"\"\n",
    "\n",
    "    markdown_output = \"\"\n",
    "    # Add header row\n",
    "    header = table[0]\n",
    "    markdown_output += \"| \" + \" | \".join([str(cell) if cell is not None else \"\" for cell in header]) + \" |\\n\"\n",
    "    # Add separator line, ensuring it aligns with header length\n",
    "    separator = \"|-\" + \"-|-\".join(['-' * (len(str(cell)) if cell is not None else 0) for cell in header]) + \"-|\\n\"\n",
    "    # Adjust separator length if needed (basic approach)\n",
    "    if len(separator) < len(markdown_output.split('\\n')[0]):\n",
    "        separator = \"|---\" * len(header) + \"|\\n\"\n",
    "    markdown_output += separator\n",
    "\n",
    "    # Add data rows\n",
    "    for row in table[1:]:\n",
    "        markdown_output += \"| \" + \" | \".join([str(cell) if cell is not None else \"\" for cell in row]) + \" |\\n\"\n",
    "    return markdown_output\n",
    "\n",
    "formatted_tables = []\n",
    "if 'extracted_tables_data' in locals() and extracted_tables_data:\n",
    "    for page_data in extracted_tables_data:\n",
    "        page_num = page_data['page']\n",
    "        tables_on_page = page_data['tables']\n",
    "        formatted_tables.append(f\"\\n--- Tables on Page {page_num} ---\\n\")\n",
    "        for table_num, table in enumerate(tables_on_page):\n",
    "            formatted_tables.append(f\"\\nTable {table_num + 1}:\\n\")\n",
    "            formatted_tables.append(format_table_as_markdown(table))\n",
    "            formatted_tables.append(\"-\" * 20)\n",
    "        formatted_tables.append(\"\\n\")\n",
    "\n",
    "# Print the formatted tables to verify\n",
    "for ft in formatted_tables:\n",
    "    print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b1a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Figure Captions:\n",
      "Page 3: Fig. 1 compares the three techniques against five criteria:\n",
      "Page 3: Fig. 1. Comparison of three LLM customization techniques.\n",
      "Page 4: Fig. 2 presents the proposed hybrid framework of multimodal\n",
      "Page 4: Fig. 2. The proposed hybrid framework of PE-GPT.\n",
      "Page 6: Fig. 3. PANN for the function evaluation of metaheuristic algorithm.\n",
      "Page 6: Fig. 4. The DAB converter and its 5DoF modulation scheme.\n",
      "Page 7: Fig. 5. Hardware prototype.\n",
      "Page 8: Fig. 6. Fine-tuning of PANN: (a) the fine-tuning process; (b) exemplary\n",
      "Page 8: Fig. 7. MSEs of diverse data-driven models (repeated for 10 times) in in-\n",
      "Page 9: Fig. 8. Fine-tuning the hyperparameters of RAG (higher metrics indicating\n",
      "Page 10: Fig. 9. Waveforms of the designed buck converter: (a) vo; (b) iL.\n",
      "Page 10: Fig. 10. Comparisons among human experts, SOTA LLMs, and PE-GPT.\n",
      "Page 10: Fig. 11. vp, vs, and iL when V2 = 160 V and: (a) 300 W; (b) 600 W; (c) 1kW.\n",
      "Page 10: Fig. 12. vp, vs, and iL when V2 = 240 V and: (a) 300 W; (b) 500 W; (c) 1kW.\n",
      "Page 11: Fig. 13. ipp, efficiency η, ZVS, and ZCS performances of the SPS, CSO-\n",
      "Page 11: Fig. 14. ipp, efficiency η, ZVS, and ZCS performances of the SPS, CSO-\n",
      "Page 11: Fig. 11. In terms of voltage step-up conditions when V2 is 240\n",
      "Page 11: Fig. 15. Potential applications of PE-GPT in PE design workflow.\n"
     ]
    }
   ],
   "source": [
    "import fitz # PyMuPDF\n",
    "\n",
    "pdf_path = 'file.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "extracted_captions = []\n",
    "\n",
    "# Common patterns for figure captions\n",
    "caption_patterns = [\n",
    "    re.compile(r'^Figure\\s+\\d+[\\.\\s_]', re.IGNORECASE),\n",
    "    re.compile(r'^Fig\\.\\s+\\d+[\\.\\s_]', re.IGNORECASE),\n",
    "    re.compile(r'^Figure\\s+\\w\\.\\s+', re.IGNORECASE), # For captions like Figure A.\n",
    "    re.compile(r'^Fig\\.\\s+\\w\\.\\s+', re.IGNORECASE), # For captions like Fig. A.\n",
    "]\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text_blocks = page.get_text(\"blocks\") # Get text blocks with coordinates\n",
    "\n",
    "    for block in text_blocks:\n",
    "        text = block[4] # The text content of the block\n",
    "        # Split the block text into lines to check each line\n",
    "        lines_in_block = text.split('\\n')\n",
    "\n",
    "        for line in lines_in_block:\n",
    "            cleaned_line = line.strip()\n",
    "            if not cleaned_line:\n",
    "                continue\n",
    "\n",
    "            # Check if the line starts with a common caption pattern\n",
    "            is_caption = False\n",
    "            for pattern in caption_patterns:\n",
    "                if pattern.match(cleaned_line):\n",
    "                    extracted_captions.append({'page': page_num + 1, 'text': cleaned_line})\n",
    "                    is_caption = True\n",
    "                    break # Move to the next line if a caption is found\n",
    "\n",
    "            # Optional: Add heuristics based on text position or size if needed\n",
    "            # For example, captions are often below figures and might have a specific font size (more advanced)\n",
    "\n",
    "\n",
    "doc.close()\n",
    "\n",
    "# Print the extracted captions to verify\n",
    "print(\"Extracted Figure Captions:\")\n",
    "for caption_data in extracted_captions:\n",
    "    print(f\"Page {caption_data['page']}: {caption_data['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f25a849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Section: Metadata (Title/Authors) ---\n",
      "\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\n",
      "PE-GPT: A New Paradigm for Power Electronics Design\n",
      "Article in IEEE Transactions on Industrial Electronics · October 2024\n",
      "DOI: 10.1109/TIE.2024.3454408\n",
      "CITATIONS 8\n",
      "READS 1,067\n",
      "8 authors, including:\n",
      "Fanfan Lin\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute\n",
      "\n",
      "\n",
      "\n",
      "--- Section: 54 PUBLICATIONS   561 CITATIONS ---\n",
      "\n",
      "University of Arkansas at Fayetteville\n",
      "\n",
      "\n",
      "\n",
      "--- Section: 54 PUBLICATIONS   577 CITATIONS ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Section: 178 PUBLICATIONS   3,120 CITATIONS ---\n",
      "\n",
      "2,111 PUBLICATIONS 115,701 CITATIONS\n",
      "All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\n",
      "The user has requested enhancement of the downloaded file.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "Abstract— Large Language Models (LLMs) have shown exciting potential in powering the growth of many industries, yet their adoption in the power electronics (PE) sector is hindered by a lack of specialized PE technical expertise and challenges in processing PE-specific data. This study presents a pioneering approach to establish a multimodal LLM tailored for PE design applications, named PE-GPT. The methodology involves enhancing PE-GPT with retrieval augmented generation from a PE knowledge base, and proposes a hybrid framework that integrates an LLM agent with metaheuristic algorithms, Model Zoo, and Simulation Repository. This enhances its multimodal processing capabilities and enables integration into the existing design workflow. The PE-GPT methodology is demonstrated with two case studies: modulation design of the dual-active bridge (DAB) converter and circuit parameter design of the buck converter. PE-GPT demonstrates a 22.2% increase in correctness compared to human experts. Against other leading LLMs, PE-GPT shows a 35.6% improvement in correctness and a 15.4% enhancement in consistency, reducing hallucination. Hardware experiments validate PE-GPT’s multimodal capabilities in optimizing a five-degree-of-freedom modulation strategy for the DAB converter. The generalizability of PE-GPT to other PE design applications and associated AI ethical considerations are also discussed. This research concludes by outlining inspiring future research directions, encouraging researchers to expand the boundaries of the PE industry and advance toward a more intelligent era.\n",
      "Index Terms — Large language model, multimodal AI, power electronics design, power converter design, physics-informed AI.\n",
      "\n",
      "\n",
      "\n",
      "--- Tables in Section: 178 PUBLICATIONS   3,120 CITATIONS ---\n",
      "\n",
      "\n",
      "Table on Page 1:\n",
      "\n",
      "| See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/384548400\n",
      "PE-GPT: A New Paradigm for Power Electronics Design\n",
      "Article in IEEE Transactions on Industrial Electronics · October 2024\n",
      "DOI: 10.1109/TIE.2024.3454408\n",
      "CITATIONS READS\n",
      "8 1,067\n",
      "8 authors, including:\n",
      "Fanfan Lin Xinze Li\n",
      "Zhejiang University - University of Illinois Urbana-Champaign Institute University of Arkansas at Fayetteville\n",
      "54 PUBLICATIONS 561 CITATIONS 54 PUBLICATIONS 577 CITATIONS\n",
      "SEE PROFILE SEE PROFILE\n",
      "J.J. Rodriguez-Andina Josep M. Guerrero\n",
      "University of Vigo Zhejiang University\n",
      "178 PUBLICATIONS 3,120 CITATIONS 2,111 PUBLICATIONS 115,701 CITATIONS\n",
      "SEE PROFILE SEE PROFILE |  |\n",
      "|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n",
      "| All content following this page was uploaded by J.J. Rodriguez-Andina on 08 October 2024.\n",
      "The user has requested enhancement of the downloaded file. |  |\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Section: INTRODUCTION ---\n",
      "\n",
      "O accelerate the world’s transition to sustainable energy, power electronics (PE) devices play important roles. Unlike traditional fossil energy from which we can expect steady and predictable power, most of the sustainable energy sources exhibit the characteristic of intermittency, including solar and wind energy [1]. This intermittency, resulted from day-night cycle for solar energy and wind speed for wind energy, poses serious challenges for grid stability and reliability. To address this challenge, PE devices adjust the voltage and frequency of the generated power before it is fed into the grid. Furthermore, PE devices work in conjunction with energy storage systems like batteries in microgrids to store excess energy generated during peak generation hours and release it during low generation or high-demand hours [2].\n",
      "PE design is an intricate and highly specialized process, necessitating extensive experience and domain knowledge. The\n",
      "design usually covers the main circuit, switching device, magnetic component, control mechanism and modulation strategies, structural design, etc. To ensure optimal operating performance, the design process typically considers various performance metrics comprehensively. Crucially, as the grid evolves to be more dominated by PE devices, the scale and complexity of PE systems increase, leading to exponentially increasing work and difficulty in PE design [3].\n",
      "Intriguingly, large language models (LLMs) such as ChatGPT (2022), GPT-4 (2023) [4], LLaMa2 (2023) [5], and Gemini (2024) [6] have garnered significant attention, as they show great potential for augmenting humans in certain role across many industries. Essentially, LLMs are deep neural networks pre-trained on large general corpora like Wikipedia or Common Crawl in an unsupervised manner. With billions of parameters, LLMs embody vast linguistic and factual knowledge, enabling a spectrum of language-related tasks, including understanding, translation, question-answering, and summarization [4]. Recent studies have delved into specialized applications of LLMs. In the healthcare sector, Lee et al. explored applications in medical note-taking and clinical consultation support [7]. Wang et al. introduced TransGPT [8], a multimodal LLM tailored to elucidate traffic phenomena, address traffic queries, and generate traffic reports, offering valuable insights for transportation professionals. In the realm of electric energy, LLMs are leveraged for tasks like insulation defect detection, load forecasting, wildfire recognition, and optimal power flow quantification from linguistic data stipulations [9].\n",
      "However, amidst the excitement of these diverse applications, the PE industry faces specific challenges when incorporating LLMs for the PE design: • Challenge 1 - Limited PE Technical Expertise: Trained on general corpora, LLMs lack domain expertise. Consequently, they may struggle to offer precise insights or outcomes, particularly in PE-design-related tasks. • Challenge 2 - Incapability to Process PE-Specific Unstructured Data: PE design often involves unstructured data types like time-series operating waveforms which LLMs are not capable to handle. Furthermore, the specialized processing techniques required, such as Fourier transforms, and machine learning, are beyond the conventional capabilities of LLMs, limiting their potential contributions to PE design. As a solution to these challenges, this paper proposes PE- GPT, a multimodal LLM tool tailored for PE design. To address Challenge 1 mentioned above, PE-GPT is enhanced by retrieval augmented generation (RAG) built upon PE- specific knowledge base. This enables the LLM to obtain\n",
      "Fanfan Lin, Member, IEEE, Xinze Li, Member, IEEE, Weihao Lei, Student Member, IEEE, Juan J. Rodriguez-Andina, Fellow, IEEE, Josep M. Guerrero, Fellow, IEEE, Changyun Wen, Fellow,\n",
      "IEEE, Xin Zhang, Senior Member, IEEE, Hao Ma, Senior Member, IEEE\n",
      "PE-GPT: a New Paradigm for Power Electronics\n",
      "This is the author's version of an article that has been accepted for publication in this journal. https://doi.org/10.1109/TIE.2024.3454408\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "domain knowledge from the provided knowledge base before generating text responses. To address the aforementioned Challenge 2, we propose a hybrid framework for PE-GPT, integrating an LLM agent, metaheuristic algorithms, a Model Zoo, and a Simulation Repository. This hybrid framework supports three workflows: Design with Model Zoo, Simulation Verification, and Model Fine-Tuning. Firstly, through the metaheuristic algorithm and Model Zoo for function evaluation, PE-GPT optimizes design parameters and performs quantitative analysis. Secondly, PE-GPT can initiate simulation software and embed the design outcomes upon the user’s request, facilitating simulation verification. Thirdly, it supports fine-tuning data-driven models within the Model Zoo to enhance accuracy. The proposed hybrid framework addresses the LLM’s limitations in processing PE-specific quantitative data, equipping it with multimodality processing and PE design capabilities. For illustration and verification, this paper presents two design cases: modulation design for the dual-active-bridge (DAB) converter and circuit parameter design for the buck converter. In these cases, PE-GPT provides sequential text guidance to users, understands design requirements, performs optimized quantitative design, and conducts PE-specific reasoning and analysis.\n",
      "In summary, the contributions of this research are listed below: o We firstly propose PE-GPT, a multimodal LLM tailored for PE design, marking the beginning of a new paradigm for PE design. o We introduce a hybrid framework, consisting of an LLM agent, metaheuristic algorithms, a Model Zoo, and a Simulation Repository. This framework supports three workflows, enhancing the flexibility and generalization of the LLM for various tasks in PE design and seamlessly integrating PE-GPT into existing design workflows. o We present a PE knowledge-associated RAG technique, which addresses the limitations of LLMs in PE domain expertise. o Building upon this initial version of PE-GPT, this research sheds light on the promising potential of LLM-empowered intelligent PE design. We also discuss potential future directions and relevant ethical considerations. This paper is organized as follows: Section II covers preliminary knowledge about LLMs. Section III introduces the proposed PE-GPT methodology. Sections IV and V analyze design cases in detail. Section VI discusses the extension of PE- GPT to other PE design applications and relevant AI ethical concerns. Section VII validates the feasibility and effectiveness of PE-GPT. The conclusion and future directions are presented in the final section.\n",
      "\n",
      "\n",
      "\n",
      "--- Section: II. PRELIMINARIES: LARGE LANGUAGE MODELS ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Section: introduction of the Transformer model in 2017 [10]. Its core ---\n",
      "\n",
      "capture long-term dependencies in language.\n",
      "Following that, Google introduced Bidirectional Encoder Representations from Transformers (BERT) in 2018 which overcame the preceding language models’ constraints in unidirectional language modeling. In 2023, OpenAI trained GPT-4 on extensive Internet datasets, which has 1.76 trillion parameters, pushing the development of LLM to a new height. Although GPT-4 is capable of processing both text and image data formats, its direct application to technical tasks in the field of PE remains challenging.\n",
      "To tailor LLMs for specific industry applications, generally there are three prevailing methods: Prompt Engineering, RAG and model fine-tuning [4]. • Prompt Engineering involves leveraging textual inputs to guide the LLM in generating the desired output. Chain of thoughts (CoT) is a prompt engineering technique which guides the model to answer multi-step questions through a logical sequence of reasoning steps. • RAG equips the LLM with an external, vectorized knowledge base, thereby expanding the model's access to domain-specific, professional, or exclusive knowledge beyond its initial training data. • Model Fine-tuning is a technique where the pre-trained weights of the LLM are retrained with new training data. This retraining process may involve specific layers of the model, with others remaining frozen. The computational efforts required for retraining can vary based on the complexity and size of the task-specific dataset. Fig. 1 compares the three techniques against five criteria: accuracy of the response (less hallucination), simplicity of implementation, explainability of the technique, lightweight computation and flexibility to update external knowledge base. Considering the flexibility and simplicity, combinations of prompt engineering and RAG are applied to develop a tailored PE-GPT model.\n",
      "Fig. 1. Comparison of three LLM customization techniques.\n",
      "This section introduces the methodology behind the proposed multimodal PE-GPT from the following pivotal perspectives: the hybrid framework, the adaptation of a RAG technique for PE, and the integration of Model Zoo as well as the adopted physics-in-architecture neural network (PANN) inside Model Zoo.\n",
      "The PE-GPT methodology is generally applicable to power\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "converters. For clarity, this section presents the methodology with a focus on the modulation design of the DAB converter. Generalization of the methodology and its applications to other cases are discussed afterwards.\n",
      "\n",
      "\n",
      "\n",
      "--- Figure Captions in Section: introduction of the Transformer model in 2017 [10]. Its core ---\n",
      "\n",
      "- Page 3: Fig. 1 compares the three techniques against five criteria:\n",
      "\n",
      "- Page 3: Fig. 1. Comparison of three LLM customization techniques.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Section: A. Hybrid Framework of PE-GPT ---\n",
      "\n",
      "Fig. 2 presents the proposed hybrid framework of multimodal PE-GPT. There are two types of acceptable input data from the user: textual queries and PE-specific fine-tuning data.\n",
      "All textual queries first go through the RAG process, indicated by the green dotted section in Fig. 2, to enhance the domain expertise of the original LLM agent. To facilitate RAG, a specialized PE knowledge base is constructed beforehand based on the application-specific needs. For instance, if the PE- GPT is expected to handle modulation design tasks, relevant academic papers and technical documents on modulations can be intentionally collected for the knowledge base. Both the textual query and the knowledge base are stored in a vector database after vector embedding. Relevant information chunks are then retrieved based on the similarity of these embeddings, combined with the textual queries, and passed forward to the LLM agent.\n",
      "When the LLM agent takes in the textual query processed by RAG or the PE-specific fine-tuning data such as time-series\n",
      "waveforms, one of the following three workflows will be initiated. • Workflow 1: Design with Model Zoo. With a design request, the LLM agent initiates Workflow 1. This process involves a metaheuristic algorithm that iterates through three steps until the optimal design parameters are found [12]: Function Evaluation via Model Zoo, Selection, and Evolution. Metaheuristic algorithms are advantageous in PE design tasks because they offer robust global optimization, adapt to complex problems effortlessly and handle multi- objective optimization. The Model Zoo, detailed in Section III-C, evaluates the performance of candidate design solutions, such as the DAB converter's time-series performance across different modulation parameters. It plays a crucial role in overcoming the limitations of the original LLM agent in processing PE-specific data. • Workflow 2: Simulation Verification. As engineers and researchers often verify their PE designs using simulation tools, Workflow 2 offers an easy integration of design outcomes into existing simulations. The simulation repository includes relevant tools for PE design, such as Simulink, PLECS, and PSIM for circuit simulation, and COMSOL, ANSYS, and SIMSCALE for multi-physics simulation. Upon the user's request for simulation\n",
      "Fig. 2. The proposed hybrid framework of PE-GPT.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "verification, the appropriate simulation tool is initialized, and the design outcome is passed to the simulation model for one-click verification. • Workflow 3: Fine-Tuning. The design outcome is greatly influenced by the model accuracy in Model Zoo. To address discrepancies between model outputs and practical results, PE-GPT offers a mechanism for further fine-tuning the data- driven models in Model Zoo. When the user provides fine- tuning data, such as time-series waveform data or tabular performance data, Workflow 3 is activated to fine-tune the neural connections within the data-driven models.\n",
      "\n",
      "\n",
      "\n",
      "--- Figure Captions in Section: A. Hybrid Framework of PE-GPT ---\n",
      "\n",
      "- Page 4: Fig. 2 presents the proposed hybrid framework of multimodal\n",
      "\n",
      "- Page 4: Fig. 2. The proposed hybrid framework of PE-GPT.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Section: B. Retrieval Augmented Generation (RAG) ---\n",
      "\n",
      "LLM agents such as GPT-4 and Llama have been pretrained on vast and general corpora. The pretraining equips them with rich general domain knowledge. However, it also inherently limits their depth in specific domain expertise.\n",
      "The analysis in Section II explores three techniques: prompt engineering, RAG, and model fine-tuning. Among these, PE- GPT notably leverages RAG to tailor LLM for PE-specific applications. RAG effectively bridges the knowledge gap of LLMs through retrieving relevant excerpts from external knowledge bases. It notably enhances PE-GPT’s response performance in two key ways: • Accuracy and Relevancy: PE-specific applications demand a high degree of technical details. A knowledge base comprising technical documents, such as academic literatures, compensates for LLMs' limitations in domain- specific expertise, thereby enhancing the accuracy and relevancy of PE-GPT's outputs. • Customizability and Generalization: In PE design, tasks can significantly diverge in technical specifics. For instance, PE-GPTs designed for converter modulation might require different expertise than those aimed at converter circuit design. Through access to varied knowledge bases, RAG ensures both customizability and generalization for diverse applications.\n",
      "Fig. 2, outlined by the green dotted part, illustrates the comprehensive workflow of RAG, which includes three critical steps: chunking, embedding, and retrieval. 1) Chunking: As an initial preprocessing step, the existing knowledge database, comprising academic literature and industrial documents, is segmented into discrete, smaller units through a process known as \"chunking.\" 2) Embedding: Both the chunks from the knowledge base and the text query are transformed into vector representations during the embedding process, with these vectors subsequently stored in a vector database. As formulated in (2), A denotes the embedded vector of the text query, and Bi is the vector representation of the ith chunk. 3) Retrieval: The retriever calculates the similarity score similarityi between the query vector A and the vectorized chunks Bi, as given in (3). Based on the similarity search results, the most pertinent chunks are identified, retrieved, and integrated with the text query to formulate a\n",
      "synthesized prompt. This enriched query then serves as the basis for the LLM agent to generate a PE-aware response.\n",
      "Embed( ) , Embed( ) i i A query B chunk = = (2)\n",
      "Within the RAG workflow, there are 2 important hyperparameters to tune to optimize the effectiveness and efficiency of RAG for the PE-specific knowledge base: one is chunk size that determines the maximum number of characters that a chunk contains, and another one is top-k that refers to the number of top results retrieved after similarity search.\n",
      "\n",
      "\n",
      "\n",
      "--- Section: C. Model Zoo and the Adopted PANN Model Inside ---\n",
      "\n",
      "As elaborated in PE-GPT Workflow 1, the role of the Model Zoo is function evaluation, upon which the metaheuristic algorithm iterates until the optimal design outcome is found.\n",
      "Model Zoo consists of three types of models: white-box models, black-box models and gray-box models. Both black- box models and gray-box models are open for finetuning with new data from the user. • White-Box Models: These are theoretically closed-form analytical formulas for the evaluated objective. • Black-Box Models: These are pure AI-based data-driven models built on statistical information from the data [13]. • Gray-Box Models: These models combine partial theoretical structures with partial data-driven structures, such as Physics-Informed Neural Networks (PINNs) and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) [14].\n",
      "The integration of Model Zoo in Workflow 1 facilitates adaptability to various PE design tasks. Given the limited scope of this research paper, we highlight the use of PANN, an emerging PINN method, as a detailed example in Model Zoo. In the long term, the diversity of models in the Model Zoo will require collective contributions from the entire PE community.\n",
      "PANN, featuring a bespoke recurrent neural architecture designed to integrate circuit physics, plays pivotal roles in evaluating diverse performance metrics, such as current stress, soft switching conditions, circulating current, etc. Its lightweight and explainable advantages of PANN foster prompt and flexible design of PE devices [15].\n",
      "Inspired by neural ordinary differential equations, PANN uses numerical differentiation algorithms to customize its recurrent architecture, effectively characterizing the general form of circuit physics, as shown in (4). x(t) and u(t) represent state and input variables, while g(u(t);θ) and h(u(t);θ) are their respective behavioral functions, and θ symbolizes the circuit parameters.\n",
      "( ) ( ) ( ) ( ); ( ) ( ); x t g u t x t h u t t\n",
      "For the typical topology of DAB converters, the customized architecture of PANN is shown in Fig. 3. It embeds the physics of inductor current iL using an implicit Euler algorithm, as formulated in (5), where ∆tk+1 is the time interval, iL is the state\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "variable being modeled, vp and vs are input variables, and the parameters θ include L, RL and n. PANN predicts the subsequent state iL(tk+1) based on the present state iL(tk) and inputs vp(tk+1) and vs(tk+1), whose prediction process is recurrently implemented for a switching period Ts as necessitated by performance evaluation.\n",
      "The interaction between Model Zoo and other constituents of the PE-GPT suite is outlined. In Workflow 1, upon receiving a design request from the LLM agent, a metaheuristic algorithm is triggered. As evident in Fig. 3, candidate modulation parameters are delivered to the selected model in Model Zoo, namely PANN, for performance evaluation during each evolutionary iteration, consisting of three steps: First, inputs vp and vs are generated based on the domain knowledge inherent in the suggested modulation, where D0, D1, D2 are phase shift angles, and φ1, φ2 indicate adjustable duty cycles; Second, the PANN model recurrently predicts the inductor current iL; Third, diverse performance metrics are assessed with vp, vs, and the predicted iL in the time domain. For instance, current stress ipp is evaluated by the difference between the maximum and minimum of iL. Soft switching performance is assessed by judging the directions and values of inductor current at each switching instant. The assessed performances are used to compute objective function values which guide the metaheuristic algorithm in the evolving process.\n",
      "PE-GPT Metaheuristic Algorithm\n",
      "Generate Candidates Objective Values\n",
      "Mod. Strategy; Spec. (V1, V2, PL)\n",
      "Fig. 3. PANN for the function evaluation of metaheuristic algorithm.\n",
      "Other than textual requests from users, the proposed PE-GPT receives time-series data to fine-tune Model Zoo, exhibiting multimodal processing competency. The provided time-series data is divided into three datasets for the training of PANN, selection of hyperparameters, and test of generalization capability. Feeding the training data into the computational graph in Fig. 3, the mean-squared error MSE between the ground truth iL* and the predicted iL in (6) is back-propagated to fine-tune circuit parameters θ, where NT is the sequence length of the provided time-series data.\n",
      "L L L k L k k T MSE i i i t i t N θ θ θ θ\n",
      "D. Discussion on the Generalization of PE-GPT\n",
      "a) RAG Enhances Generalization\n",
      "RAG significantly enhances the generalization of PE-GPT by creating a flexible and customizable knowledge base. The flexibility allows users to tailor the knowledge base as per specific requirements or objectives, incorporating diverse types of text-based resources such as academic literature, industrial documents, customized case studies or other documents encapsulating domain expertise and experience. A regular update routine to the RAG will also be helpful for capturing up- to-date information.\n",
      "b) Model Zoo Enhances Generalization\n",
      "The integration of the Model Zoo in Workflow 1 enhances PE-GPT’s generalization. Different design cases in Workflow 1 have various objective functions to evaluate when calling the metaheuristic algorithm. And Model Zoo is a repository of different models, providing flexibility in selecting the relevant model for function evaluation. Admittedly, building a comprehensive Model Zoo for PE-GPT is a long-term and collaborative commitment and the authors aim to inspire more researchers in the PE community to join this campaign.\n",
      "Moreover, some gray-box models in the Model Zoo also enable stronger generalization capability. Taking the adopted PANN as an example, for the case of modulation parameter design, PANN is trained to learn and predict the time-series performance for different modulation parameter selections. Thanks to its “training free” domain transfer capability, a trained PANN model can be seamlessly transferred to various PE topology variants through physics-guided deformation of its neural architecture, while accommodating diverse operating conditions through the direct assignment of priori domain knowledge [15], [16].\n",
      "IV. DESIGN CASE I: MODULATION DESIGN\n",
      "A. Design Case I Background: the DAB Converter and Modulation Strategies\n",
      "The DAB converter is taken as a design case to validate the proposed methodology of PE-GPT. The bidirectional isolated DC-DC converter is commonly adopted in the solid-state transformer, energy storage systems, electric vehicles, and more [17]. As depicted in Fig. 4, it is composed of two full bridges, a high-frequency transformer providing galvanic isolation, and a leakage inductor L.\n",
      "D0, D2, φ2 D1, φ1 Fig. 4. The DAB converter and its 5DoF modulation scheme.\n",
      "To regulate the power transfer, modulation scheme of a DAB converter determines how the switches of two full bridges are controlled. There are five common modulation schemes: single\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "phase shift (SPS), double phase shift (DPS), extended phase shift (EPS), triple phase shift (TPS) [18] and five DoF modulation schemes (5DoF) [19]. The adopted 5DoF scheme features an outer phase shift D0, two inner phase shifts D1, D2, and two duty cycles φ1, φ2, which is simplified to phase shift strategies if primary and secondary duty cycles are fixed at 50%.\n",
      "For professionals to design modulation strategies and parameters, the paramount factors steering the design process encompass the complexity of modulation implementation and the critical performance metrics of the converter. These metrics include current stress, zero voltage switching range, zero current switching range, reactive power, efficiency, etc. [20].\n",
      "B. Design Case I Specifications\n",
      "TABLE I. DESIGN CASE I SPECIFICATIONS AND HARDWARE SETUPS\n",
      "OPERATING CONDITIONS OF THE DESIGN CASE I Input voltage V1 200 V Output voltage V2 [160 V, 240 V] Rated power Prated 1 kW Switching frequency fs 50 kHz SETTINGS OF HARDWARE PROTOTYPE Semiconductors Wolfspeed SiC C2M0080120D Transformer Nanocrystalline ribbon 1K107B; Turn ratio n = 1; Leakage inductance L = 64.6 μH; RL = 1.9 Ω Oscilloscope Lecroy HDO8058a Controller DS1202 platform CONFIGURATIONS OF PE-GPT LLM LLM model: GPT-4-0125; temperature = 0.1; RAG hyperparameters: chunk size = 512, top-k = 7; RAG knowledge base: Research articles about modulation design PANN Train-validation-test partition: 5, 100, 100 samples; In-domain cases: 160 V; Out-of-domain cases: 200 V and 240 V; NT = 250; Time interval ∆tk = 80 ns; θ = {L; RL; n}\n",
      "The operating conditions for the design case are shown in Table I. The prototype built for data collection and hardware validation is shown in Fig. 5, where a DS1202 MicroLabBox is\n",
      "used as the control platform, and a Lecroy HDO8058a oscilloscope captures the time-series waveforms at 25 MS/s.\n",
      "Transformer with integrated leakage L\n",
      "C. Design Case I Breakdown and Analysis on the Competencies of PE-GPT\n",
      "A design case study is presented, illustrating an iterative design approach for developing the modulation strategy of the DAB converter. The performance requirements outlined in this case are current stress and soft switching. Table II provides an analysis of a specific dialogue excerpt between the user and PE- GPT, with the full interaction available for reference online1. The capability of PE-GPT has been divided into three types: adaptive semantic understanding (ASU), PE-specific knowledge and reasoning (PE-KR), multimodal processing (MMP).\n",
      "The case study covers both Workflow 1 and Workflow 3, and unfolds across four key conversational stages: 1) Initial modulation strategy recommendations (ID 1 and 2): PE-GPT initiates the process by suggesting a modulation strategy tailored to the operational requirements. 2) Design of modulation parameters and performance analysis\n",
      "TABLE II. DESIGN CASE I DIALOGUE EXCERPT: MODULATION DESIGN (Video demo1)\n",
      "ID User’s Textual Input PE-GPT’s Response Capability and Workflow (WF)\n",
      "Hi, can you help me design the modulation strategy for my dual\n",
      "Of course, I'd be happy to assist (…) If there are any particular performance indicators or constraints you're aiming for, such as efficiency, thermal performance, control complexity, or ease of implementation, please let me know. Your detailed requirements will help me provide you with the most suitable modulation strategy.\n",
      "DEFINE DESIGN REQUIREMENTS Can you recommend a phase shift\n",
      "modulation for a lowest inductor current for the full operating range?\n",
      "(…), I would recommend the Triple Phase Shift (TPS) modulation method. The TPS method is effective in reducing the current stress on the inductor by allowing independent control of the phase shifts between the primary and secondary sides of the converter, as well as between the active bridges. This can lead to a reduction in the RMS current, which in turn minimizes the current stress on the inductor.\n",
      "To further refine the recommendation and (…), could you please provide the following operating conditions (…)\n",
      "CONDITIONS: The operating conditions are V1 = 200 V, V2 = 160 V, Prated=1000 W\n",
      "Under the TPS modulation strategy, the optimal D1 is (…), the number of switches that achieve zero-voltage turn-on is 8. (…) At rated power level (Prated = 1000 W), the peak-to-peak current is 19.05 A. Under light load conditions (PL = 300 W), (…)\n",
      "Under the SPS modulation strategy, the number of switches that achieve zero-voltage turn-on is (…). Under light load conditions (PL = 300 W), (…)\n",
      "The performance of current TPS modulation far exceeds that of conventional SPS.\n",
      "ITERATIVE DESIGN But I think the soft switching range cannot satisfy my application in TPS.\n",
      "If the soft-switching range provided by the TPS modulation strategy does not meet the requirements of your application, I recommend considering the Five-Degree-of-Freedom (5DoF) modulation scheme. The 5DoF modulation scheme is not a phase shift modulation method, but it offers superior performance in terms of soft- switching range. The 5DoF modulation scheme provides enhanced control over the converter by independently adjusting five different parameters: (…). This level of control allows for an expanded ZVS and ZCS ranges, which can significantly improve efficiency, especially under varying load conditions. (…)\n",
      "STRATEGY OK, let us go ahead with 5DoF in the\n",
      "Under the 5DoF modulation strategy, the optimal D1 is (…), φ1 is designed to be (…), the number of switches that achieve zero-voltage turn-on is (…). At rated power level(Prated=1000 W), the peak-to-peak current is 18.69 A.\n",
      "Under light load conditions (PL = 300 W), (…)\n",
      "ASK FOR MODEL FINE-TUNING Now I have some experimental data. Can you\n",
      "help improve the design accuracy?\n",
      "Absolutely, I can assist you with improving the design accuracy using your experimental data. Please upload three CSV files containing the following waveforms. (…) Ø MMP o WF3\n",
      " ASU: Adaptive Semantic Understanding; PE-KR: PE-Specific Knowledge and Reasoning; MMP: Multimodal Processing; WF1: Design with Model Zoo; WF3: Fine-tuning.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "(ID 3): PE-GPT proceeds to design modulation parameters through metaheuristic algorithm based on specified operational specifications and conducts a thorough performance analysis. 3) Adaptation to user feedback and redesign (ID 4 and 5): PE- GPT flexibly adjusts its approach in response to user feedback, iteratively refining the modulation strategy. 4) Model fine-tuning (ID 6): Lastly, PE-GPT enhances its function evaluation accuracy by incorporating the user's experimental data, which is Workflow 3 as denoted in Fig. 2.\n",
      "The unique competencies of the proposed multimodal PE- GPT are analyzed as follows: a) Adaptive Semantic Understanding (ASU)\n",
      "Building upon the linguistic capability inherited from GPT- 4, PE-GPT’s capabilities are further enhanced through RAG technique. As demonstrated in Table II, PE-GPT effectively interprets and adapts to the user's specific requirements and feedback regarding operational performance. b) PE-Specific Knowledge and Reasoning (PE-KR)\n",
      "PE-GPT exhibits strong domain expertise by recommending optimal modulation strategies tailored to user requirements and providing thorough analyses. This capability showcases the model's advanced reasoning within the PE field. c) Multimodal Processing (MMP)\n",
      "Integration with PANN in Model Zoo allows PE-GPT to analyze the time-series performance of the design outcomes. Additionally, it can incorporate experimental time-series data provided by users to refine and enhance model accuracy.\n",
      "Multimodality is a convincing feature that distinguishes the proposed PE-GPT from the original GPT framework. Upon receiving time-series waveforms acquired from hardware experiments and the fine-tuning query from user’s textual inputs, Workflow 3 will be initiated, and the PANN model will be fine-tuned to reduce the loss in (6), bridging the gap between the modeling of inductor current iL and experimental measurements iL*.\n",
      "MSEval MSEtest Before fine-tune (Epoch=1)\n",
      "After fine-tuning (Epoch = 34)\n",
      "Modeling accuracy greatly improves\n",
      "R2 improves by 4.88% R2 improves by 4.88%\n",
      "Initial θ RL=500mΩ L=65μH n = 1.1\n",
      "Final θ RL=1.88Ω L=64.4μH n = 1\n",
      "(d) Fig. 6. Fine-tuning of PANN: (a) the fine-tuning process; (b) exemplary waveforms before fine-tuning; (c) waveforms after fine-tuning; (d) trajectory of parameter variations during fine-tuning.\n",
      "The fine-tuning procedure is shown in Fig. 6 (a), which reduces the test mean square error (MSEtest) by 84%. Exemplary waveforms of the 5DoF modulation strategy are provided to justify the modeling accuracy, as shown in Fig. 6 (b) and (c), where V2 = 160 V and PL = 300 W. After the fine-tuning process, the coefficient of determination R2 is refined by 4.88%. The trajectory of parameter variations during fine-tuning is disclosed in Fig. 6 (d), where under the learning rates of 5e-6 for L and 1e-1 for n and RL, the estimations for L and n oscillate and gradually stabilize until convergence, whereas that for RL smoothly converges.\n",
      "Additionally, the time-domain modeling accuracy of various data-driven models is comparatively studied in Fig. 7. Long- short-term-memory (LSTM) net, temporal convolutional net (TCN), time-series transformer (TST), and PINN [21] are benchmarked. Trained on a purposefully biased and incomplete dataset with 5 time-series samples, PANN advances the latest TST and PINN models by 84% and 81.9% in in-domain cases. When extrapolating to out-of-domain cases, PANN outperforms the latest PINN by 37.3%. Notably, PANN demonstrates the highest robustness, with negligible accuracy variations over 10 algorithm repetitions.\n",
      "T-SNE SCATTER PLOT OF DATA PARTITIONS\n",
      "IN-DOMAIN MSE OUT-OF-DOMAIN MSE\n",
      "Fig. 7. MSEs of diverse data-driven models (repeated for 10 times) in in- domain (160 V) and out-of-domain (200 V and 240 V) cases.\n",
      "In PE-GPT, hyperparameters of RAG need to be finely adjusted to efficaciously balance between the information precision and the query efficiency for the PE-specific domain knowledge. Performances of PE-GPT with respect to diverse chunk size and top-k are depicted in Fig. 8, including hit rate (HR), mean reciprocal rank (MRR), response speed (RS), faithfulness (FF), and relevancy (Rel). These five evaluators comprehensively describe diverse aspects of PE-GPT: HR\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "evaluates precision in retrieving related responses, MRR ranks the correct answers, RS measures the response generation speed, FF denotes the extent to truthfully reflect the PE knowledge base, and Rel reflects the relevancy between the response contexts and the user’s queries.\n",
      "Fig. 8. Fine-tuning the hyperparameters of RAG (higher metrics indicating better performance), and RS and MRR are normalized to unit scale.\n",
      "The finely tuned chunk size and top-k for RAG are 512 and 7, respectively, contributing to response completeness, balanced information granularity, and prompt retrieval. HR, FF, and Rel are close to 1, while a high MRR and fast response speed have been realized, demonstrating the optimal comprehensive performances. Other hyperparameter combinations either sacrifice their hit rate and MRR or compromise the response speed.\n",
      "V. DESIGN CASE II: CIRCUIT PARAMETER DESIGN\n",
      "A. Design Case II Specifications\n",
      "Given the specifications in Table III, Design Case II presents a design process of circuit parameters for the buck converter using PE-GPT. It includes the design of the inductance Lf and\n",
      "capacitance Cf of the output LC filter, with design requirements on ripple constraints and high power density.\n",
      "TABLE III. DESIGN CASE II SPECIFICATIONS OPERATING CONDITIONS OF THE DESIGN CASE II Input voltage V1 200 V Output voltage V2 [20 V, 180 V] Rated power Prated 1 kW Switching frequency fs 50 kHz Current ripple iripple 20% Voltage ripple vripple 0.5% CONFIGURATIONS OF PE-GPT LLM RAG knowledge base: Research articles about buck converter design PANN NT = 250; Time interval ∆tk = 80 ns; θ = {Lf ; Cf}.\n",
      "B. Design Case II Breakdown and Analysis\n",
      "Design Case II covers both Workflow 1 and Workflow 2, and unfolds across three key conversational stages, as the dialogue excerpt presented in Table IV and available online2: 1) Circuit Parameter Design (ID 1-3): PE-GPT understands the user’s request, guides the user to specify the design requirements and initializes Workflow 1 to find the optimal circuit parameters. 2) Rationale Analysis (ID 4-5): PE-GPT explains the rationale behind the recommended design with PE-specific knowledge. 3) Simulation Verification (ID 6): With the user’s request to verify the design, PE-GPT initiates Workflow 2, starting PLECS for circuit simulation and passing the design parameters into the simulation.\n",
      "Similar to Design Case I, PE-GPT exhibits unique competences in this case. It understands users’ design requirements (ASU), automates the optimal design process (MMP) and provides professional rationale for the design outcome (PE-KR).\n",
      "The Lf and Cf designed for the buck converter via PE-GPT are 482.7 μH and 12.5 μF, respectively. As shown in Fig. 9, iL\n",
      "TABLE IV. DESIGN CASE II DIALOGUE EXCERPT: CIRCUIT PARAMETER DESIGN (Video demo1)\n",
      "ID User’s Textual Input PE-GPT’s Response Capability and Workflow (WF)\n",
      "INITIALIZE DESIGN PROCESS: I want to design the output LC filter for a Buck converter. Can\n",
      "Absolutely, I'd be glad to help with (…) To get started, could you share with me the specific design objectives and requirements for your converter? For example, are you focusing on minimizing ripple, maximizing efficiency, or optimizing for power density? Any details you provide will help me tailor my assistance to best meet your needs.\n",
      "DEFINE DESIGN REQUIREMENTS Yes, I have two design requirements. Firstly, the output voltage\n",
      "ripple and inductor current ripple should be limited. Secondly,\n",
      "high power density is expected.\n",
      "Great, (…). To provide you with the best guidance, could you please share the following details:\n",
      "1.The output voltage ripple constraint (%) 2. (…)\n",
      "PROVIDE OPERATING CONDITIONS: Voltage ripple constraint is 0.5%, and current ripple constraint is\n",
      "20%. And operating conditions are as follows: switching frequency is 50 kHz; input voltage is 200V; output voltage is 80V;\n",
      "Well received. (…) The optimal inductance L is designed to be 482.7 μH, and the optimal capacitance C is designed to be 12.51 μF. The output voltage ripple and inductor current ripple are 0.398 V (0.50%) and 1.992 A (19.92%), respectively.\n",
      "The output waveform of the inductor current in steady state under this operating condition is shown in the following figure: (…)\n",
      "4 ASK FOR RATIONALE What is the rationale behind this design?\n",
      "(…) Output Voltage Ripple Constraint: Minimizing the output voltage ripple is critical for stable load operation. A smaller output ripple requires an adequately sized capacitor, which directly influences the quality of the DC output.\n",
      "Inductor Current Ripple Constraint: Controlling the inductor current ripple is essential for efficient energy transfer and minimizing losses. It also impacts the inductor size—lower ripple percentages typically require larger inductors. (…)\n",
      "ASK FOR FURTHER ANALYSIS Can you analyze the harmonic components of the inductor current\n",
      "The harmonics of inductor current are 10.0 A (DC), 0.8 A (1st), 0.123 A (2nd), 0.0549 A (3rd)… The harmonics of output voltage are 80.0 V (DC), 0.204 V (1st), 0.0157 V (2nd), 0.00466 V (3rd)…\n",
      "6 ASK FOR SIMULATION VERIFICATION The design sounds reasonable. Can we verify this design in PLECS?\n",
      "Waiting... PLECS is starting up... The PLECS simulation is running… Complete! (…) o WF2\n",
      " ASU: Adaptive Semantic Understanding; PE-KR: PE-Specific Knowledge and Reasoning; MMP: Multimodal Processing; WF1: Design with Model Zoo; WF2: Simulation Verification.\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "and vo predicted by PANN closely align with the PLECS simulation results with MSE of only 2.9e-3 and 3.02e-4, respectively.\n",
      "PLECS waveform Δvo% = 0.49% ≤ vripple (0.5%)\n",
      "MSE = 3.02e-4 MSE = 3.02e-4 PANN\n",
      "ΔiL% = 20% ≤ iripple (20%) (a) (b) Fig. 9. Waveforms of the designed buck converter: (a) vo; (b) iL.\n",
      "Based on the design rationale analysis, larger inductor and capacitor values provide better ripple limitation performance but result in lower power density. The design outcome demonstrates that PE-GPT understands the trade-off between these design objectives. It presents a reasonable design that meets the ripple constraints without requiring an oversized filter for the buck converter.\n",
      "Experiments have been conducted to verify the effectiveness of Design Case I in two aspects: PE domain comprehension, and the performance of the DAB modulation design case.\n",
      "A. Validation for PE Domain Comprehension: Benchmark with Human Experts and State-of-the-Art (SOTA) LLMs\n",
      "Apart from the linguistic capability justified in Section IV-C, 32 user requests targeting at various converter performances are meticulously prepared, acting as composite recommendation tasks to validate the capability to comprehend PE domain knowledge.\n",
      "As benchmarks, we have invited 5 human experts with 1~10 years of experience to recommend suitable modulation strategies based on the requests individually, which is to verify the usefulness of PE-GPT for both newbies and experienced engineers. Additionally, SOTA LLMs, incorporating ChatGPT (2022), GPT-4 (2023) [4], Llama-2-13b (2023) [5], Gemini (2024) [6], Claud 3 Sonnet (2024) [19] are compared to verify the superior PE domain comprehension of PE-GPT. For LLMs, each request is repeated 10 times, and the average accuracy and consistency are computed. The reference answers are formulated by a group of domain experts.\n",
      "Fig. 10. Comparisons among human experts, SOTA LLMs, and PE-GPT.\n",
      "As summarized in Fig. 10, the proposed PE-GPT attains an average correctness of 77.2%, which substantially outperforms experienced human experts by 22.2%. Besides, PE-GPT surpasses the best benchmarked LLM, Claude 3 Sonnet, by\n",
      "35.6%. Moreover, although being derived from GPT-4, the PE- specific RAG of PE-GPT greatly enhances its recommendation accuracy by 49.4%. From the perspective of consistency, the benchmarked LLMs cannot generate outputs coherently, suffering from hallucination. In contrast, the proposed PE-GPT addresses LLM’s hallucination by the PE-tailored RAG. Furthermore, as evident in Table II, PE-GPT has generated structured, rational, and comprehensive explanations to support its recommendation, manifesting great reasoning capability.\n",
      "B. Validation for the 5DoF Design Case: Steady-State Waveforms and Performance Analysis\n",
      "V2 = 160 V, PL = 300 W, (D1, D2, φ1, φ2) = (0.71, 0.88, 57%, 56.8%)\n",
      "V2 = 160 V, PL = 600 W, (D1, D2, φ1, φ2) = (0.78, 1, 50%, 51%)\n",
      "V2 = 160 V, PL = 1 kW, (D1, D2, φ1, φ2) = (0.88, 1, 50%, 50%) s1, s2, s3, s4 ZVS\n",
      "(b) (c) Fig. 11. vp, vs, and iL when V2 = 160 V and: (a) 300 W; (b) 600 W; (c) 1kW.\n",
      "V2 = 240 V, PL = 300 W, (D1, D2, φ1, φ2) = (0.81, 0.64, 41%, 40.5%)\n",
      "V2 = 240 V, PL = 500 W, (D1, D2, φ1, φ2) = (1, 0.78, 50%, 45.5%)\n",
      "V2 = 240 V, PL = 1 kW, (D1, D2, φ1, φ2) = (1, 0.81, 50%, 50%) s1, s2, s3, s4 ZVS\n",
      "(b) (c) Fig. 12. vp, vs, and iL when V2 = 240 V and: (a) 300 W; (b) 500 W; (c) 1kW.\n",
      "The design cases of the 5DoF modulation strategy discussed in Section IV are verified in hardware experiments for the sake of demonstrating the multimodal capability of the proposed PE-\n",
      "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n",
      "GPT. The 5DoF design cases are validated from two main dimensions: steady-state waveforms and performance analysis.\n",
      "vp, vs, and iL waveforms of voltage step-down scenarios (V2 = 160 V) are shown in Fig. 11. For the soft-switching-optimized 5DoF strategy, all 8 switches have met zero voltage switching (ZVS). Meanwhile, when PL = 300 W, s2, q1, q3, and q4 have realized zero current switching (ZCS), and when PL = 600 W, s1 and s2 have achieved ZCS. Under light-load conditions PL = 300 W, the zero-voltage plateau of vs is aggregated to its right side by applying a wider duty cycle to q1 (56.8%).\n",
      "Voltage step-up waveforms (V2 = 240 V) are depicted in Fig. 12. Similarly, across the entire operating conditions, all-switch ZVS has been attained, while ZCS ranges are expanded for light and medium loads. Under PL = 300 W, s2, s3, s4, and q4, and when PL = 500 W, s1 and q4 have satisfied ZCS. By assigning a wider duty cycle to s2 (59%), the zero-voltage plateau of vp under PL = 300 W is shifted to its left side, achieving wider ZCS ranges.\n",
      "b) Detailed Performance Analysis\n",
      "Diverse performances of the designed 5DoF strategy, including the current stress ipp, efficiency η, and soft switching conditions, are evaluated and compared with current-stress- optimized TPS (CSO-TPS) and SPS. Performance comparison results are provided in Fig. 13 and Fig. 14.\n",
      "TPS 4 6 6 6 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "50 100 200 300 400 500 600 700 800 900 1000 PL /W\n",
      "TPS 0 0 0 0 2 2 2 0 0 0 0 2 2 0 0 0 0 0 0 2 4 3 Ø Extended ZVS & ZCS ranges V2 = 160 V Fig. 13. ipp, efficiency η, ZVS, and ZCS performances of the SPS, CSO- TPS, and the designed 5DoF strategy via PE-GPT under V2 = 160 V.\n",
      "TPS 4 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      "50 100 200 300 400 500 600 700 800 900 1000 PL /W\n",
      "TPS 0 0 0 0 2 4 2 0 0 0 2 4 2 0 0 0 0 2 4 2 Ø Extended ZVS & ZCS ranges V2 = 240 V\n",
      "Fig. 14. ipp, efficiency η, ZVS, and ZCS performances of the SPS, CSO- TPS, and the designed 5DoF strategy via PE-GPT under V2 = 240 V.\n",
      "Generally, efficiency η enhancements of the designed 5DoF are more considerable in light-load conditions PL ≤ 500 W,\n",
      "while ipp of 5DoF and TPS are comparable across all load levels. Under voltage step-down scenarios when V2 is 160 V, the 5DoF design cases enhance efficiency of CSO-TPS by 4.9%, 1.8%, and 0.8% for 50 W, 100 W, and 200 W, respectively, by virtue of their optimized soft switching ranges. The CSO-TPS strategy compromises its ZVS and ZCS for the pursuit of low ipp, with 4 switches realizing ZVS for 50 W, and 6 switches operating at ZVS under other light loads. In comparison, the 5DoF strategy attains all-switch ZVS and extends its ZCS ranges within [100 W, 600 W], as the 4-switch ZCS under 300 W and 2-switch ZCS under 600 W shown in Fig. 11. In terms of voltage step-up conditions when V2 is 240 V, the efficiency of the designed 5DoF surpasses that of CSO- TPS by 7.1%, 2.9%, and 0.94% for 50 W, 100 W, and 200 W, respectively, and the ZCS ranges within [100 W, 300 W] are extended to 2, 2, 4 switches.\n",
      "The analysis above has validated that the design output of PE-GPT delivers optimal performance in meeting the user's requirements in the design case.\n",
      "A. Extending Across the PE Design Workflow\n",
      "The hybrid framework of PE-GPT enables flexibility and generalization beyond the DAB converter modulation design case and the buck converter ci\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = 'file.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Organize tables and captions by page\n",
    "tables_by_page = {}\n",
    "if 'extracted_tables_data' in locals():\n",
    "    for page_data in extracted_tables_data:\n",
    "        tables_by_page[page_data['page']] = page_data['tables']\n",
    "\n",
    "captions_by_page = {}\n",
    "if 'extracted_captions' in locals():\n",
    "    for caption_data in extracted_captions:\n",
    "        page = caption_data['page']\n",
    "        if page not in captions_by_page:\n",
    "            captions_by_page[page] = []\n",
    "        captions_by_page[page].append(caption_data['text'])\n",
    "\n",
    "# Use filtered headings to assign page ranges\n",
    "section_titles_ordered = list(section_contents.keys())\n",
    "section_page_ranges = {}\n",
    "\n",
    "# Handle \"Metadata\" section\n",
    "if section_titles_ordered:\n",
    "    first_heading_page = filtered_headings[0]['page'] if filtered_headings else 1\n",
    "    section_page_ranges[section_titles_ordered[0]] = (1, first_heading_page - 1)\n",
    "\n",
    "# Remaining sections\n",
    "for i in range(1, len(section_titles_ordered)):\n",
    "    start_page = filtered_headings[i-1]['page']\n",
    "    end_page = filtered_headings[i]['page'] - 1 if i < len(filtered_headings) else doc.page_count\n",
    "    section_page_ranges[section_titles_ordered[i]] = (start_page, end_page)\n",
    "\n",
    "# Combine sections with tables and captions\n",
    "final_combined_output_list = []\n",
    "\n",
    "for section_title, section_text in section_contents.items():\n",
    "    final_combined_output_list.append(f\"\\n--- Section: {section_title} ---\\n\")\n",
    "    final_combined_output_list.append(section_text)\n",
    "    final_combined_output_list.append(\"\\n\")\n",
    "\n",
    "    # Get page range for section\n",
    "    start_page, end_page = section_page_ranges.get(section_title, (0, -1))\n",
    "\n",
    "    # Add tables in the section\n",
    "    section_tables = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        if page_num in tables_by_page:\n",
    "            for table in tables_by_page[page_num]:\n",
    "                section_tables.append({'page': page_num, 'table': table})\n",
    "\n",
    "    if section_tables:\n",
    "        final_combined_output_list.append(f\"\\n--- Tables in Section: {section_title} ---\\n\")\n",
    "        section_tables.sort(key=lambda x: x['page'])\n",
    "        for table_data in section_tables:\n",
    "            page_num = table_data['page']\n",
    "            table = table_data['table']\n",
    "            final_combined_output_list.append(f\"\\nTable on Page {page_num}:\\n\")\n",
    "            final_combined_output_list.append(format_table_as_markdown(table))\n",
    "            final_combined_output_list.append(\"-\" * 20)\n",
    "        final_combined_output_list.append(\"\\n\")\n",
    "\n",
    "    # Add figure captions in the section\n",
    "    section_captions = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        if page_num in captions_by_page:\n",
    "            for caption in captions_by_page[page_num]:\n",
    "                section_captions.append({'page': page_num, 'text': caption})\n",
    "\n",
    "    if section_captions:\n",
    "        final_combined_output_list.append(f\"\\n--- Figure Captions in Section: {section_title} ---\\n\")\n",
    "        section_captions.sort(key=lambda x: x['page'])\n",
    "        for caption_data in section_captions:\n",
    "            page_num = caption_data['page']\n",
    "            caption_text = caption_data['text']\n",
    "            final_combined_output_list.append(f\"- Page {page_num}: {caption_text}\\n\")\n",
    "        final_combined_output_list.append(\"\\n\")\n",
    "\n",
    "doc.close()\n",
    "\n",
    "# Final combined string\n",
    "final_combined_output = \"\\n\".join(final_combined_output_list)\n",
    "\n",
    "# Preview first 10000 characters\n",
    "print(final_combined_output[:50000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a84fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# === STEP 1: Chunking with overlap ===\n",
    "def chunk_text_with_headings(text, max_chunk_size=1200, overlap=200):\n",
    "    sections = re.split(r\"(?i)(?=^\\s*(?:abstract|introduction|related work|conclusion|references|section|chapter)[\\s:])\", \n",
    "                        text, flags=re.MULTILINE)\n",
    "    chunks = []\n",
    "    for section in sections:\n",
    "        heading_match = re.match(r\"^\\s*(.+)\", section.strip(), flags=re.MULTILINE)\n",
    "        heading = heading_match.group(1) if heading_match else \"Unnamed Section\"\n",
    "\n",
    "        start = 0\n",
    "        while start < len(section):\n",
    "            end = min(start + max_chunk_size, len(section))\n",
    "            chunk = section[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append((heading, chunk))\n",
    "            start = end - overlap if end - overlap > start else end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# === STEP 2: Summarize individual chunks (short + precise) ===\n",
    "def summarize_chunk_with_groq(chunk_text, heading):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gemma2-9b-it\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"You are an expert research assistant.\\n\\n\"\n",
    "                    f\"Section heading: {heading}\\n\\n\"\n",
    "                    f\"Text:\\n{chunk_text}\\n\\n\"\n",
    "                    f\"Task: Write a very concise summary (3–4 sentences maximum). \"\n",
    "                    f\"Prefer brevity over detail. Remove pseudocode, examples, and lists unless essential. \"\n",
    "                    f\"Keep only the scientific insights.\"\n",
    "                )\n",
    "            }],\n",
    "            temperature=0.4,\n",
    "            max_completion_tokens=150,\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# === STEP 3: Group summaries in batches ===\n",
    "def batch_summarize_summaries(summaries, heading, batch_size=5):\n",
    "    grouped = []\n",
    "    for i in range(0, len(summaries), batch_size):\n",
    "        batch = summaries[i:i+batch_size]\n",
    "        batch_text = \"\\n\\n\".join(batch)\n",
    "\n",
    "        print(f\"\\n[INFO] Merging batch {i//batch_size + 1} for section: {heading}...\")\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gemma2-9b-it\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        f\"Section heading: {heading}\\n\\n\"\n",
    "                        f\"Here are partial summaries:\\n{batch_text}\\n\\n\"\n",
    "                        f\"Task: Merge these into a single concise summary (max 6 sentences). \"\n",
    "                        f\"Remove redundancy and irrelevant details, but preserve all key insights.\"\n",
    "                    )\n",
    "                }],\n",
    "                temperature=0.3,\n",
    "                max_completion_tokens=200,\n",
    "            )\n",
    "            group_summary = completion.choices[0].message.content.strip()\n",
    "            grouped.append(group_summary)\n",
    "\n",
    "            # ✅ Log intermediate group summary\n",
    "            print(f\"[GROUP SUMMARY {i//batch_size + 1}] {group_summary}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            grouped.append(f\"Error: {e}\")\n",
    "    return grouped\n",
    "\n",
    "\n",
    "# === STEP 4: Final summary of group summaries ===\n",
    "def finalize_section_summary(grouped_summaries, heading):\n",
    "    all_text = \"\\n\\n\".join(grouped_summaries)\n",
    "    print(f\"\\n[INFO] Creating detailed final summary for section: {heading}...\")\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Final synthesis for section: {heading}\\n\\n\"\n",
    "                    f\"Partial summaries:\\n{all_text}\\n\\n\"\n",
    "                    f\"Task: Write a detailed, structured, and comprehensive final summary. \"\n",
    "                    f\"Length should be equivalent to 1.5–2 pages of text (~800–1200 words). \"\n",
    "                    f\"Organize the summary into clear paragraphs (at least 6–10). \"\n",
    "                    f\"Cover all critical points, technical insights, and nuances from the grouped summaries. \"\n",
    "                    f\"Do not shorten excessively, and ensure readability. \"\n",
    "                    f\"Avoid redundant filler and skip citation numbers.\"\n",
    "                )\n",
    "            }],\n",
    "            temperature=0.3,\n",
    "            max_completion_tokens=1500,  # ✅ supports ~2 pages\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# === MASTER FUNCTION ===\n",
    "def process_section(text, section_heading):\n",
    "    print(f\"\\n--- Processing Section: {section_heading} ---\")\n",
    "\n",
    "    result = {\n",
    "        \"section\": section_heading,\n",
    "        \"chunk_summaries\": [],\n",
    "        \"group_summaries\": [],\n",
    "        \"final_summary\": \"\"\n",
    "    }\n",
    "\n",
    "    # Step 1: Chunk section\n",
    "    chunks = chunk_text_with_headings(text)\n",
    "\n",
    "    # Step 2: Summarize chunks\n",
    "    chunk_summaries = []\n",
    "    for heading, chunk in chunks:\n",
    "        summary = summarize_chunk_with_groq(chunk, heading)\n",
    "        chunk_summaries.append(summary)\n",
    "    result[\"chunk_summaries\"] = chunk_summaries\n",
    "\n",
    "    # Step 3: Batch summarize\n",
    "    grouped_summaries = batch_summarize_summaries(chunk_summaries, section_heading, batch_size=5)\n",
    "    result[\"group_summaries\"] = grouped_summaries\n",
    "\n",
    "    # Step 4: Final summary\n",
    "    final_summary = finalize_section_summary(grouped_summaries, section_heading)\n",
    "    result[\"final_summary\"] = final_summary\n",
    "\n",
    "    print(f\"\\n[FINAL SUMMARY for {section_heading}] {final_summary}\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# === CALLING EXAMPLE ===\n",
    "# Suppose `intro_text` contains the \"Introduction\" section of the paper:\n",
    "# intro_result = process_section(intro_text, \"Introduction\")\n",
    "# print(intro_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5189f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Section: Full Document ---\n",
      "\n",
      "[INFO] Merging batch 1 for section: Full Document...\n",
      "[GROUP SUMMARY 1] This paper introduces PE-GPT, a novel multimodal large language model specifically designed for power electronics (PE) design. PE-GPT overcomes the limitations of general LLMs by incorporating a PE knowledge base, integrating with metaheuristic algorithms, and utilizing a Model Zoo and Simulation Repository.  Evaluations through case studies and hardware experiments demonstrate PE-GPT's superior correctness, consistency, and design optimization capabilities compared to both human experts and other LLMs.  The research highlights PE-GPT's potential for generalizability to various PE applications while raising ethical considerations for AI in this field.  Led by a team of renowned researchers, this work emphasizes the crucial role of effective PE design in transitioning to a sustainable energy future.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 2 for section: Full Document...\n",
      "[GROUP SUMMARY 2] Power electronics (PE) design is becoming increasingly complex, prompting exploration of AI assistance. While large language models (LLMs) like ChatGPT show promise, their lack of domain expertise hinders their application in PE. To address this, PE-GPT, a novel multimodal LLM, is proposed. PE-GPT integrates a PE-specific knowledge base through Retrieval Augmented Generation (RAG) and combines LLMs with metaheuristic algorithms, a Model Zoo, and a Simulation Repository. This hybrid framework enables PE-GPT to process both textual and quantitative data, facilitating design optimization, simulation verification, and fine-tuning of data-driven models.  Case studies demonstrate PE-GPT's potential to revolutionize PE design by enhancing efficiency and accuracy.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 3 for section: Full Document...\n",
      "[GROUP SUMMARY 3] This paper introduces PE-GPT, a hybrid framework that combines a large language model (LLM) with metaheuristic algorithms, a Model Zoo, and a Simulation Repository to enhance LLM flexibility for power electronics (PE) design. PE-GPT leverages a Retrieval Augmented Generation (RAG) technique to address domain expertise limitations, allowing it to handle both textual queries and PE-specific fine-tuning data like waveforms. The authors explore three customization techniques for LLMs: chain-of-thoughts prompting, RAG, and fine-tuning, ultimately proposing a tailored approach combining prompt engineering and RAG due to their flexibility and simplicity.  PE-GPT is designed to be adaptable to various power converters, with a focus on the DAB converter. This research demonstrates the potential of LLMs for intelligent PE design and explores future directions and ethical considerations.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 4 for section: Full Document...\n",
      "[GROUP SUMMARY 4] PE-GPT is a hybrid framework that combines generative pre-trained transformer models (LLMs) with simulation tools to streamline power electronics (PE) design.  It leverages a Model Zoo of data-driven models for design exploration and verification, enabling users to request simulation verification of LLM-generated designs.  Discrepancies between model outputs and practical results can be addressed through fine-tuning workflows.  PE-GPT incorporates Retrieval Augmented Generation (RAG) to enhance LLM performance in PE-specific tasks by accessing technical details from knowledge bases like academic literature.  RAG utilizes chunking, embedding, and retrieval techniques to improve the accuracy and relevance of LLM responses.  Fine-tuning hyperparameters such as chunk size and top-k is crucial for optimizing the retrieval effectiveness and efficiency of the RAG system.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 5 for section: Full Document...\n",
      "[GROUP SUMMARY 5] This research introduces PE-GPT, a novel multimodal framework for parametric engineering (PE) design optimization, particularly in power electronics. PE-GPT leverages a Model Zoo containing diverse models, including Physics-Informed Neural Networks (PANN), to evaluate design performance.  A key component is a customized PANN model trained on a three-part dataset to predict inductor current dynamics, enabling efficient and explainable design. PE-GPT integrates a metaheuristic algorithm that utilizes these predictions to optimize modulation parameters, enhancing efficiency and reliability of power electronic systems. The framework's flexibility is further enhanced by a Retrieval Augmented Generation (RAG) system, allowing for customizable knowledge bases tailored to specific applications. This approach combines the strengths of physics-based modeling and machine learning, paving the way for advanced PE design solutions.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 6 for section: Full Document...\n",
      "[GROUP SUMMARY 6] This research introduces PE-GPT, a framework for parameter engineering in power electronics using a large language model (LLM). PE-GPT leverages a curated Model Zoo and a RAG component to enhance generalization and access to current information.  The framework's \"training-free\" domain transfer capability allows adaptation to various power electronic topologies and operating conditions. A case study using a DAB converter demonstrates PE-GPT's effectiveness in predicting time-series performance and optimizing modulation parameters for current stress reduction and soft switching.  Through an iterative design process, PE-GPT showcases its capabilities in adaptive semantic understanding, PE-specific knowledge and reasoning, and multimodal processing. This research highlights the potential of AI in accelerating and improving the design process for complex electronic systems.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 7 for section: Full Document...\n",
      "[GROUP SUMMARY 7] This document explores advanced modulation strategies for power converters, focusing on minimizing inductor current stress and maximizing efficiency.  Triple Phase Shift (TPS) modulation is presented as a promising solution due to its ability to independently control phase shifts, but its soft-switching range may be limited in certain applications.  A 5DoF modulation scheme is proposed as an alternative when TPS falls short, offering a wider soft-switching range and enhanced efficiency under varying load conditions.  \n",
      "\n",
      "The document introduces PE-GPT, a multimodal platform leveraging GPT-4 and Retriever-Augmented Generation (RAG) to design optimal modulation parameters. PE-GPT combines linguistic understanding with domain-specific knowledge to reason about power electronics design and recommend strategies.  It further refines its accuracy by incorporating user feedback and experimental data, bridging the gap between textual and experimental knowledge in the field.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 8 for section: Full Document...\n",
      "[GROUP SUMMARY 8] This research explores the effectiveness of physics-informed artificial neural networks (PANNs) and PE-GPT, a data-driven model for personalized education, in various engineering applications.  PANNs demonstrate significant accuracy improvements in predicting inductor current, outperforming existing models like TST and PINN.  Fine-tuning PANN's hyperparameters is crucial for optimal performance.  PE-GPT, fine-tuned for circuit parameter design and buck converter design, achieves high accuracy, fast response times, and demonstrates capabilities in understanding user requirements, automating design processes, and providing rationale based on specialized knowledge.  Optimal hyperparameter settings for PE-GPT's RAG component are identified as chunk size = 512 and top-k = 7.  These findings highlight the potential of data-driven models in enhancing engineering design and analysis.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 9 for section: Full Document...\n",
      "[GROUP SUMMARY 9] PE-GPT, a large language model trained on power electronics (PE) domain knowledge, demonstrates exceptional capabilities in circuit design optimization.  It successfully designs circuits, such as an output LC filter for a buck converter, meeting desired specifications for voltage and current ripple while considering power density.  PE-GPT's performance is validated through comparison with PLECS simulations and human expert designs, achieving accurate ripple predictions.  Furthermore, PE-GPT outperforms both human experts and state-of-the-art LLMs in understanding and responding to PE-related queries, showcasing its effectiveness in comprehending complex PE concepts.  This research highlights PE-GPT's potential as a valuable tool for assisting engineers in designing efficient and reliable PE circuits.  Its ability to understand design requirements, automate the design process, and provide rationale for design outcomes makes it a promising advancement in the field of power electronics.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 10 for section: Full Document...\n",
      "[GROUP SUMMARY 10] This collection of research explores advancements in both artificial intelligence and power electronics. PE-GPT, a novel AI model, significantly outperforms existing LLMs in recommendation accuracy by leveraging a specialized RAG system for enhanced reasoning and reduced hallucination.  \n",
      "\n",
      "In the realm of power electronics, a 5-degree-of-freedom (5DoF) modulation strategy for power converters is demonstrated, achieving both zero-voltage switching (ZVS) and zero-current switching (ZCS) across various load conditions. This 5DoF strategy outperforms existing methods in terms of efficiency, current stress, and soft switching capabilities, particularly at light loads.  \n",
      "\n",
      "These findings highlight the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 11 for section: Full Document...\n",
      "[GROUP SUMMARY 11] PE-GPT is a groundbreaking multimodal AI tool designed specifically for power electronics (PE) design.  Leveraging a retrieval augmented generation (RAG) system and a customized PE knowledge base, PE-GPT surpasses the limitations of general-purpose LLMs by enabling PE-specific reasoning and knowledge access. This innovative framework supports diverse PE design workflows, from topology selection to documentation, and demonstrates its effectiveness through real-world case studies.  However, the authors emphasize the crucial need to address ethical considerations such as data privacy, security, and potential bias in training data.  Mitigating these risks through robust security measures, diverse datasets, fairness-aware algorithms, and transparent decision-making processes is essential for responsible implementation.  Ultimately, PE-GPT is poised to revolutionize the PE industry, augmenting engineers' capabilities and accelerating the design process.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 12 for section: Full Document...\n",
      "[GROUP SUMMARY 12] This document explores advancements in various fields, including power electronics, grid-connected photovoltaic systems, and artificial intelligence.  \n",
      "\n",
      "It highlights the success of PE-GPT, a multimodal LLM, in power electronics design, showcasing its superior performance compared to human experts and other LLMs.  \n",
      "\n",
      "The research also delves into the potential and challenges of using large language models like GPT-4 in medicine, emphasizing the need for ethical considerations and responsible deployment.  \n",
      "\n",
      "Furthermore, the document examines the application of metaheuristic algorithms in multimedia big data and explores innovative approaches in power converter design and modeling.  \n",
      "\n",
      "Finally, it introduces open-source models like Llama 2 and Gemini, emphasizing their capabilities and potential impact across diverse domains.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 13 for section: Full Document...\n",
      "[GROUP SUMMARY 13] This document explores the intersection of artificial intelligence (AI) and power electronics, particularly in the design of dual active bridge DC-DC converters.  Research highlights AI-driven modulation strategies for improved efficiency and zero-voltage switching, with physics-informed AI models like PE-GPT showing promise for interactive and efficient design.  The work of Dr. Lin and Dr. Li, both leading researchers in the field, is featured, emphasizing AI-powered converter design, life cycle management, and the importance of responsible AI implementation and explainability.  Dr. Li's team, including Weihao Lei and Prof. Juan J. Rodríguez-Andina, investigates AI applications in power electronics and physics-informed systems. Prof. Rodríguez-Andina's expertise lies in implementing intelligent sensors and control algorithms in embedded systems, a field recognized by numerous IEEE awards.  While exploring the potential of AI, the document acknowledges the ethical implications, including fairness and bias, urging careful consideration for its beneficial use.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 14 for section: Full Document...\n",
      "[GROUP SUMMARY 14] This document profiles several researchers specializing in industrial electronics, particularly power electronics, control systems, and renewable energy.  Antoni, Changyun Wen, and Xin Zhang are highlighted for their expertise and significant contributions to the field, evidenced by their highly cited publications and leadership roles in IEEE.  Changyun Wen holds a professorship at Nanyang Technological University and is a Fellow of IEEE, while Professor Wen serves as editor-in-chief for multiple journals.  Xin Zhang and Hao Ma, both affiliated with Zhejiang University, focus on power electronics and electrical engineering respectively, with international research experience. Dr. Ma's research delves into advanced control, wireless power transfer, and fault diagnosis, significantly advancing power electronics technology.\n",
      "\n",
      "\n",
      "[INFO] Merging batch 15 for section: Full Document...\n",
      "[GROUP SUMMARY 15] This document details the professional accomplishments of an individual affiliated with the China Power Supply Society.  They held a significant role as an AdCom Member of the IEEE Industrial Electronics Society, demonstrating their expertise in the field.  Their leadership is evident in their chairing of multiple IEEE conferences, including ISIE 2012, PEAC 2014, PEAC 2018, and PEAS 2021. These conferences likely focused on power electronics and industrial applications, given their association with the IEEE Industrial Electronics Society.  Their contributions to these organizations highlight their prominent position within the power electronics community.  Overall, this document showcases their impactful career and contributions to the field.\n",
      "\n",
      "\n",
      "[INFO] Creating detailed final summary for section: Full Document...\n",
      "\n",
      "[FINAL SUMMARY for Full Document] **Introduction to PE-GPT and its Significance**\n",
      "\n",
      "The introduction of PE-GPT, a novel multimodal large language model, marks a significant advancement in the field of power electronics (PE) design. Specifically designed to overcome the limitations of general large language models (LLMs), PE-GPT incorporates a PE knowledge base, integrates with metaheuristic algorithms, and utilizes a Model Zoo and Simulation Repository. This innovative framework enables PE-GPT to process both textual and quantitative data, facilitating design optimization, simulation verification, and fine-tuning of data-driven models. Evaluations through case studies and hardware experiments demonstrate PE-GPT's superior correctness, consistency, and design optimization capabilities compared to both human experts and other LLMs.\n",
      "\n",
      "**Technical Overview of PE-GPT**\n",
      "\n",
      "PE-GPT leverages a Retrieval Augmented Generation (RAG) technique to address domain expertise limitations, allowing it to handle both textual queries and PE-specific fine-tuning data like waveforms. The authors explore three customization techniques for LLMs: chain-of-thoughts prompting, RAG, and fine-tuning, ultimately proposing a tailored approach combining prompt engineering and RAG due to their flexibility and simplicity. PE-GPT is designed to be adaptable to various power converters, with a focus on the DAB converter. This research demonstrates the potential of LLMs for intelligent PE design and explores future directions and ethical considerations. The framework's flexibility is further enhanced by a RAG system, allowing for customizable knowledge bases tailored to specific applications.\n",
      "\n",
      "**Applications and Case Studies**\n",
      "\n",
      "The effectiveness of PE-GPT is demonstrated through various case studies, showcasing its potential to revolutionize PE design by enhancing efficiency and accuracy. For instance, PE-GPT successfully designs circuits, such as an output LC filter for a buck converter, meeting desired specifications for voltage and current ripple while considering power density. PE-GPT's performance is validated through comparison with PLECS simulations and human expert designs, achieving accurate ripple predictions. Furthermore, PE-GPT outperforms both human experts and state-of-the-art LLMs in understanding and responding to PE-related queries, showcasing its effectiveness in comprehending complex PE concepts.\n",
      "\n",
      "**Advancements in Power Electronics**\n",
      "\n",
      "In addition to PE-GPT, the research explores advancements in power electronics, including the development of a 5-degree-of-freedom (5DoF) modulation strategy for power converters. This strategy achieves both zero-voltage switching (ZVS) and zero-current switching (ZCS) across various load conditions, outperforming existing methods in terms of efficiency, current stress, and soft switching capabilities. The 5DoF strategy is particularly effective at light loads, making it a promising solution for power electronic systems. These findings highlight the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion.\n",
      "\n",
      "**Ethical Considerations and Responsible Implementation**\n",
      "\n",
      "While PE-GPT and other AI-powered solutions offer significant benefits, the authors emphasize the crucial need to address ethical considerations such as data privacy, security, and potential bias in training data. Mitigating these risks through robust security measures, diverse datasets, fairness-aware algorithms, and transparent decision-making processes is essential for responsible implementation. The research highlights the importance of careful consideration for the beneficial use of AI in power electronics, ensuring that these technologies are developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "**Intersection of AI and Power Electronics**\n",
      "\n",
      "The research explores the intersection of artificial intelligence (AI) and power electronics, particularly in the design of dual active bridge DC-DC converters. AI-driven modulation strategies for improved efficiency and zero-voltage switching are investigated, with physics-informed AI models like PE-GPT showing promise for interactive and efficient design. The work of leading researchers in the field, including Dr. Lin and Dr. Li, is featured, emphasizing AI-powered converter design, life cycle management, and the importance of responsible AI implementation and explainability.\n",
      "\n",
      "**Contributions of Researchers and Experts**\n",
      "\n",
      "The document profiles several researchers specializing in industrial electronics, particularly power electronics, control systems, and renewable energy. Antoni, Changyun Wen, and Xin Zhang are highlighted for their expertise and significant contributions to the field, evidenced by their highly cited publications and leadership roles in IEEE. The contributions of these researchers, along with others, have significantly advanced power electronics technology and demonstrate the importance of collaboration and knowledge sharing in driving innovation.\n",
      "\n",
      "**Professional Accomplishments and Leadership**\n",
      "\n",
      "The document details the professional accomplishments of an individual affiliated with the China Power Supply Society, demonstrating their expertise in the field. Their leadership is evident in their chairing of multiple IEEE conferences, including ISIE 2012, PEAC 2014, PEAC 2018, and PEAS 2021. These conferences likely focused on power electronics and industrial applications, given their association with the IEEE Industrial Electronics Society. Their contributions to these organizations highlight their prominent position within the power electronics community.\n",
      "\n",
      "**Conclusion and Future Directions**\n",
      "\n",
      "In conclusion, PE-GPT represents a significant advancement in the field of power electronics design, offering superior correctness, consistency, and design optimization capabilities compared to human experts and other LLMs. The research highlights the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion. As the field continues to evolve, it is essential to address ethical considerations and ensure responsible implementation of these technologies. The contributions of researchers and experts, along with the leadership and professional accomplishments of individuals in the field, will drive innovation and advancement in power electronics, ultimately shaping the future of this critical technology.\n",
      "\n",
      "\n",
      "=== RESULTS ===\n",
      "\n",
      "--- Chunk Summaries ---\n",
      "[Chunk 1] This paper explores the application of Large Language Models (LLMs) in power electronics (PE) design.  It addresses the limitations of LLMs in this field due to a lack of specialized PE knowledge and challenges in handling PE-specific data. The authors propose a novel approach, PE-GPT, to overcome these hurdles and enable the utilization of LLMs for PE design.  PE-GPT aims to enhance the capabilities of LLMs by incorporating PE-specific expertise and data processing techniques.\n",
      "[Chunk 2] This study introduces PE-GPT, a multimodal large language model tailored for power electronics (PE) design.  PE-GPT leverages a PE knowledge base and integrates with metaheuristic algorithms, a Model Zoo, and a Simulation Repository to enhance its capabilities.  Evaluation through two case studies demonstrates PE-GPT's superior correctness and consistency compared to both human experts and other leading LLMs. Hardware experiments validate PE-GPT's ability to optimize PE designs.\n",
      "[Chunk 3] This paper introduces PE-GPT, a novel physics-informed AI model for power electronics design. PE-GPT leverages multimodal capabilities to optimize power converter designs, as validated through hardware experiments.  The research explores the model's generalizability to other power electronics applications and raises ethical considerations for AI in this field.  Future directions are proposed to advance the intelligent development of power electronics.\n",
      "[Chunk 4] This research, conducted by a team of authors from Zhejiang University, the University of Illinois Urbana-Champaign, the University of Arkansas at Fayetteville, and the University of Vigo, explores a topic related to publications.  The authors have a combined total of over 300 publications and thousands of citations, indicating significant contributions to their respective fields.  The research was uploaded on October 8, 2024, by J.J. Rodriguez-Andina.\n",
      "[Chunk 5] Power electronics (PE) are crucial for integrating sustainable energy sources into the grid.  Sustainable energy sources like solar and wind are intermittent, posing challenges to grid stability. PE devices mitigate these challenges by regulating voltage and frequency, and they work with energy storage systems to balance supply and demand.  Effectively designed PE is essential for a smooth transition to a sustainable energy future.\n",
      "[Chunk 6] Power electronics (PE) design is a complex process requiring specialized expertise due to the increasing scale and complexity of PE systems in evolving grids.  Large language models (LLMs) like ChatGPT and GPT-4, trained on massive datasets, offer potential for assisting humans in various domains, including PE design. LLMs' vast linguistic and factual knowledge, derived from their pre-training, could be leveraged to aid in the design process.\n",
      "[Chunk 7] Large language models (LLMs) are powerful AI systems trained on massive text datasets, enabling them to perform various language-related tasks.  Recent research explores specialized applications of LLMs in fields like healthcare and transportation.  Despite their potential, LLMs face challenges when applied to power engineering (PE) design due to their lack of domain-specific knowledge.  Bridging this gap is crucial for effectively integrating LLMs into PE workflows.\n",
      "[Chunk 8] This paper investigates the challenges of incorporating Large Language Models (LLMs) into Power Electronics (PE) design.  LLMs lack domain expertise and struggle with unstructured PE data like time-series waveforms.  To overcome these limitations, the authors propose PE-GPT, a multimodal LLM enhanced with a PE-specific knowledge base through Retrieval Augmented Generation (RAG). This allows PE-GPT to access specialized knowledge and potentially contribute to more accurate and insightful PE design solutions.\n",
      "[Chunk 9] This paper introduces PE-GPT, a novel framework for power electronics design that combines a large language model (LLM) with metaheuristic algorithms, a Model Zoo, and a Simulation Repository. PE-GPT enables three key workflows: design optimization using the Model Zoo and metaheuristics, simulation verification through embedded simulation software, and fine-tuning of data-driven models. This hybrid approach aims to enhance the efficiency and accuracy of power electronics design processes.\n",
      "[Chunk 10] This research introduces PE-GPT, a multimodal large language model specifically designed for electronic circuit design. PE-GPT overcomes limitations of traditional LLMs by integrating metaheuristic algorithms, a Model Zoo, and a Simulation Repository, enabling it to process quantitative design data and perform optimized circuit design. This hybrid framework offers three distinct workflows, enhancing flexibility and applicability to various design tasks.  The authors demonstrate PE-GPT's capabilities through two case studies, showcasing its potential to revolutionize electronic circuit design.\n",
      "[Chunk 11] This paper introduces PE-GPT, a hybrid framework combining a large language model (LLM) with metaheuristic algorithms, a Model Zoo, and a Simulation Repository. PE-GPT enhances LLM flexibility for power electronics (PE) design by incorporating a knowledge-associated retrieval augmented generation (RAG) technique to address domain expertise limitations.  The framework supports three workflows, integrating PE-GPT into existing design processes.  This research demonstrates the potential of LLMs for intelligent PE design and explores future directions and ethical considerations.\n",
      "[Chunk 12] This paper explores the application of large language models (LLMs) to the field of power electronics (PE).  LLMs like GPT-4, while powerful, require adaptation for technical tasks in PE.  The authors discuss three common methods for tailoring LLMs: prompt engineering, retrieval augmented generation (RAG), and fine-tuning.  Prompt engineering, specifically chain of thoughts (CoT), is highlighted as a technique for guiding LLMs to reason through complex PE problems.\n",
      "[Chunk 13] This section explores three techniques for customizing Large Language Models (LLMs): chain-of-thoughts prompting, Retrieval Augmented Generation (RAG), and model fine-tuning.  Each technique is evaluated based on accuracy, implementation simplicity, explainability, computational cost, and knowledge base update flexibility.  The authors propose a tailored approach combining prompt engineering and RAG due to their flexibility and simplicity.\n",
      "[Chunk 14] This paper proposes a multimodal PE-GPT model for power electronics design, leveraging a hybrid framework that combines a large language model (LLM) with a Retrieval Augmented Generation (RAG) technique.  The PE-GPT model is designed to be adaptable to various power converters, with a focus on the DAB converter as a primary example.  The authors highlight the integration of Model Zoo and physics-in-architecture neural networks (PANNs) within this framework.\n",
      "[Chunk 15] This paper proposes a hybrid framework combining a pre-trained Generative Pre-trained Transformer (GPT) model with a Retrieval Augmented Generation (RAG) process for enhanced performance in a specific domain (PE). The framework utilizes a domain-specific knowledge base to augment the LLM's understanding through RAG, enabling it to handle both textual queries and PE-specific fine-tuning data like waveforms. This hybrid approach aims to leverage the strengths of both pre-trained language models and domain-specific knowledge for improved performance in PE applications.\n",
      "[Chunk 16] This research presents an LLM agent capable of handling both textual and PE-specific data like time-series waveforms.  The agent utilizes three workflows: Design with Model Zoo, Simulation Verification, and PE-Specific Fine-tuning.  Workflow 1 employs a metaheuristic algorithm and a Model Zoo to optimize design parameters, overcoming limitations of traditional LLMs in processing PE data.  Workflow 2 facilitates integration of LLM-generated designs into existing simulation environments.\n",
      "[Chunk 17] This paper proposes PE-GPT, a hybrid framework that combines generative pre-trained transformer models with simulation tools for power electronics design. PE-GPT leverages a Model Zoo of data-driven models for design exploration and verification.  Users can request simulation verification of designs, and discrepancies between model outputs and practical results can be addressed through fine-tuning workflows. This framework aims to streamline and enhance the power electronics design process.\n",
      "[Chunk 18] This research investigates techniques to enhance the performance of Large Language Models (LLMs) in the field of personalized education (PE).  The study focuses on Retrieval Augmented Generation (RAG), which leverages external knowledge bases to address the inherent limitations of LLMs in domain-specific expertise.  RAG is shown to improve the accuracy and relevance of LLM responses in PE applications by providing access to technical details found in documents like academic literature.  The proposed PE-GPT framework utilizes RAG to bridge the knowledge gap and enhance LLM performance in PE-specific tasks.\n",
      "[Chunk 19] This text describes a Retrieval-Augmented Generation (RAG) system called PE-GPT, designed to enhance the accuracy and relevance of AI-generated outputs in technical domains. RAG leverages a knowledge base of technical documents to overcome the limitations of large language models (LLMs) in specific expertise areas.  By chunking, embedding, and retrieving information from this knowledge base, RAG allows for both customization and generalization across diverse technical applications.\n",
      "[Chunk 20] This section describes the model zoo and the chosen PANN model for a specific task.  The authors highlight the importance of two hyperparameters: chunk size and top-k, which influence the retrieval effectiveness and efficiency of the model.  Fine-tuning these parameters is crucial for optimizing the performance of the model on the task.\n",
      "[Chunk 21] This research explores a workflow for parametric engineering (PE) design optimization using a Model Zoo containing different types of models. The Model Zoo includes white-box (analytical), black-box (AI-driven), and gray-box (hybrid) models, allowing for adaptability to various PE tasks.  The study focuses on Physics-Informed Neural Networks (PANN) as a representative gray-box model within this framework.  The Model Zoo's role is to provide function evaluations for an iterative metaheuristic algorithm that seeks the optimal design outcome.\n",
      "[Chunk 22] This research presents a Model Zoo for PE (presumably Power Electronics) design, showcasing the adaptability of the platform to various tasks. The paper focuses on PANN, a novel PINN (Physics-Informed Neural Network) method with a recurrent architecture designed to incorporate circuit physics. PANN's lightweight and explainable nature allows for efficient and flexible design of PE devices by evaluating performance metrics like current stress and soft switching conditions.  The authors highlight the potential of PANN's numerical differentiation approach to characterize general circuit physics behavior.\n",
      "[Chunk 23] This research presents a novel physics-informed generative pre-trained transformer (PE-GPT) framework for DAB converter design.  The framework utilizes a customized PANN model that incorporates inductor current dynamics using an implicit Euler algorithm.  Model Zoo, a component of the PE-GPT suite, houses various models like PANN, which are evaluated for performance based on design requests from a large language model (LLM) agent.  This approach leverages the strengths of both physics-based modeling and machine learning to optimize DAB converter design.\n",
      "[Chunk 24] This research utilizes a novel metaheuristic algorithm, PE-GPT, to optimize modulation parameters for power electronics.  The algorithm leverages a pre-trained artificial neural network (PANN) to evaluate the performance of candidate modulation strategies based on time-domain inductor current predictions.  Performance metrics such as current stress and soft switching quality are used to guide the evolutionary process and refine the modulation parameters. This approach aims to enhance the efficiency and reliability of power electronic systems.\n",
      "[Chunk 25] PE-GPT is a multimodal model that combines text and time-series data for function evaluation in metaheuristic algorithms. It utilizes a PANN architecture trained on a three-part dataset to predict iL* values from input time-series data.  The model's generalization ability is enhanced by a RAG system, allowing for a customizable knowledge base that can be tailored to specific domains and objectives. This flexibility enables PE-GPT to adapt to diverse applications and improve its performance.\n",
      "[Chunk 26] This research proposes PE-GPT, a framework for parameter engineering (PE) using a large language model (LLM).  PE-GPT leverages a curated repository of models (Model Zoo) to enhance generalization capabilities by selecting appropriate models for different PE tasks.  The authors highlight the importance of regularly updating the RAG (Retrieval Augmented Generation) component of PE-GPT to ensure access to current information. Building a comprehensive Model Zoo is presented as a long-term collaborative effort within the PE community.\n",
      "[Chunk 27] This research proposes a physics-informed generative pre-trained transformer (PE-GPT) for optimizing modulation parameters in power electronic (PE) converters.  PE-GPT leverages a \"training free\" domain transfer capability, enabling adaptation to various PE topologies and operating conditions.  The DAB converter is used as a case study to demonstrate the effectiveness of the proposed methodology.  The model aims to predict time-series performance for different modulation parameter selections.\n",
      "[Chunk 28] This research explores the design of a DAB (Dual Active Bridge) converter, focusing on its 5DoF (five degrees of freedom) modulation scheme for power transfer regulation.  The 5DoF scheme offers flexibility through adjustable phase shifts (D0, D1, D2) and duty cycles (φ1, φ2).  The design process prioritizes balancing implementation complexity with performance metrics like current stress, switching ranges, and efficiency.\n",
      "[Chunk 29] This study investigates a power electronic design case with a specific voltage range, power rating, and switching frequency. A hardware prototype was built using SiC semiconductors and a nanocrystalline ribbon transformer to collect data for analysis.  A GPT-4 based language model was trained on research articles about modulation design to assist in understanding the system's behavior.  The analysis focuses on the impact of key transformer parameters (leakage inductance, resistance, and turn ratio) on the system's performance.\n",
      "[Chunk 30] This research presents a case study demonstrating the iterative design process for a DAB converter's modulation strategy using PE-GPT, a powerful AI assistant.  The study focuses on achieving current stress reduction and soft switching, analyzing PE-GPT's performance in three key areas: adaptive semantic understanding, PE-specific knowledge and reasoning, and multimodal processing.  Through four conversational stages, PE-GPT assists in generating modulation strategies, analyzing parameters, and refining the design based on user feedback.  The research highlights the potential of AI in accelerating and improving the design process for complex electronic systems.\n",
      "[Chunk 31] This dialogue excerpt demonstrates a human-AI interaction focused on designing a modulation strategy for a dual-active-bridge converter. The user seeks a phase shift modulation method that minimizes inductor current. The AI recommends Triple Phase Shift (TPS) modulation due to its ability to independently control phase shifts, leading to reduced inductor current stress.\n",
      "[Chunk 32] This research compares two modulation strategies, TPS and SPS, for a converter. TPS outperforms SPS in terms of current stress reduction and efficiency.  However, the TPS soft-switching range might be insufficient for some applications. The 5DoF modulation scheme is suggested as an alternative when TPS's soft-switching range is inadequate.\n",
      "[Chunk 33] This text discusses the advantages of using a 5DoF modulation scheme for power converters, highlighting its superior soft-switching range compared to phase shift modulation. The 5DoF scheme allows for independent control of five parameters, leading to enhanced efficiency, particularly under varying load conditions.  Experimental data is sought to further refine the design accuracy of the 5DoF modulation strategy.\n",
      "[Chunk 34] This paper presents PE-GPT, a multimodal platform for designing modulation parameters. PE-GPT leverages GPT-4's linguistic capabilities and a RAG technique for adaptive semantic understanding.  It employs a metaheuristic algorithm for initial design and iteratively refines the modulation strategy based on user feedback.  Finally, PE-GPT enhances its accuracy by incorporating user experimental data.\n",
      "[Chunk 35] PE-GPT is a language model enhanced by Retriever-Augmented Generation (RAG) and domain-specific knowledge, enabling it to understand and adapt to user requirements in power electronics (PE).  It demonstrates strong reasoning capabilities by recommending optimal modulation strategies and analyzing PE design performance.  PE-GPT's integration with PANN allows for multimodal processing, incorporating time-series data from hardware experiments to refine its accuracy. This multimodal approach distinguishes PE-GPT from the original GPT framework, bridging the gap between textual and experimental data in the PE domain.\n",
      "[Chunk 36] This study fine-tunes a physics-informed artificial neural network (PANN) model to improve its accuracy in predicting inductor current (iL).  Fine-tuning reduces the test mean square error (MSEtest) by 84% and increases the coefficient of determination (R2) by 4.88%. The fine-tuning process adjusts the model's parameters (L, n, and RL) to better align with experimental measurements of inductor current.\n",
      "[Chunk 37] This research presents PANN, a novel data-driven model for time-series prediction.  PANN outperforms existing models like TST and PINN, achieving significant accuracy improvements in both in-domain and out-of-domain scenarios. Notably, PANN demonstrates high robustness with minimal accuracy fluctuations across multiple algorithm repetitions.  Fine-tuning of PANN's hyperparameters is crucial for optimal performance.\n",
      "[Chunk 38] This research investigates the performance of PE-GPT, a data-driven model for personalized education, across in-domain and out-of-domain scenarios.  Fine-tuning the RAG component's hyperparameters (chunk size and top-k) is crucial for balancing information precision, query efficiency, and faithfulness to the PE knowledge base.  The study evaluates PE-GPT using metrics like hit rate, mean reciprocal rank, response speed, faithfulness, and relevancy to comprehensively assess its performance.  Optimal hyperparameter settings (chunk size = 512, top-k = 7) enhance response completeness and relevance.\n",
      "[Chunk 39] This research investigates the performance of a PE-GPT model fine-tuned for circuit parameter design.  The model achieves high accuracy (HR, FF, Rel close to 1) and fast response times, demonstrating optimal performance.  The optimal chunk size and top-k parameters for the RAG component were identified as 512 and 7, respectively.  The model is applied to a design case involving the optimization of inductance and capacitance for a buck converter, considering ripple constraints and power density.\n",
      "[Chunk 40] This research investigates the performance of a PE-GPT large language model (LLM) in assisting with buck converter design.  PE-GPT demonstrates capabilities in understanding user requirements, automating the design process, and providing rationale based on specialized knowledge.  The model utilizes two workflows: one for parameter optimization and another for circuit simulation verification.  Overall, PE-GPT shows promise as a tool for streamlining and enhancing the buck converter design process.\n",
      "[Chunk 41] PE-GPT demonstrates capabilities in understanding user design requirements, automating the optimal design process, and providing rationale for design outcomes.  In a case study, PE-GPT successfully designed an output LC filter for a buck converter, achieving desired specifications for output voltage ripple, inductor current ripple, and power density. The designed inductor and capacitor values were 482.7 μH and 12.5 μF, respectively.  These results highlight PE-GPT's potential as a valuable tool for circuit design optimization.\n",
      "[Chunk 42] This research optimized a power converter design to meet voltage and current ripple constraints.  The optimal inductance and capacitance values were determined to achieve a 0.5% output voltage ripple and a 20% inductor current ripple.  A smaller output ripple is crucial for stable load operation and requires a larger capacitor.  Controlling inductor current ripple is also important for efficient operation.\n",
      "[Chunk 43] This research investigates the design and simulation of a DC-DC converter.  The study analyzes the harmonic components of the inductor current and output voltage, finding a strong correlation between predicted and simulated results.  Simulation verification using PLECS software confirms the effectiveness of the proposed design, demonstrating a minimal mean squared error (MSE) between predicted and actual waveforms.  These findings highlight the potential of the proposed design for efficient energy transfer in DC-DC converters.\n",
      "[Chunk 44] This research investigates the ability of PE-GPT, a large language model, to design power electronic (PE) circuits.  PE-GPT demonstrates an understanding of PE design trade-offs, balancing ripple limitations with power density. \n",
      "\n",
      "The model's design performance is validated through comparison with PLECS simulations and human expert designs, achieving accurate voltage and current ripple predictions.  These results suggest PE-GPT's potential for assisting in the design of efficient and reliable PE circuits.\n",
      "[Chunk 45] This research evaluated the performance of PE-GPT, a large language model trained on power electronics (PE) domain knowledge, against human experts and state-of-the-art (SOTA) LLMs. PE-GPT demonstrated superior accuracy (77.2%) in understanding and responding to PE-related queries compared to both human experts and the best-performing SOTA LLM, Claude 3 Sonnet.  These results highlight PE-GPT's effectiveness in comprehending complex PE concepts and its potential for assisting both novice and experienced engineers.\n",
      "[Chunk 46] PE-GPT, a novel AI model, significantly outperforms both human experts and existing LLMs like Claude 3 Sonnet in recommendation accuracy.  This improvement is attributed to PE-GPT's specialized RAG (Retrieval Augmented Generation) system, which enhances its reasoning capabilities and addresses the issue of hallucination common in LLMs. PE-GPT generates structured, rational, and comprehensive explanations for its recommendations, demonstrating its advanced reasoning abilities.\n",
      "[Chunk 47] This research validates a 5-degree-of-freedom (5DoF) modulation strategy for a power electronic converter, demonstrating its multimodal capability.  Hardware experiments show that the strategy achieves zero voltage switching (ZVS) for all eight switches while also realizing zero current switching (ZCS) for some switches under certain load conditions.  The study analyzes the steady-state waveforms and performance of the converter under different power levels.  These findings contribute to the advancement of efficient and reliable power electronic control techniques.\n",
      "[Chunk 48] This research presents a 5DoF strategy for a power converter, achieving both zero-voltage switching (ZVS) and zero-current switching (ZCS) across various load conditions.  Performance analysis demonstrates that the 5DoF strategy outperforms both current-stress-optimized TPS (CSO-TPS) and SPS in terms of current stress (ipp), efficiency (η), and soft switching capabilities.  By adjusting duty cycles, the researchers effectively expanded the ZCS ranges, particularly under light and medium load conditions.\n",
      "[Chunk 49] This research investigates the performance of a novel 5-degree-of-freedom (5DoF) control strategy for a power converter, comparing it to existing strategies (SPS and CSO-TPS).  The 5DoF strategy significantly improves efficiency, especially at light loads, by optimizing soft switching ranges.  While ipp values are comparable across strategies, the CSO-TPS strategy sacrifices ZVS and ZCS performance to achieve lower ipp.\n",
      "[Chunk 50] The PE-GPT framework demonstrates superior performance in optimizing power electronic (PE) designs compared to existing methods.  It achieves this through a hybrid approach that combines the strengths of generative pre-trained transformers with traditional PE design techniques.  The framework's flexibility allows it to be applied across the entire PE design workflow, from circuit parameter optimization to modulation strategy design.  PE-GPT's ability to generalize to diverse PE applications highlights its potential for accelerating and improving the design process.\n",
      "[Chunk 51] PE-GPT is a customizable AI tool designed to assist in the entire process of power electronics (PE) design. It leverages a knowledge base and model zoo to achieve this, encompassing stages from topology design to documentation.  The authors emphasize the importance of addressing potential ethical concerns, particularly regarding data privacy and security, when implementing PE-GPT.\n",
      "[Chunk 52] This section discusses key ethical considerations for PE-GPT, a system using generative AI for design.  It highlights the risks of privacy breaches through data poisoning or parameter manipulation, emphasizing the need for robust security measures.  The authors also address the potential for bias in design outputs due to skewed training data, advocating for diverse datasets and fairness-aware algorithms to mitigate this issue.  Finally, the importance of transparency and accountability in PE-GPT's decision-making processes is stressed.\n",
      "[Chunk 53] This research explores the potential benefits and risks of using PE-GPT, a generative AI model, in the field of product engineering.  The authors highlight the importance of mitigating bias in PE-GPT through diverse data, bias-aware algorithms, and user feedback.  Transparency in design decisions and accountability mechanisms are crucial for building trust and managing potential negative consequences.  PE-GPT is expected to augment the role of engineers, automating routine tasks and accelerating the design process.\n",
      "[Chunk 54] This research introduces PE-GPT, the first multimodal large language model tailored for power electronics (PE) design.  PE-GPT overcomes limitations of existing LLMs by incorporating a retrieval augmented generation (RAG) system with a customized PE knowledge base. This enables PE-GPT to process PE-specific data and enhance its technical expertise. The development of PE-GPT is expected to significantly impact the PE industry, necessitating upskilling and reskilling initiatives for engineers.\n",
      "[Chunk 55] PE-GPT is a novel framework designed to address the challenges of applying large language models to power electronics (PE) design. It combines a retrieval augmented generation approach with a customized PE knowledge base, enabling PE-specific reasoning and knowledge access.  A hybrid framework integrating an LLM agent, metaheuristic algorithms, and a Model Zoo supports various PE design workflows, including model selection, simulation verification, and model fine-tuning. PE-GPT demonstrates its effectiveness through two design case studies, showcasing its ability to adapt and learn from user-specific data.\n",
      "[Chunk 56] PE-GPT, a multimodal LLM, demonstrates strong performance in power electronics (PE) design. Through iterative design and user data fine-tuning, it achieves superior correctness (77.2%) and output consistency (98.8%) compared to both human experts and other LLMs. Hardware experiments validate PE-GPT's ability to design for optimal soft switching performance. This case study highlights PE-GPT's potential for broader application in the PE industry.\n",
      "[Chunk 57] This research explores enhancing the capabilities of PE-GPT, a model for power electronics (PE) design.  Strategies include expanding its knowledge base, improving its ability to handle complex designs, and integrating its outputs into existing design workflows.  The authors aim to make PE-GPT more widely applicable and contribute to the advancement of intelligent PE design.\n",
      "[Chunk 58] This document provides a concise overview of recent research in grid-connected photovoltaic systems and emerging PV converter technology. It also explores hierarchical control strategies for microgrids and the application of artificial intelligence in power converter design.  Finally, the document delves into the evaluation of large language models and introduces open-source models like Llama 2 and Gemini, highlighting their capabilities and potential impact.\n",
      "[Chunk 59] This research explores the potential benefits, limitations, and risks of using large language models like GPT-4 in medicine.  The authors analyze the capabilities of these models for tasks like text generation and question answering, while also discussing ethical concerns and potential biases.  The paper highlights the need for careful evaluation and responsible deployment of AI chatbots in healthcare settings.  Further research is encouraged to address open questions and mitigate potential risks.\n",
      "[Chunk 60] This document reviews metaheuristic algorithms and their applications in computational intelligence for multimedia big data. It explores grey-box modeling for predictive analytics in smart manufacturing and examines adaptive-network-based fuzzy inference systems (ANFIS) for modeling.  Recent research on temporal modeling for power converters using physics-in-architecture recurrent neural networks and optimization techniques for dual active bridge converters is also discussed.\n",
      "[Chunk 61] This research explores the application of artificial intelligence (AI) in power electronics, specifically in the design of dual active bridge DC-DC converters.  Studies investigate AI-driven modulation strategies to improve efficiency and achieve full zero-voltage switching.  The potential of physics-informed AI models, like PE-GPT, is highlighted for interactive and efficient converter design.  However, the ethical implications of AI, including fairness and bias, are also acknowledged and require careful consideration.\n",
      "[Chunk 62] This text presents a selection of research articles focused on artificial intelligence (AI).  The articles explore various aspects of AI, including ethical considerations like bias mitigation and transparency, the impact of AI on employment, and strategies for responsible development and deployment of AI systems, particularly large language models.  The authors highlight the need for careful consideration of the societal implications of AI and the development of safeguards to ensure its beneficial use.\n",
      "[Chunk 63] This document presents the biographies of two researchers, Dr. Lin and Dr. Li, who specialize in power electronics and artificial intelligence.  Dr. Lin's work focuses on AI-powered power converter design, life cycle management, and responsible AI implementation. Dr. Li's research interests encompass power converter and modulation design, AI-driven design automation, and the application of explainable AI in power electronics. Both researchers have received prestigious awards for their contributions to the field.\n",
      "[Chunk 64] This paper explores the intersection of power electronics, physics-informed systems, and AI applications.  Dr. Li, a recipient of several awards for his research, leads the study. Weihao Lei, a master's student, and Juan J. Rodríguez-Andina, a professor, also contribute to the research.\n",
      "[Chunk 65] This text provides a biographical sketch of Prof. Rodríguez-Andina, highlighting his academic affiliations and research expertise.  He focuses on implementing complex control algorithms and intelligent sensors in embedded systems.  His work has been recognized with prestigious awards from the IEEE Industrial Electronics Society, including the Outstanding Paper Award and the Anthony Hornfeck Award.  He has also held leadership positions within the society.\n",
      "[Chunk 66] This text presents the academic and professional background of two individuals, likely experts in the field of industrial electronics.  Both have significant experience with IEEE publications, serving in various editorial roles.  One individual, Josep M. Guerrero, has recently expanded his research interests to include neuroscience, earning additional degrees in psychobiology and sleep medicine.\n",
      "[Chunk 67] This document presents the biographies of two researchers: Antoni  and Changyun Wen. Antoni specializes in microgrids and renewable energy systems, achieving significant recognition for his highly cited publications. Changyun focuses on control systems, cyber-physical systems, and smart grids, holding a professorship at Nanyang Technological University and being a Fellow of IEEE.\n",
      "[Chunk 68] This document describes the research and professional experience of Professor Wen and Xin Zhang. Professor Wen is a renowned expert in control systems, cyber-physical systems, and smart grids, holding prestigious fellowships and serving as editor-in-chief for multiple journals.  Xin Zhang is a senior member of IEEE and holds a Ph.D. in electronic and electrical engineering. Both individuals are actively involved in organizing international conferences.\n",
      "[Chunk 69] This document presents the biographies of two researchers, Xin Zhang and Hao Ma.  Both are affiliated with Zhejiang University in China and hold senior memberships in the IEEE.  Zhang's research focuses on power electronics, power systems, and advanced control theory, while Ma's expertise lies in electrical engineering.  They have both held academic positions at Zhejiang University and have international research experience.\n",
      "[Chunk 70] Dr. Ma is a highly accomplished researcher in power electronics. His expertise spans advanced control, wireless power transfer, fault diagnosis, and diverse applications.  He has authored numerous publications and held leadership positions in professional organizations.  Dr. Ma's work significantly contributes to the advancement of power electronics technology.\n",
      "[Chunk 71] This text highlights the professional affiliations and contributions of an individual associated with the China Power Supply Society.  They served as an AdCom Member of the IEEE Industrial Electronics Society and chaired several IEEE conferences, including ISIE 2012, PEAC 2014, PEAC 2018, and PEAS 2021.  Their expertise likely lies in the field of power electronics and industrial applications.\n",
      "\n",
      "--- Group Summaries ---\n",
      "[Group 1] This paper introduces PE-GPT, a novel multimodal large language model specifically designed for power electronics (PE) design. PE-GPT overcomes the limitations of general LLMs by incorporating a PE knowledge base, integrating with metaheuristic algorithms, and utilizing a Model Zoo and Simulation Repository.  Evaluations through case studies and hardware experiments demonstrate PE-GPT's superior correctness, consistency, and design optimization capabilities compared to both human experts and other LLMs.  The research highlights PE-GPT's potential for generalizability to various PE applications while raising ethical considerations for AI in this field.  Led by a team of renowned researchers, this work emphasizes the crucial role of effective PE design in transitioning to a sustainable energy future.\n",
      "[Group 2] Power electronics (PE) design is becoming increasingly complex, prompting exploration of AI assistance. While large language models (LLMs) like ChatGPT show promise, their lack of domain expertise hinders their application in PE. To address this, PE-GPT, a novel multimodal LLM, is proposed. PE-GPT integrates a PE-specific knowledge base through Retrieval Augmented Generation (RAG) and combines LLMs with metaheuristic algorithms, a Model Zoo, and a Simulation Repository. This hybrid framework enables PE-GPT to process both textual and quantitative data, facilitating design optimization, simulation verification, and fine-tuning of data-driven models.  Case studies demonstrate PE-GPT's potential to revolutionize PE design by enhancing efficiency and accuracy.\n",
      "[Group 3] This paper introduces PE-GPT, a hybrid framework that combines a large language model (LLM) with metaheuristic algorithms, a Model Zoo, and a Simulation Repository to enhance LLM flexibility for power electronics (PE) design. PE-GPT leverages a Retrieval Augmented Generation (RAG) technique to address domain expertise limitations, allowing it to handle both textual queries and PE-specific fine-tuning data like waveforms. The authors explore three customization techniques for LLMs: chain-of-thoughts prompting, RAG, and fine-tuning, ultimately proposing a tailored approach combining prompt engineering and RAG due to their flexibility and simplicity.  PE-GPT is designed to be adaptable to various power converters, with a focus on the DAB converter. This research demonstrates the potential of LLMs for intelligent PE design and explores future directions and ethical considerations.\n",
      "[Group 4] PE-GPT is a hybrid framework that combines generative pre-trained transformer models (LLMs) with simulation tools to streamline power electronics (PE) design.  It leverages a Model Zoo of data-driven models for design exploration and verification, enabling users to request simulation verification of LLM-generated designs.  Discrepancies between model outputs and practical results can be addressed through fine-tuning workflows.  PE-GPT incorporates Retrieval Augmented Generation (RAG) to enhance LLM performance in PE-specific tasks by accessing technical details from knowledge bases like academic literature.  RAG utilizes chunking, embedding, and retrieval techniques to improve the accuracy and relevance of LLM responses.  Fine-tuning hyperparameters such as chunk size and top-k is crucial for optimizing the retrieval effectiveness and efficiency of the RAG system.\n",
      "[Group 5] This research introduces PE-GPT, a novel multimodal framework for parametric engineering (PE) design optimization, particularly in power electronics. PE-GPT leverages a Model Zoo containing diverse models, including Physics-Informed Neural Networks (PANN), to evaluate design performance.  A key component is a customized PANN model trained on a three-part dataset to predict inductor current dynamics, enabling efficient and explainable design. PE-GPT integrates a metaheuristic algorithm that utilizes these predictions to optimize modulation parameters, enhancing efficiency and reliability of power electronic systems. The framework's flexibility is further enhanced by a Retrieval Augmented Generation (RAG) system, allowing for customizable knowledge bases tailored to specific applications. This approach combines the strengths of physics-based modeling and machine learning, paving the way for advanced PE design solutions.\n",
      "[Group 6] This research introduces PE-GPT, a framework for parameter engineering in power electronics using a large language model (LLM). PE-GPT leverages a curated Model Zoo and a RAG component to enhance generalization and access to current information.  The framework's \"training-free\" domain transfer capability allows adaptation to various power electronic topologies and operating conditions. A case study using a DAB converter demonstrates PE-GPT's effectiveness in predicting time-series performance and optimizing modulation parameters for current stress reduction and soft switching.  Through an iterative design process, PE-GPT showcases its capabilities in adaptive semantic understanding, PE-specific knowledge and reasoning, and multimodal processing. This research highlights the potential of AI in accelerating and improving the design process for complex electronic systems.\n",
      "[Group 7] This document explores advanced modulation strategies for power converters, focusing on minimizing inductor current stress and maximizing efficiency.  Triple Phase Shift (TPS) modulation is presented as a promising solution due to its ability to independently control phase shifts, but its soft-switching range may be limited in certain applications.  A 5DoF modulation scheme is proposed as an alternative when TPS falls short, offering a wider soft-switching range and enhanced efficiency under varying load conditions.  \n",
      "\n",
      "The document introduces PE-GPT, a multimodal platform leveraging GPT-4 and Retriever-Augmented Generation (RAG) to design optimal modulation parameters. PE-GPT combines linguistic understanding with domain-specific knowledge to reason about power electronics design and recommend strategies.  It further refines its accuracy by incorporating user feedback and experimental data, bridging the gap between textual and experimental knowledge in the field.\n",
      "[Group 8] This research explores the effectiveness of physics-informed artificial neural networks (PANNs) and PE-GPT, a data-driven model for personalized education, in various engineering applications.  PANNs demonstrate significant accuracy improvements in predicting inductor current, outperforming existing models like TST and PINN.  Fine-tuning PANN's hyperparameters is crucial for optimal performance.  PE-GPT, fine-tuned for circuit parameter design and buck converter design, achieves high accuracy, fast response times, and demonstrates capabilities in understanding user requirements, automating design processes, and providing rationale based on specialized knowledge.  Optimal hyperparameter settings for PE-GPT's RAG component are identified as chunk size = 512 and top-k = 7.  These findings highlight the potential of data-driven models in enhancing engineering design and analysis.\n",
      "[Group 9] PE-GPT, a large language model trained on power electronics (PE) domain knowledge, demonstrates exceptional capabilities in circuit design optimization.  It successfully designs circuits, such as an output LC filter for a buck converter, meeting desired specifications for voltage and current ripple while considering power density.  PE-GPT's performance is validated through comparison with PLECS simulations and human expert designs, achieving accurate ripple predictions.  Furthermore, PE-GPT outperforms both human experts and state-of-the-art LLMs in understanding and responding to PE-related queries, showcasing its effectiveness in comprehending complex PE concepts.  This research highlights PE-GPT's potential as a valuable tool for assisting engineers in designing efficient and reliable PE circuits.  Its ability to understand design requirements, automate the design process, and provide rationale for design outcomes makes it a promising advancement in the field of power electronics.\n",
      "[Group 10] This collection of research explores advancements in both artificial intelligence and power electronics. PE-GPT, a novel AI model, significantly outperforms existing LLMs in recommendation accuracy by leveraging a specialized RAG system for enhanced reasoning and reduced hallucination.  \n",
      "\n",
      "In the realm of power electronics, a 5-degree-of-freedom (5DoF) modulation strategy for power converters is demonstrated, achieving both zero-voltage switching (ZVS) and zero-current switching (ZCS) across various load conditions. This 5DoF strategy outperforms existing methods in terms of efficiency, current stress, and soft switching capabilities, particularly at light loads.  \n",
      "\n",
      "These findings highlight the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion.\n",
      "[Group 11] PE-GPT is a groundbreaking multimodal AI tool designed specifically for power electronics (PE) design.  Leveraging a retrieval augmented generation (RAG) system and a customized PE knowledge base, PE-GPT surpasses the limitations of general-purpose LLMs by enabling PE-specific reasoning and knowledge access. This innovative framework supports diverse PE design workflows, from topology selection to documentation, and demonstrates its effectiveness through real-world case studies.  However, the authors emphasize the crucial need to address ethical considerations such as data privacy, security, and potential bias in training data.  Mitigating these risks through robust security measures, diverse datasets, fairness-aware algorithms, and transparent decision-making processes is essential for responsible implementation.  Ultimately, PE-GPT is poised to revolutionize the PE industry, augmenting engineers' capabilities and accelerating the design process.\n",
      "[Group 12] This document explores advancements in various fields, including power electronics, grid-connected photovoltaic systems, and artificial intelligence.  \n",
      "\n",
      "It highlights the success of PE-GPT, a multimodal LLM, in power electronics design, showcasing its superior performance compared to human experts and other LLMs.  \n",
      "\n",
      "The research also delves into the potential and challenges of using large language models like GPT-4 in medicine, emphasizing the need for ethical considerations and responsible deployment.  \n",
      "\n",
      "Furthermore, the document examines the application of metaheuristic algorithms in multimedia big data and explores innovative approaches in power converter design and modeling.  \n",
      "\n",
      "Finally, it introduces open-source models like Llama 2 and Gemini, emphasizing their capabilities and potential impact across diverse domains.\n",
      "[Group 13] This document explores the intersection of artificial intelligence (AI) and power electronics, particularly in the design of dual active bridge DC-DC converters.  Research highlights AI-driven modulation strategies for improved efficiency and zero-voltage switching, with physics-informed AI models like PE-GPT showing promise for interactive and efficient design.  The work of Dr. Lin and Dr. Li, both leading researchers in the field, is featured, emphasizing AI-powered converter design, life cycle management, and the importance of responsible AI implementation and explainability.  Dr. Li's team, including Weihao Lei and Prof. Juan J. Rodríguez-Andina, investigates AI applications in power electronics and physics-informed systems. Prof. Rodríguez-Andina's expertise lies in implementing intelligent sensors and control algorithms in embedded systems, a field recognized by numerous IEEE awards.  While exploring the potential of AI, the document acknowledges the ethical implications, including fairness and bias, urging careful consideration for its beneficial use.\n",
      "[Group 14] This document profiles several researchers specializing in industrial electronics, particularly power electronics, control systems, and renewable energy.  Antoni, Changyun Wen, and Xin Zhang are highlighted for their expertise and significant contributions to the field, evidenced by their highly cited publications and leadership roles in IEEE.  Changyun Wen holds a professorship at Nanyang Technological University and is a Fellow of IEEE, while Professor Wen serves as editor-in-chief for multiple journals.  Xin Zhang and Hao Ma, both affiliated with Zhejiang University, focus on power electronics and electrical engineering respectively, with international research experience. Dr. Ma's research delves into advanced control, wireless power transfer, and fault diagnosis, significantly advancing power electronics technology.\n",
      "[Group 15] This document details the professional accomplishments of an individual affiliated with the China Power Supply Society.  They held a significant role as an AdCom Member of the IEEE Industrial Electronics Society, demonstrating their expertise in the field.  Their leadership is evident in their chairing of multiple IEEE conferences, including ISIE 2012, PEAC 2014, PEAC 2018, and PEAS 2021. These conferences likely focused on power electronics and industrial applications, given their association with the IEEE Industrial Electronics Society.  Their contributions to these organizations highlight their prominent position within the power electronics community.  Overall, this document showcases their impactful career and contributions to the field.\n",
      "\n",
      "--- Final Summary ---\n",
      "**Introduction to PE-GPT and its Significance**\n",
      "\n",
      "The introduction of PE-GPT, a novel multimodal large language model, marks a significant advancement in the field of power electronics (PE) design. Specifically designed to overcome the limitations of general large language models (LLMs), PE-GPT incorporates a PE knowledge base, integrates with metaheuristic algorithms, and utilizes a Model Zoo and Simulation Repository. This innovative framework enables PE-GPT to process both textual and quantitative data, facilitating design optimization, simulation verification, and fine-tuning of data-driven models. Evaluations through case studies and hardware experiments demonstrate PE-GPT's superior correctness, consistency, and design optimization capabilities compared to both human experts and other LLMs.\n",
      "\n",
      "**Technical Overview of PE-GPT**\n",
      "\n",
      "PE-GPT leverages a Retrieval Augmented Generation (RAG) technique to address domain expertise limitations, allowing it to handle both textual queries and PE-specific fine-tuning data like waveforms. The authors explore three customization techniques for LLMs: chain-of-thoughts prompting, RAG, and fine-tuning, ultimately proposing a tailored approach combining prompt engineering and RAG due to their flexibility and simplicity. PE-GPT is designed to be adaptable to various power converters, with a focus on the DAB converter. This research demonstrates the potential of LLMs for intelligent PE design and explores future directions and ethical considerations. The framework's flexibility is further enhanced by a RAG system, allowing for customizable knowledge bases tailored to specific applications.\n",
      "\n",
      "**Applications and Case Studies**\n",
      "\n",
      "The effectiveness of PE-GPT is demonstrated through various case studies, showcasing its potential to revolutionize PE design by enhancing efficiency and accuracy. For instance, PE-GPT successfully designs circuits, such as an output LC filter for a buck converter, meeting desired specifications for voltage and current ripple while considering power density. PE-GPT's performance is validated through comparison with PLECS simulations and human expert designs, achieving accurate ripple predictions. Furthermore, PE-GPT outperforms both human experts and state-of-the-art LLMs in understanding and responding to PE-related queries, showcasing its effectiveness in comprehending complex PE concepts.\n",
      "\n",
      "**Advancements in Power Electronics**\n",
      "\n",
      "In addition to PE-GPT, the research explores advancements in power electronics, including the development of a 5-degree-of-freedom (5DoF) modulation strategy for power converters. This strategy achieves both zero-voltage switching (ZVS) and zero-current switching (ZCS) across various load conditions, outperforming existing methods in terms of efficiency, current stress, and soft switching capabilities. The 5DoF strategy is particularly effective at light loads, making it a promising solution for power electronic systems. These findings highlight the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion.\n",
      "\n",
      "**Ethical Considerations and Responsible Implementation**\n",
      "\n",
      "While PE-GPT and other AI-powered solutions offer significant benefits, the authors emphasize the crucial need to address ethical considerations such as data privacy, security, and potential bias in training data. Mitigating these risks through robust security measures, diverse datasets, fairness-aware algorithms, and transparent decision-making processes is essential for responsible implementation. The research highlights the importance of careful consideration for the beneficial use of AI in power electronics, ensuring that these technologies are developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "**Intersection of AI and Power Electronics**\n",
      "\n",
      "The research explores the intersection of artificial intelligence (AI) and power electronics, particularly in the design of dual active bridge DC-DC converters. AI-driven modulation strategies for improved efficiency and zero-voltage switching are investigated, with physics-informed AI models like PE-GPT showing promise for interactive and efficient design. The work of leading researchers in the field, including Dr. Lin and Dr. Li, is featured, emphasizing AI-powered converter design, life cycle management, and the importance of responsible AI implementation and explainability.\n",
      "\n",
      "**Contributions of Researchers and Experts**\n",
      "\n",
      "The document profiles several researchers specializing in industrial electronics, particularly power electronics, control systems, and renewable energy. Antoni, Changyun Wen, and Xin Zhang are highlighted for their expertise and significant contributions to the field, evidenced by their highly cited publications and leadership roles in IEEE. The contributions of these researchers, along with others, have significantly advanced power electronics technology and demonstrate the importance of collaboration and knowledge sharing in driving innovation.\n",
      "\n",
      "**Professional Accomplishments and Leadership**\n",
      "\n",
      "The document details the professional accomplishments of an individual affiliated with the China Power Supply Society, demonstrating their expertise in the field. Their leadership is evident in their chairing of multiple IEEE conferences, including ISIE 2012, PEAC 2014, PEAC 2018, and PEAS 2021. These conferences likely focused on power electronics and industrial applications, given their association with the IEEE Industrial Electronics Society. Their contributions to these organizations highlight their prominent position within the power electronics community.\n",
      "\n",
      "**Conclusion and Future Directions**\n",
      "\n",
      "In conclusion, PE-GPT represents a significant advancement in the field of power electronics design, offering superior correctness, consistency, and design optimization capabilities compared to human experts and other LLMs. The research highlights the potential of AI-powered solutions like PE-GPT for optimizing power electronic designs and the effectiveness of advanced control strategies for achieving efficient and reliable power conversion. As the field continues to evolve, it is essential to address ethical considerations and ensure responsible implementation of these technologies. The contributions of researchers and experts, along with the leadership and professional accomplishments of individuals in the field, will drive innovation and advancement in power electronics, ultimately shaping the future of this critical technology.\n"
     ]
    }
   ],
   "source": [
    "# === AFTER EXTRACTION ===\n",
    "\n",
    "# You already have:\n",
    "# final_combined_output = \"\\n\".join(final_combined_output_list)\n",
    "\n",
    "# Now run the summarization pipeline\n",
    "result = process_section(final_combined_output, \"Full Document\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(\"\\n--- Chunk Summaries ---\")\n",
    "for i, cs in enumerate(result[\"chunk_summaries\"], 1):\n",
    "    print(f\"[Chunk {i}] {cs}\")\n",
    "\n",
    "print(\"\\n--- Group Summaries ---\")\n",
    "for i, gs in enumerate(result[\"group_summaries\"], 1):\n",
    "    print(f\"[Group {i}] {gs}\")\n",
    "\n",
    "print(\"\\n--- Final Summary ---\")\n",
    "print(result[\"final_summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a54a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] All summaries saved to summaries.json and final_summary.txt ✅\n"
     ]
    }
   ],
   "source": [
    "import json # Import the json library\n",
    "\n",
    "# Save everything (chunks + groups + final) in JSON\n",
    "with open(\"summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save just the polished final summary in TXT\n",
    "with open(\"final_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"final_summary\"])\n",
    "\n",
    "print(\"\\n[INFO] All summaries saved to summaries.json and final_summary.txt ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eec20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
